Post Number,Title,Text,Comments
11fio85,"We are the developers behind pandas, currently preparing for the 2.0 release :) AMA","Hello everyone!

I'm Patrick Hoefler aka phofl and I'm one of the core team members developing and maintaining pandas ([repo](https://github.com/pandas-dev/pandas), [docs](https://pandas.pydata.org/docs/dev/index.html)), a popular data analysis library.

This AMA will be at least joined by

* [Marc Garcia](https://github.com/datapythonista) \-- maintainer
* [Marco Gorelli](https://github.com/marcogorelli), -- maintainer
* [Richard Shadrach](https://github.com/rhshadrach) \-- maintainer
* [me](https://github.com/phofl)! -- maintainer

**The official start time for the AMA will be 5:30pm UTC on March 2nd**, before then this post will exist to collect questions in advance. Since most of us live all over North America and Europe, **it's likely we'll answer questions before & after the official start time by a significant margin**.

**pandas** is a Python package that provides fast, flexible, and expressive data structures designed to make working with ""relational"" or ""labeled"" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, **real world** data analysis in Python. Additionally, it has the broader goal of becoming **the most powerful and flexible open source data analysis / manipulation tool available in any language**.

We will soon celebrate our 2.0 release. We released the release candidate for 2.0 last week, so the actual release is expected shortly, possibly next week. Please help us in testing that everything works through [testing the rc](https://github.com/pandas-dev/pandas/releases/tag/v2.0.0rc0) :)

Ask us anything! Post your questions and upvote the ones you think are the most important and should get our replies.

\- Patrick, on behalf of the team

&#x200B;

&#x200B;

Marc:

I'm Marc Garcia (username datapythonista), pandas core developer since 2018, and current release manager of the project. I work on pandas part time paid by the funds the project gets from grants and sponsors. And I'm also consultant, advising data teams on how to work more efficiently. I sometimes write about pandas and technical topics at my [blog](https://datapythonista.me/blog/), and I speak at Python and open source conferences regularly. You can connect with me via [LinkedIn](https://www.linkedin.com/in/datapythonista/), [Twitter](https://twitter.com/datapythonista) and [Mastodon](https://fosstodon.org/@datapythonista).

Marco:

I'm Marco, one of the devs from the AMA. I work on pandas as part of my job at Quansight, and live in the UK. I'm mostly interested in time-series-related stuff

Patrick:

I'm Patrick and part of the core team of pandas. Part of my daytime job allows me to contribute to pandas, I am based in Germany. I am currently mostly working on Copy-on-Write, a new feature in pandas 2.0. (check my [blog-post](https://medium.com/towards-data-science/a-solution-for-inconsistencies-in-indexing-operations-in-pandas-b76e10719744) or our new [docs](https://pandas.pydata.org/docs/dev/user_guide/copy_on_write.html) for more information).

Richard:

I work as a Data Scientist at [84.51](https://www.8451.com/) and am a core developer of pandas. I work mostly on groupby within pandas.

\--

* [Announcement post](https://www.reddit.com/r/Python/comments/11ebuh0/join_us_for_an_ama_with_the_developers_of_pandas/)
* [full pandas team](https://pandas.pydata.org/about/team.html)
* [Marc wrote about pandas 2.0 and our Arrow support](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)","What's the most impressive/unimaginable use of Pandas you've come across? | How does the Pandas project address the open source funding problem? Do you want pandas devs in their dayjobs to nudge management to sponsor somehow? | Why choose mm/dd/yyyy as default date rather than dd/mm/yyyy ü§î?
(Just banter from an european guy)
Real questions:

- what are the main improvment focus going forward ?

- what caused you the most problems / was the most complex parts during delevopment ? 

- what was the most fun / rewarding parts during development ?

- in my work, I use pandas as a data processing engine (kinda), the data I process if often heterogeneous and full of holes / discrepancies, I often find myself finding with rhe way pandas handle errors as most of the time I just want to log the fact that this row had a error. Why not put a 'error' arg to apply, just as in astype and such ?

I also would like to thank you guys for your amazing work, pandas has been making my life easier everyday, you are really doing amazing work. | i think one of the hardest things about using pandas is that the core classes have a gazillion methods attached to them, which makes it extremely difficult to navigate the tooling if you're not already intimately familiar with it. I've been using pandas basically since it was created, and I still find myself often needing to reference documentation just to find the method name I need since the output of dir() on any object generally gets truncated.

does any of this resonate? is anyone on your team thinking about ways to improve discoverability of functionality? will there ever be a point at which the team decides there's too much stuff being carried around by too few classes? what are your thoughts on the design philosophy of the tidyverse in juxtaposition to pandas? | Hi there! Long time pandas user -- really appreciate all the work you've done.

I'm only _slightly_ familiar with changes intended in pandas 2.0, namely the switch away from a numpy backend to apache arrow. Historically, the thing I absolutely love about the python numerical stack, is that nearly everything builds off numpy arrays, creating an easily transferable knowledge base between projects.

This is a huge boon compared to other systems where I work (namely `R`), where there is often more fragmentation in the ecosystem, making interoperability or bespoke analyses much more difficult. Of course, fragmentation in the Python ecosystem has become more common with things like PyTorch tensors, etc.

As an end user, am I going to be losing the numpy < - > pandas interoperability in 2.0? Please feel free to correct any inaccuracies on my end. | What do you think is the most important advice for someone just starting to work with pandas? | I just started working with pandas two weeks ago, there is so much for me to learn and unpack there so I don‚Äôt have a question. Just wanted to give you a shout out for your awesome body of work. | Are there any improvements that are coming by way of working with larger datasets/operations without consuming available RAM? I struggle with workarounds when dealing with large data on my 24GB RAM laptop. 

Awesome work by the way, Pandas is amazing and we appreciate the work you guys do. | I frequently work with pyspark, and although I don't use this feature I know it has support for ""pandas udfs"" while using arrow behind the scenes.

Now that arrow will be integrated into pandas, do you think we will see improvements in this area? (Performance improvements more features between spark and pandas) | How do you look at the success of the tidyverse library in R, and what lessons or good ideas are in there that pandas can benefit from? | Thanks for all the work that you do! My question is who pays for pandas development and why? Is most of the development done by volunteers? | Has polars influenced development in any way? 

Pandas used to be the only kid on the block, but it seems there are some other libraries popping up claiming to be faster/better/etc. Have you evaluated any of these other libraries to potentially integrate features into pandas (or improve existing ones)? | Any plan on improving pandas I/O load/export and out of mem processing? 

I like Pandas but my data nowadays grew beyond that. So I am currently all in spark. | Pandas finally support arrow which support decimal. Which means pandas can be used in financial production system. Finally! | In general, are the plans to have the `rolling` API more closely align with the rest of the pandas API? In particular, are there any plans to have `df.rolling.groupby()` return similarly indexed results as a normal `df.groupby()`? 

E.g., with the latter you have the wonderful `.transform()` method to add a column to the `df`. When working with the rolling window, you always get a MultiIndexed dataframe that is much harder to align to the index of the original `df`. 

Perhaps (hopefully?) there are better ways, but I currently use a combination of extracting a single column as Series, using `groupby(as_index=False)` and finally a call to `set_axis(df.index)` to get the desired result to align with my original dataframe. | I work with pandas quite a bit for geospatial data analysis, weather data mostly.  Because of the higher dimensionality of the data I typically stack the dependent variables into the index as a multi-index [T,X,Y].

Recently I‚Äôve been working with Generic[Enum] types to type annotate the columns inside of a DataFrame.

What kind of support will 2.0 provide for type annotations. One thing I‚Äôve found as a particular annoyance is disconnect between numpy and pandas typing.  Where I have to explicitly state the dtype for NDArray[np.int_] and Series[int] and can‚Äôt use a TypeVar DType. | Pandas is quite a large and mature project already, is there any space for beginners to contribute? | Love yall love üêº pandas | Will Pandas 2.0 impact numba/cython extensions that leverage Numpy? 

Many complain about the API of Pandas. Was there any discussion about revamp/cleaning it up during 2.0 release? | What would the next big leap for Pandas be? What kind of resources would you need to achieve it? | Wonderful to see you guys on here. I personally use pandas so often!

Do you guys have any advice for someone wanting to contribute back to the pandas project? | I work almost daily with Pandas so I definitely want to give me thanks and appreciation for this excellent tool.

Any plans for built-in parallelization in Pandas? I know there are many modules attempting to implement this with varying success, like pandarallel, dask or swifter. However I had difficulty getting any of these to work in an existing application without major refactoring. 

In our case, we have a high level application class or processor that ingests many dataframes which sit in memory as properties to the processor instance. This processor does various processing to different dataframes in conjunction with eachother, like iterrows or applys on one dataframe while checking other dataframes which are all unique attributes of the same object running in memory concurrently.

However when the processor class actually runs, ultimately everything is stuck in a single core but I would say most systems have at least 6 or more cores now, even cheap laptops. Having a model or two to apply parallelization using concurrent.futures based on threads or processes seems like it would make a lot of sense. I think threads would likely work well if implemented intelligently, but I'm sure I am oversimplifying. | I've developed my own library that has gotten the attention of a handful of people i don't know. I'm most curious about the beginnings of `pandas`‚Äîhow did you handle its monumental growth? It's such a staple of Python programming these days, how did you manage all the influx of issues, contributions, etc.? | I have used Pandas extensively. I want to contribute. What are the languages or stack i need to know apart from Python? | How much data and fast pandas 2.0 can read at once. If you compare it with pyspark csv reader, how pandas will perform? | Any good first issues that I can help contribute to in the pandas repository? | I'm just a newbie but just wanted to hop in and appreciate y'all. As a data science major, pandas has been super helpful so thanks for your work :) | Dear Pandas, please make a universal UTF-8 translator for tabulated data. | Just a stupid question, do you still know how everything works in pandas or do you go to the documentations sometimes as well? Haha | With the integration of Arrow will pandas support Structs from Bigquery better? Like having the types and understand lists inside columns, better visualization of them... I remember the last time I had to deal with them, couldn't make pandas understand what was inside the column with an .astype(...), I checked the code of bigquery and they were getting the data into an arrow table, and creating the schema to use it while exporting to a pandas dataframe

Edit: I saw an issue [about it](https://github.com/pandas-dev/pandas/pull/45745), but it was stale. | Thanks for the AMA.  How do you expect performance to evolve in pandas 2.0 and beyond? I tend to flip between python+pandas and R+data.table. For large datasets (10M-50M rows) I find that group_by/aggregate type operations are an order of magnitude slower in pandas, especially as the number of unique subgroups increases. Also I find memory usage to be significantly higher and well and tend to resort to techniques like downcasting column types after reading to keep memory consumption down, whereas R ‚Äújust works‚Äù for the most part. 

Do you view this as a fair comparison? Is performance a first-order concern for you and your team?  Hoping to move more from R to Python one of these days but pandas speed is definitely holding me back. 

Thanks! | what are some good pandas books that you would recommend for beginners , or intermediate users ? | I have kind of a broad question. I work as an ETL engineer, and the vast majority of my current work is with CSV files (sometimes xlsx). In my experience, a lot of people in the ETL space default to pandas because of the simple ""read\_csv/xlsx ‚Üí to\_sql"" pipeline and the speed of vectorized operations. I also suspect there's a ""popular because it's popular"" thing happening to some degree. I've developed (and cultivated) a bit of a reputation as a pandas hater for a few reasons:

1. We do basically zero numerical analysis or aggregation during ETL
2. Every validation happens within a single record (i.e. no record has any bearing on any other record), so we end up using iterrows a lot
3. Lack of support for strings and None (dealt with in 2.0)

Ultimately, I feel that pandas is a hammer that makes everything look like a nail *specifically for ETL*. I have used pandas for data analysis and really appreciate its usefulness and how easy it is to teach to others (even if they don't know Python). But when we're talking ETL, it just isn't the right tool...right?

Anyway, that's barely a question, but if you have time I'd love to hear your thoughts on pandas usage in non-data analysis spaces. Also I want to qualify this by saying I am far from an expert and I'm really just looking to learn! Thanks for your time. | Thanks a lot for coming here. I am a huge pandas user and I feel excited about this release. I have a question not related to pandas, but maybe you know the answer. In my case I use pandas for EDA and preprocessing before feeding a model from sklearn or tensorflow. Do you know if a dataframe based on arrow will be compatible with sklearn/tensorflow? Or of there are plans to make them compatible? 

And what about all numpy methods, do you know if the arrow API is similar to the numpy one? Maybe, thanks to duck typing, many things could still work the same... | Where can I find a high-level software architecture diagram that illustrates how Pandas is built? | The size of pandas codebase and features like integrated graphs and stuff made it more difficult to be able to change the backend from numpy to pyarrow? With pyarrow are we going to have any difficulty or limitation working with numpy? | Thanks for your great work. As a long time pandas user I literally owe half of my career to you. I have two questions:

1.	Reading what Marc wrote about Pandas 2.0 and Arrow, how has the emergence of Polars affected your decisions around Pandas 2.0? 
2.	Programming errors are bound to happen. While for small projects, the feedback cycle between writing code and finding errors during runtime is small, this approach doesn't scale very well for large applications, due to their complexity and cost of testing. Are there any plans that allow for more static analysis of Pandas code? | Thank you for the hard work. Loved pandas for a long time and I use it daily | Excited for the new release! As a maintainer of code that relies on pandas, what will I need to look out for that might break upon upgrading to 2.0? | I hear pandas is not good for production ready code, is this true? | Why does pandas have a dependency on numpy?

This prevents pandas from being used properly in a WSGI web-server environment due to numpy requiring an interpreter is never loaded twice. But that's exactly what happens in a WSGI Flask Application, so using pandas in a Flask application will break it. Thanks. | As someone (currently) a student who's used open source packages for some time now (mostly Pandas and Django, alongside many many others), I've been looking to contributing to them. I've started to go through the [contributing](https://pandas.pydata.org/docs/development/contributing.html) page, as well as reading some of your blogs. Also stumbled across your Google Calendar for meetings - will be there soon! 

In general, what have you found is the biggest obstacle (technical or otherwise) preventing aspiring contributors from progressing? What have people done right, and what have people done wrong? Personally, if I was hitting a wall in any way at all, I'd hesitate to ask (whether on slack / otherwise) mostly because the goal is to *contribute* to this effort - but taking up another member's time to deal with a newcomer might do the opposite. Any thoughts on this? | Polars or pandas? | For beginners and new projects, does it make sense to move to 2.0 right away? If so, what options should be enabled (e.g., `nullable_dtypes`, `dtype_backend`, `copy_on_write`), assuming that the goal is reasonable stability and modernity?

I built and teach a workshop for other academic researchers, and I'm currently  doing an update (ahead of the next session in June). Pandas is a big part, though the course ranges from beginner Python language topics to data retrieval/wrangling. The core of this content update is moving from local conda installs to devcontainers/Codespaces, but it would be a good opportunity for me to move to `pandas>=2.0.0` if that's a good idea on this timeline. | I've used pandas a bunch, so thank you! 

One thing I've never really grasped is why multi-indexed results are returned rather than flattening. I have never had a circumstance where I've used that sort of result directly, and always have to figure out how to get rid of the ""outer"" index. What was the reasons for this multi-index, and not just a rectangular dataframe? | As a total noob to pandas, what makes it substantively different than the python standard library? For instance I can create lists of lists or dicts of lists. Drop/add items etc. I can combine lists of lists too.

I'm not trying to be a smart*ss, I'm genuinely asking: what's the differentiator? Pandas is a bit heavy. If I can write some classes that kind of handle those operations and a few more with syntatic sugar, why install pandas? | firstly - huge huge fan of pandas. thank you. and <3 the docs/api.  I spend a lot of time there and know 99% of the time get the answers I need. 

quite a simple one from me - why no normalize in groupby() and pivottable()? the lack of parity between them and pd.crosstab() is a daily pain point. | I was personally quite surprised that pandas was an important tool used to obtain the [first image of a black hole](https://eventhorizontelescope.org/blog/astronomers-reveal-first-image-black-hole-heart-our-galaxy). I was lucky to meet some of the scientists behind it and learn from them, and their work is much more impressive than what it sounds. | Last years has been better. pandas got some funding, including few core devs being paid to work in pandas in companies such as Quansight, Intel or NVIDIA. And we also received money from the Chan Zuckerberg Initiative, Tidelift, Bodo and smaller donors. Just few years ago funding was very limited, but today, we're lucky to be able to have a decent amount of paid maintainers. | If you use pandas for work and your employer wanted to contribute, then

1. thanks!
2. they could do so via NumFOCUS: [https://pandas.pydata.org/donate.html](https://pandas.pydata.org/donate.html)

&#x200B;

Marc's right though, the funding situation has drastically improved recently | It's also helpful, if developers can get paid time by their employer to work on pandas! | \> Why choose mm/dd/yyyy as default date rather than dd/mm/yyyy

I presume you mean, when a date could be ambiguously read as either month-first or day-first? Like 02/01/2000.

In the past, pandas would prefer to parse with month-first, and then try day-first. Unfortunately, it would do so midway through parsing its input, because it was very lax about allowing mixed formats. This would regularly cause problems for anyone outside of the US (which I think is the only place in the world to use the month-first convention).

As of pandas 2.0, datetime parsing will no longer swap formats half-way through. See: [https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html](https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html) , which I spent several months on.

In dealing with the PDEP I linked above, my biggest pain-point was having to understand and then update decade-old C code

Regarding your last question, if you put together a reproducible example with expected output, it might be a reasonable feature request.

Thanks, and thank you for your comment! | > in my work, I use pandas as a data processing engine (kinda), the data I process if often heterogeneous and full of holes / discrepancies, I often find myself finding with rhe way pandas handle errors as most of the time I just want to log the fact that this row had a error. Why not put a 'error' arg to apply, just as in astype and such ?

According to [this](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) blogpost by /u/datapythonista it sounds like a limitation of the numpy backend dataframes are built-on, check out this excerpt, I bolded the relevant part:

>While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations. A couple of examples are the poor support for strings and **the lack of missing values**.

So maybe something we can hope to see fixed with the migration to Arrow in 2.0? | We are spending a lot of time on improving the extension array interface right now. Right now there are some parts that are special cased internally for our own extension arrays which makes it harder for third party authors to implement their own without falling back to NumPy. GroupBy is a good example for an area where we are still not as good as we would like. This becomes kind of necessary for improving support for our pyarrow extension arrays as well.  


We have some areas in our code-base that are pretty complex, indexing is one of them for example. In general, we try to avoid breaking stuff in an incompatible way in minor releases. This makes improving pandas tricky sometimes, because it stands in the way of cleaning up internally/refactoring internally to be more compatible with new stuff. | Ugh please fix this! Love pandas | Fully agree on this. There are too main things. The first is finding a better API, which is not trivial, and having the functions too divided may not be ideal for some users who prefer \`df.whatever()\` for everything. Second is that even if we have a better alternative, we may break tens or hundreds of thousands of pandas programs, that won't work after the changes. And we will make millions of users have to relearn the API.

That being said, I'm thinking about a proposal to for example standardize all I/O methods under a \`DataFrame.io\` namespace (e.g. \`df = pandas.DataFrame.io.read\_csv(fname)\`). More research is needed, and it'll be challenging to reach an agreement with the whole team about this. But maybe 10% of the DataFrame methods you're mentioning would live in a separate and intuitive namespace. There is always a trade-off, and in this case it's clear. Difficult to decide what's best. | +1 this is an excellent point I'd never given much thought. I find myself referencing pandas docs more than any other and use it for about 1/4 of the overall code/libs. | I think you will find similar sentiments among most if not all pandas devs. Our API is huge. This requires a lot of maintenance and bugfixes, and takes time away from further enhancements. But at the same time, it can be very hard because I may not personally find a particular method or argument useful, but maybe many of our users do. | There has been some work to make pandas and Polars share data (open a pandas dataframe with Polars, and the other way round). You can read more about it at the end of [this post](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i). Not sure if there is any other integration that makes sense, any idea? | I believe pandas will be moving over to apache arrow. | They already work almost seamlessly, just make sure to use pyarrow backend when converting your data in and out of pandas. | Not at all. NumPy is not only staying in pandas 2.0, but it'll still be the default.

That being said, if in the very long term NumPy is eventually dropped, I think exporting from Arrow to NumPy (in our end, not that you'll need to do it) is not only easy, but I think in most cases it can be done without copying (extremely fast, even for huge data). The thing is that NumPy data types are more limited, mostly numeric. If you want to export a string column to NumPy, that's a different story, but there is probably no good reason you want to do that. But for the types that NumPy support well, getting a NumPy array from Arrow backed data won't be a problem. But as said, in pandas 2.0, nothing changed, unless you want it to change and you ask explicitly for pandas types. | Very good point. I myself find the index column in the output csv annoying every single time I use \`to\_csv\`. I wasn't in the project when that was implemented, but I assume the reason is that pandas was initially implemented for financial data, and the index was mostly the timestamp and not the default autonumeric. If that was not the data pandas developers had in mind at that time, probably pandas wouldn't even have row indices (I think Vaex doesn't, not sure about Polars).

The next question is why we don't change it now. And it's something worth considering, and you're free to open an issue in GitHub. But in general, pandas developers (others much more than me), try to not break the API, unless it's in cases where very few users will be affected and the status quo is obviously inconsistent. I'd personally like to see that changed, but I don't think it'll be easy to get consensus.

What I think it can make sense is to try to move all pandas I/O (read\_\* and to\_\*) to third-party projects. In that case the pandas to\_csv would continue to behave in the same way, but hopefully someone would develop a new one like to\_csv(engine='whatever') that could potentially be faster, have a better API, and more appropriate for your needs. But let's see if there is consensus for this to happen. | Try to spend some time understanding the internals, as you make progress with pandas. Not at the beginning, when you'll have too much to learn just with the basics. But as you become more familiar, it's good to have an idea of what's really happening, in particular when things aren't intuitive. Things like missing values, the infamous copy warning... | I would say having a high level understanding of where pandas gets it speed from; that one should avoid doing computations ""in Python space"" whenever possible. Similarly, understanding the difference between the pandas Index and columns and how this makes an impact on compute. Finally, thinking really hard about your data model and using it to set up appropriate (multi)indices can go a long way to improving your use of pandas. | That would be a huge change in pandas, and we try to keep pandas stable, so existing users don't need to make huge migrations and relearn the API often.

I don't think lazy evaluation is likely to land in pandas, at least not in the short or mid term. Luckily other options are being created that are or can be lazy, like Polars, Dask or Koalas. | [Being able to use Arrow](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) as a backend for your data can save a significant amount of RAM in some cases. Also there is a lot of work related to copy-on-write, that will avoid copying the data when not needed, and will also help reduce the memory needs. | I think it'll take a while, but hopefully we'll eventually see more feature sharing between libraries given we all use Arrow internally. Arrow itself has the concept of kernel, that it's a computation that can be applied to Arrow data. And those can be reused by any library. And the same would apply to user defined functions (udfs). That being said, pyspark is probably using the Java implementation, while pandas is using PyArrow. So, I guess difficult to share many features (I'm not an expert on the JVM, not sure if you could easily call C++ code from a scala program). | I'd remove having a row index (at least by default), and the I/O API: being consistent with read\_\*/write\_\* or from\_\*/to\_\*. I'd also probably remove half of the code in pandas to other third-party extensions. ![gif](emote|free_emotes_pack|sunglasses) | Personally, I'd love to be able to change the default indexing behaviour.

The Index is useful if it means something (e.g. a DatetimeIndex), but if it's just a RangeIndex / NumericIndex, then it can be annoying and confusing.

&#x200B;

But this is really hard to change because:

- introducing optional behaviour comes with a huge maintenance cost (I started making such a proposal [here](https://github.com/pandas-dev/pandas/pull/49694), but then withdrew it)
- changing the existing behaviour would have backwards-compatibility implications

I don't know what the solution is yet, but I would like to revisit PDEP5 at some point - _something_ should be possible, I just don't know what yet. | From our docs, it appears the keyword on encoding was perhaps at one point used with xlwt (a writer that is no longer maintained) but today is not actually used by pandas. That parameter has been removed in pandas 2.0. | The main reason in releasing pandas 2.0 and not 1.6 is that in major version changes (1 -> 2) is when users expect to have breaking changes. pandas 2.0 is not so significantly different to a 1.6 in terms of features. The main difference is that you really want to make sure that you don't have FutureWarning in your pandas code before upgrading your pandas version. | Personally polars' strictness is making me think about situations when in pandas we end up with object dtype, which we should probably avoid. Here's an example: [https://github.com/pandas-dev/pandas/issues/50887](https://github.com/pandas-dev/pandas/issues/50887) (polars would just error in such a case, which I think is the correct thing to do) | I'm not an expert on it, but I think DuckDB is implementing out-of-core algorithms on parquet, csv and others. I think it shouldn't be difficult to write a wrapper: read\_parquet\_with\_duckdb(my\_file, filter\_predicate=""this\_column = 'whatever'""). Or not wrap and simply load with DuckDB to Arrow with whatever out-of-core processing you need, and load the Arrow structure as a pandas dataframe.

I don't think pandas itself is likely to implement anything out of core. I'm working on a proposal to make it easier to develop pandas connectors as third-party packages, maybe a pandas-duckdb connector could benefit from it. | Being able to use Apache Arrow internally. I wrote an [article](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) with the details about it, since it's not trivial for regular users to understand why this is important. | I didn't work on it, but copy-on-write will be pretty neat https://pandas.pydata.org/docs/dev/user\_guide/copy\_on\_write.html | I'll also mention copy-on-write. And I know it's not exciting, but all of the bug fixes throughout the code that make pandas more predictable and reliable to use. In the area I work on, groupby, using categorical data has seen a lot of fixes. | I'm unsure what's the support for decimal in pandas right now. One thing is to be able to load Arrow columns in pandas, and the other is what operations for that data type are implemented. In any case, if not all what you need is in pandas 2.0, it'll come eventually. Particularly if you open issues and PRs in our issue tracker.

That being said, you can do like the UK stock market, just have all the amount in cents, and you can do it with integers. ;) | Also checkout our docs! https://pandas.pydata.org/pandas-docs/dev/development/contributing.html | importing pandas as pd ;) | Iterating a dataframe is slow. If speed is important, you should try to build your pandas code in a way that you never implement loops, but delegate to pandas the operations, so they happen fast in C, and not via the Python interpreter.

If you iterate the data, then you're just in regular Python, with a Python tuple object, and you can write any code that is valid Python. Not sure in what case map() wouldn't be an option, but you can always replace a map by a loop (or a comprehension) when you're in Python. | There is not much impact in pandas 2.0 regarding numba/cython.

We fix small inconsistencies to the pandas API, but we avoid changing it too much, since we consider that the cost in users having to migrate code and relearning things is too much. | I think Python and R are very different worlds. At least in my experience, R API is nicer for statisticians or scientists, but confusing for software developers. So, I think it's difficult to compare. For what I know about dplyr/tidyverse they did a pretty nice job, and some ideas should probably be beneficial. But the problem is that any change to the pandas API impacts millions of users, so we try to avoid them as much as possible. Whether this is a good thing or not depends on the case. It would surely be good for new users, but if you have an application with 10M lines of code, you'll hate us for getting back to another Python 2/3 10 year migration, no? | For indexing: yes, all indexing operations on a DataFrame or Series will just work the same if the columns are backed by arrow data.  

Adding new columns works as well, however pandas will not yet automatically use an arrow-based dtype for that. For example, if you have a DataFrame with all columns backed by arrow data, and then set a new column (`df[""new_col""] = arr`), then this new column will right now still use a numpy-backed data type (unless you would first convert the numpy array to a pandas extension array backed by arrow data using `pd.array(ar, dtype=..)`). | I'd suggest starting with the contributing guide https://pandas.pydata.org/docs/dev/development/contributing.html | I'd say just keep using pandas, and the day something feels wrong (a bug, a typo, the documentation not being very clear,...), try to fix it. We have a lot of documentation for contributors, you can open an issue in github and ask questions there (or in a PR directly if you can get something implemented), there are also bi-weekly meetings with some core devs (I don't join them, can't say much about them, but they should be helpful).

Another option is to go to github issues and try to find something labelled as ""good first issue"", but there are many people looking for those, not always easy to find them.

Finally, if you're just starting, smaller projects are usually easier to get started contributing. There are simpler tasks, maintainers can have more time, the code base is simpler... Even if you want to contribute to pandas, starting by a smaller project can make the learning curve flatter. | Yes - we love getting new contributors! Check out our documentation and guides on becoming a contributor to pandas: [https://pandas.pydata.org/pandas-docs/dev/development/index.html](https://pandas.pydata.org/pandas-docs/dev/development/index.html)

pandas is a large project with some pretty complex code. It will likely be overwhelming at first. But we are here to help. If you stick with it, you will learn *a* *lot.* | Historically, pandas has relied on other libraries in the ecosystem to support parallelization such as [https://www.dask.org/](https://www.dask.org/) which uses pandas under the hood. One thing to also keep in mind is that certain NumPy operations (which pandas uses) may be parallel depending on how your BLAS (Basic Linear Algebra Subprograms) are setup. In general, you want to avoid having multiple levels of parallelism which can actually hurt performance. | The hard work of some dedicated volunteers! Nowadays we have more people that get paid to work on pandas which has certainly helped to sustainably manage the growing influx of issues, but we still rely on volunteers a lot as well to fix bugs, triage issues, review, etc. | Awesome - please check the contributing guide https://pandas.pydata.org/docs/dev/development/contributing.html | I don't know about pyspark csv reader, but pandas 2.0 shouldn't perform much differently for reading than pandas 1.5. Did you try using pandas.read\_csv(enging='pyarrow')? That should help, you can read more about it in this blog post I wrote: https://datapythonista.me/blog/pandas-with-hundreds-of-millions-of-rows | pandas should be much faster than PySpark on smaller data because of the amount of overhead when using PySpark. But if you are reading many CSVs with a lot of data, I think PySpark will overtake pandas as far as performance. | Just continue using pandas, and when you see something that could be improved (maybe clarify something in the documentation, add an example to a function that doesn't have it...), just go for it. If that doesn't happen, as Marco said, the best if to try to find a ""good first issue"", but when I create one, they're usually taken care of in hours. | Once you've got a dataframe, your data is already into memory. I guess by ""on the fly"" you mean out-of-code, when the data is read from disk or other I/O, and while is being loaded into memory. This can surely be done, but there is no easy way to do it, or a standard pandas way to support it. I guess what it can make more sense is to monkeypath the connector you're using, and transform (encrypt/decrypt) the data at the right time doing the import/export. | If I'm not wrong, we're adding second resolution in pandas 2.0. With second resolution and 64 bits I think you can represent from the big bang until the end of the universe. ;) We also support Arrow dtypes, I should check what are the exact types they provide for datetime. So, no plans for day resolution if Arrow doesn't provide them, but you may not need it, since second is likely to be enough. Feel free to open an issue if we missed a use case that wasn't considered when the decision to only support second and not day was made. | To be totally honest, pandas plotting isn't the best maintained part of pandas. I'd really like to take it out of pandas have it live as a separate package, and hopefully some community of users could help maintain it - but I have yet to make a concrete proposal or action plan in this respect | You can deal with high(>2)-dimensional data using pandas MultiIndex in your DataFrames. Are there pain points when doing so in your experience? | >Dear Pandas, please make a universal UTF-8 translator for tabulated data.

Could you elaborate? | I won't believe anyone who tells me they know even 10% of pandas ;) | I'm not sure if there is already support for all Arrow complex types in pandas 2.0, but we have some support of lists for sure, and I think structs too. For the bigquery part, I think you can ask this to the developers of this repo: [https://github.com/googleapis/python-bigquery-pandas](https://github.com/googleapis/python-bigquery-pandas) We basically wrap that library with the read\_gbq() function. but there is not much big query specific in pandas other than that, so not much idea. | I'm not sure if Arrow structs are already supported in main, I think they probably are, but you won't have operations to do with it. I'm working on a blog post to show how to implement your own operations in Rust for pandas if you're using Arrow, maybe that can be helpful.

I guess using the builder/fluid pattern would be helpful in many places of the pandas API. But we assume users in general prefer that we don't break their code, and that they don't need to relearn pandas at every release, so we keep changes to the API very limited.

We've got the \`query\` function to filter rows (\`where\` would probably be a better name for it). Is that what you're looking for? | He's been working in Arrow for a long time. Until couple of years ago he was still active in some discussions, but I don't think he's doing much in pandas directly right now (of course his work in Arrow has a huge impact in pandas). | We are aware of pandas being slow and not memory efficient in many instances, compared to what it could be. The fixes aren't usually trivial, but huge. But we're surely moving into that direction, and it's a priority for the project. A lot of work is being done avoiding unnecessary copies of data when operations are performed in pandas. Using Arrow should help making things faster in the future too. But everything in pandas development is slow, so you'll see some gains, but rarely any dramatic improvement. | I think Python for data analysis by Wes McKinney, the original creator of pandas is a good start. I haven't read other books about pandas myself, can't tell, sorry. | I have never read a book on pandas, so I can't recommend. I myself learned pandas through reading many tutorials. You can find a list of these (which is by no means complete) in the pandas docs: [https://pandas.pydata.org/pandas-docs/dev/getting\_started/tutorials.html](https://pandas.pydata.org/pandas-docs/dev/getting_started/tutorials.html) | It's hard to say without knowing what validation you're doing, but my experience is that even very complex operations can be vectorized. If you are able to do this, whether with pandas or something else, you'll experience significant performance benefits.

You mention not using aggregation, but pandas can also be very efficient at reshaping data - though I don't know if you might have a use for that in your ETL.

One thing to potentially look into is using numba: https://pandas.pydata.org/docs/user\_guide/enhancingperf.html#numba-jit-compilation | You make some very good points. pandas was designed as both an ETL tool, and a data analysis tool. I wrote [an article](https://datapythonista.me/blog/pandas-the-two-cultures) about it long time ago. But summarizing, it won't ever be able to master both. And feels like many decisions were made more for the data analysis tool.

For an ETL tool I'd expect things to never fail silently, type conversions never happen automatically, schemas have to be provided and not inferred...

For your use case, pandas may be an overkill. DuckDB could be a nicer option if your validations can be vectorized. But if you're going to iterate row by row, I'd personally just write your ETL in Rust (may be trickier for the xlsx, not sure if there are good libraries for it). | The best way to tell would be to install the release candidate and see:

\`\`\`

pip install -U --pre pandas

\`\`\`

&#x200B;

And if you do come across any bugs, please do report them so we can fix them before the final 2.0.0 release | Not really, pandas 2.0 is actually pandas 1.6 with a fancy name. ;)

The main thing is that you need to take care of any FutureWarning in 1.5.3 before you migrate, and more than 99% chance you'll be just fine. :) | I don't believe there is currently anything on the road map. pandas does have window functionality, and though different from PySpark, can accomplish a lot of the same things. Do you find certain operations are difficult with pandas but easier with PySpark to code? | Yup, you can use https://github.com/pandas-dev/pandas-stubs | You can test that out today! [https://pypi.org/project/pandas/2.0.0rc0/](https://pypi.org/project/pandas/2.0.0rc0/)

To answer your question though, there is quite a lot. If you are using pandas 1.5.3 and seeing an FutureWarnings, that is something that will change or break when you switch to 2.0.

You can see our release notes here: [https://pandas.pydata.org/pandas-docs/dev/whatsnew/v2.0.0.html](https://pandas.pydata.org/pandas-docs/dev/whatsnew/v2.0.0.html) | Any DeprecationWarning and FutureWarning. Also, run your test suite with pandas 2.0 RC, it's already available. :) | It depends. If you have a website with millions of daily users, probably good if you can avoid pandas, since it has pretty high memory consumption for what it does, and it can be slow in some operations too.

That being said, pandas is used by millions of users, has a massive test suite, and it's unlikely that anything breaks, or there is any important bug. In that sense is surely fine to use in production. | I don't think that's true - and to be able to respond better it'd be good to know *why* it's thought that pandas isn't good for production. Because it's open source? Because it has bugs? Because the API is evolving?

We use pandas in production all the time where I work. | pandas is a wrapper on numpy. Every column of a dataframe is a numpy array, so no way to get rid of it.

That being said, for the long term we may depend on Arrow, and maybe numpy may not be needed. But not something you can count on having soon.

More than the numpy dependency, I'd probably try to find alternatives to using pandas in a web application that requires more than one process. It may be a waste of resources regardless of the numpy limitation. | pandas isn't a pure-Python package; we also have Cython and C code that needs to be compiled. This means to work on pandas, you need to be able to build pandas from source. This can be a difficult process, especially dealing with error messages when things go awry. Also, maintaining a large codebase like pandas requires significant tooling in the form of linters and type-checkers and pandas uses these extensively. 

It can be a lot of learning for a new contributor, but in my experience it pays off. A lot of what I've learned about Python development has been thru working on pandas. | Really depends on what you are doing. pandas is great if your amount of data isn't too large. It's mature, better tested our API is more stable. Also, we have lots of convenience features build in (this could be a positive or a negative ![gif](emote|free_emotes_pack|grin)).

If your data outgrow a certain size, then you are probably better of switching to another tool, there are many out there, Polars, Dask, ...

Right now, I'd be more likely to use pandas in production. Polars is still pretty new, so it does not have the support and integration that pandas has right now. This will probably change over time though. | It depends. If you are starting, you care about speed, and don't mind having to update your code every time you upgrade your dataframe framework, I'd probably give pandas a try.

If you care on something well tested, more stable and with more people understanding your code, for now pandas is probably a better choice. | I highly recommend learning to work with a MultiIndex. They enable many performant operations like joining and taking [cross sections](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html). | Assuming you care at least a little bit about performance:
Pandas and numpy are vectorized and the actual operations happen mostly in C and cpython which makes it way faster than anything you could do in only python. If you would implement this in c, then you‚Äôd build a new numpy üòÄ
This is a really short answer but hope that you get the gist of the difference | PRs are welcome if you've got a suggestion!

The Excel interface can be touchy because it goes through openpyxl (and previously xlrd) so in some cases there isn't much that can be done within pandas.  But we never know until we try! | This is literally type of stuff I dream of doing. 

Question, what made you start the pandas library? Did you envision the entire world using it before it happened? | What was the funding process like getting started? In my area of work (science research) it seems like funding only comes in for a project after you‚Äôve done the majority of the project. Was there a plan for getting Pandas funded or did the project grow organically until you realized you could get funding for it? | Year-month-day is already the default - even if your input is some other format, once parsed by pandas, it'll be displayed year-month-day:
```
In [2]: to_datetime(['01/01/2000'])
Out[2]: DatetimeIndex(['2000-01-01'], dtype='datetime64[ns]', freq=None)
``` | Would it be too silly to have a button on your doc page that generates a random function for a user to ‚Äúdiscover?‚Äù

‚Äîlong time pandas user. Super super appreciate of all that your team does. Thank you for all that you do! | I basically live in the pandas docs whenever I use it. I think the library optimizes too much for readability. Whenever I look back on pandas code I've written, the solution is concise and elegant and easy to understand, but it disguises how long it took me to get to that small chunk of code. | I love hearing this! At times I find myself wondering how much our users are utilizing our documentation (especially when compared to some of the great pandas tutorials that are out there). Hearing things like this makes me much more motivated to spend effort there. | i don't think the ""too much stuff"" issue is because a lot of the ""stuff"" is cruft that should be removed. Rather, I think the issue is mainly that the API lacks organization. It's like the difference between trying to find a particular lego piece in big box of assorted lego parts vs trying to find that same piece when the parts are organized into separate shelves like at the lego store. 

you've sort of become victims of your own success: as another pandas dev mentioned, you want to preserve backwards compatibility and this significantly complicates any restructuring. I'm sympathetic and am not sure what the best solution here would be. I had [this idea](https://github.com/dmarx/bench-warmers/blob/main/pandas_wrangler.md) last night but i'm not sure I like this approach either. | Thanks so much for the answer, and again, appreciate all the work you and others have done to make pandas such a great tool! | 6 years working with pandas I still have the docs open every day for simple things. And especially for all those long to wide and wide to long (unstack, stack, pivot etc...) transformations. | Yeah I'm pretty new to pandas and also new at my job. We do pricing optimization using ML so we got a new client that I was assigned to, and I basically had to take a huge drop of their historic data and analyze it to inform configuration decisions and validate the data's accuracy and soundness. 

I had used Python before but never pandas. Decided this was the time, because frankly the data was dirty as hell and the thought of trying to do what I needed to do in SQL seemed super tedious. I decided to use jupyterNotebooks with pandas and it is an AWESOME combo. 

In addition to what you said about being able to check your transformations stepwise, you also don't have to read in the entire dataset each time you want to make a change and check the results. It was just really easy and fast to organize my thinking and make changes here or there and quickly see the results. Made pretty quick work of some transformations that would've taken me quite a while using only SQL. | Dask actually opens up a question I have. Some open-source projects like Pandas have seemed to figure out a good cadence for features vs bugs and accepting PRs. Some, like joblib and Dask and their role in sklearn, have remained pretty rough around the edges on their process and evolution.

So my question is, other than simply more funding, is there something about the culture/ethic/process for Pandas that makes it all work out and that other FOSS projects could learn from? Or in your experience really does monetary support become the bottom line on how things turn out? | Does that also mean that Polars and Pandas won't be able to share an in-memory Arrow object? I think Polars is using Rust. But it will be faster and more reliable to convert data between the frameworks? | Late to the party, but I totally agree that row index should not be there by default. IMO it breaks the Python principle of ""there should only be one obvious way of doing something."" This makes it confusing for new users, since there are many ways of indexing (plain \_\_getitem\_\_/\_\_setitem\_\_, .iloc, .loc, .iat, .at), and for someone new it's not clear which way is the ""best"" way of doing a certain indexing operation. It also makes it non-obvious how setting elements works: for example, if I assign to the column of a dataframe a pandas Series with its own index, and the dataframe's index does not match the row number, does it use the Series Index or the integer position to set the elements? Even more confusingly, the index is not necessarily unique, so what happens if there are duplicate indices then? | OK, thanks! IIRC, pandas was originally inspired by R \`data.frame\`s, so I figured the devs might keep a sharp eye on what's happening on the other side of the wall. | Awesome. I get scared when I see the red future warning dialog box in jupyter labs. Thanks for everything you guys are doing. Pandas is amazing - I use it most days and always enjoy learning new things. Can't wait to explore 2.0! | I have just read the 2.0 blog post, I think using arrow is a better way than rewrite the whole pandas core too. 

I have already using arrow engine when I fire up pandas and spark. Looks like it won't be much changes. I will be waiting for the release. | Apologies, I mixed up the order of the operations. See the code below, doing something similar as a `.groupby()[""colummn""].transform()` with a `.groupby().rolling()` feels clunky at the moment. Below are the 2 methods I know of to extract the desired results, ensuring it has the correct index to assign it to the original DataFrame.

```
import numpy as np
import pandas as pd

# example DataFrame with 3 columns: date, id and a random value
dates = list(pd.date_range(start=""2019-01-01"", end=""2019-12-01"", freq=""MS""))
length = len(dates)
n = 2
ids = sorted(list(range(n)) * length)
values = np.random.randint(low=0, high=10, size=length).tolist()
df = pd.DataFrame({""date"": dates * n, ""id"": ids, ""value"": values * n})


# groupby transform
df[""max_per_id""] = df.groupby(""id"")[""value""].transform(""max"")

# similar expression for groupby.rolling
df[""rolling_max_per_id_v1""] = df.set_index(""date"").groupby(""id"", as_index=False)[""value""].rolling(window=3, min_periods=3).max()[""value""]
df[""rolling_max_per_id_v2""] = df.groupby(""id"").rolling(window=3, min_periods=3, on=""date"")[""value""].max().set_axis(df.index)
``` | Thanks for the response! I‚Äôll try to generate a representative example of once I get back to my personal laptop and share the performance of R vs Python. I am not doing anything more complicated than aggregating a bunch of columns. I already forego pandas‚Äô more expressive but slower apply() syntax, which I think is the closest in functionality to what R provides. | Yeah same here, I used pandas in more or less all f my production applications. | Also, numpy is a fundamental package in the PyData ecosystem that almost everyone relies upon in some way (PyArrow depends on it as well). So even if pandas didn't depend on numpy, we would still need to implement a lot of things that numpy does (or rely on another package), and that doesn't necessarily make it compatible with mod\_wsgi.  


The main issue here is that mod\_wsgi uses ""subinterpreters"", and there are very little packages out there with C extensions that are compatible with subinterpreters. | Thanks u/datapythonista

In this case, the only simple solution was to do what we were doing without Pandas, as you suggest.

Just a thought, it could be possible to release a ""pandas-web"" package that provides an identical/similar interface to the normal pandas, but in a ""web-safe"" way so that it can be safely used within web applications.

In this case, when wanting to use pandas we weren't concerned with the speed, it was more about the ""ease of use"". So even if it was doing everything in pure Python without some C backbone, AKA was slower, we still would have used it for this case. But now, we simply have no choice except to not. Food for thought, thanks for your response. | U wrote pandas in both stuations üò≠ | Great, thanks! I'll turn that on in my notebooks and see what happens.

Research papers have a natural lifecycle of a few years, typically with a lot of work up front. So, I generally suggest that they start with the newest versions, pin them, and upgrade if needed (usually for a feature or bugfix) while looking out for errors or data changes.

That gives stability/reproducibility within projects (i.e. papers), and upgrades across projects, as newer projects start and older ones are (hopefully) published.

That's not to undercut the value of upgrading projects unnecessarily as a form of procrastination (also known as my plans for some day shortly after 2.0 release).

As an aside, it's really great to see the Arrow stuff coming to fruition in pandas (and over in polars, too). I've been around long enough to remember pandas2/10 things, and the excitement around what the future could look like. A huge congrats to you all for getting a big part of that over the line. | My apologies, its been a couple months, but I believe group_by operations were often the culprit. I never felt like I got the group_by -> aggregate -> group_by -> aggregate workflow figured out, which I use constantly to summarize replicate data in an experiment and then summarize at the treatment level. This was probably a me problem though!

It may stem from me having started with R-tidyverse, which as you likely know shares much of the same namespace, but often operates just differently enough to make transitions between tidyverse and pandas just a bit of a headache. | pandas is built on top of numpy | When I started using it pandas was already very popular. There were many things about pandas I didn't like, so I just started fixing them. In particular, the API reference was quite poor at the time, many undocumented things, most functions didn't have examples, lots of format inconsistencies... Then other things too, and here I am, still trying to fix pandas. :) | For many years there was only the support of few companies letting people work on pandas as part of their job, and small personal donations via the NumFOCUS website. That money helped cover small expenses like CI services.

The main difference came with CZI, who started supporting open source software used in biology. We got funding to start paying for hours of maintainers with it. Also Tidelift provided monthly payments in exchange to implement small practices, like having a standard (and not customized) license, and providing a way to report security vulnerabilities. We got some other funding, and now more maintainers allowed to work on pandas as part of their job, but the situation is good mainly because of that particular funding. NumFOCUS provided some funding to for specific projects (with the money that comes from general NumFOCUS sponsors, and PyData conferences). | I think this is a fantastic idea, but I'd rather have this implemented as a separate website (happy to link it from the official website, just ping me on github if you ever do it). We've got intersphinx setup afaik, that should make it easy to get the pandas API available to you via a webservice. | That's great to hear. The large number of unique examples that can be found in the doc strings is extremely useful and an area Pandas excels over other libraries.

Not just visiting the website, I use the docs from within VS Code _all the time_. | [heard](https://www.reddit.com/r/Python/comments/11fio85/we_are_the_developers_behind_pandas_currently/jaldfv6/) | Funding is surely an important factor. But even with unlimited funding, there are many things that pandas wouldn't change, even if they're considered to be wrong. When we make decisions, we consider what's the impact on users. pandas is very popular and used in many critical applications. If we focus in features more than bugs, and those imply changing how things work, there is a big impact for users. Imagine we do with pandas what Python did with Python 2/3. We would have projects taking years to migrate...

Projects that are starting like Polars are more free to change things. So, any mistake pandas did they could fix, as well as any mistake they make themselves. This is good since you can improve things much more than pandas. And it's bad since you don't want to use Polars in production, unless you want to rewrite your code every month. I think that's how things need to be. pandas will serve the existing users, and if very innovative things can be done in the dataframe space, it'll be for some other project to implement them. | Good point. I was probably not very clear. The language shouldn't affect sharing the data, the language affects sharing algorithms. In some cases, it's not a problem even from different language, interfaces named FFI allow Python or Rust to call C for example. But I'm not sure about FFI and languages running on a JVM, maybe that's more challenging.

About pandas and Polars, I think since Polars released a new version recently, the example at the end of this blog post about [pandas 2.0 and Arrow](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) should already happen copy-free. :) | Getting rid of your FutureWarnings is a really good idea :) So I applaud you for that. Generally, we wanted to get rid of all the deprecations we introduces since 1.0, so we had to do 2.0 at some point. If your code is free of FutureWarnings then you are good to go. We made some backwards incompatible changes, but not many and they are clearly documented in the release notes.  https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#backwards-incompatible-api-changes | I think this is an interesting question! I've opened https://github.com/pandas-dev/pandas/issues/51751 | There is a typing effort that is led by some core members (unfortunately none of them takes part today). You can check the stubs package out at [https://github.com/pandas-dev/pandas-stubs](https://github.com/pandas-dev/pandas-stubs). I am not really familiar with the progress there | I might be missing something, but where and mask should work for you?

[https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html#pandas-dataframe-where](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.where.html#pandas-dataframe-where)

As a side note: Copy-on-Write should give you a nice speedup when doing chaining like this :) | It'd depend on the exact operation (not sure what benchmark exactly you're referring to).

If you do a simply \`my\_series.sum()\`, that would delegate to the pyarrow.compute.sum function, which exposes a kernel. If Arrow doesn't provide a kernel (string operations for example), then we need to implement them ourself (or use a library that does, but there are not many yet afaik). For strings, I think we implemented them in C++, but other options like Cython or Rust exist. I'm writing a blog post about writing pandas/arrow extensions with Rust, hopefully I'll be publishing in the next couple of weeks. | There isn't a direct way of getting credit, none of our features are assigned to a person that implemented it.

There is our cheat-sheet though that lists the original author:

https://pandas.pydata.org/Pandas\_Cheat\_Sheet.pdf | Do you mean on large data? This is hard for us because of how pandas is build. Dask would be a better alternative if your data don't fit comfortably into memory.

Small and medium sized dataset: Copy-on-Write should help a lot in improving performance if applied correctly (I will write a post explaining the details a bit more soonish that will help with that). Also, using Arrow as a backend get's us a performance boost in operations where Arrow is doing really well (string manipulation for example). Hope this answers your question. | Pandas 2.0 enters the room... I think that's changing progressively to not be the case anymore I'm favour of Arrow. But I don't understand it enough. | That's a decision that needs to be made. I see your point, and mostly agree, but there are always implications. numpy does more what you're saying, and they have a pretty big namespace for the \`numpy\` module (much bigger than pandas.DataFrame). scikit-learn is more modularized, and the structure probably makes more sense, but then you require lots of imports, which could be annoying for people doing exploratory analysis with pandas.

Also, pandas pipelines can be expressed nice with method chaining (e.g. df.query(cond).sum()...). If we move things outside of DataFrame we break that API, which many users find convenient.

I think it requires careful analysis to see all the implications of any approach, since I don't think there is an obvious good way of implementing the pandas API. So, I agree with your comment, but it's not obvious to me where to draw the line. I think an io namespace for DataFrame could make sense, but other than that, I have more questions than answers on what would be the API that maximizes the benefits and minimizes the costs. | Lmao my visits alone would‚Äôve netted y‚Äôall a fortune. I don‚Äôt want you to take this as a dig on pandas in any way btw, I probably wouldn‚Äôt be where I am in life without it!! Obviously there‚Äôs newer libraries that don‚Äôt have as much of a history which have been able to make ‚Äúcleaner‚Äù API decisions because they don‚Äôt need to avoid breaking changes, but in a way that is a positive reflection on pandas, that there‚Äôs such a wide variety and background of users that interact with pandas in their own diverse ways. And the docs are so amazing that I barely ever have to go anywhere besides them to get what I need. | Oh, I practically live on the pandas doc website and never stumbled on that where method. How curious! | Sorry for the delay.  I pulled a common dataset at work, it had about 15M rows, and 10 category columns with about 150K unique groups.  I put the code below but in R:

Just counting: 0.54 sec elapsed  
Basic Stats: 0.85 sec elapsed  
A more involved calculation: 1.38 sec elapsed

vs python:

Just counting: 10.050905704498291  
Basic Stats: 8.346287250518799  
A more involved calculation: 83.47735095024109

&#x200B;

\####################

\## R CODE

\####################

library(data.table)  
  
NUM\_OBS <- 15 \* 1000 \* 1000  
NUM\_CATEGORIES <- 150 \* 1000  
  
make\_fake\_category\_column <- function(n\_choices, n\_obs) {   
  sample(sprintf(""Category: %d"", 1:n\_choices), n\_obs, replace=TRUE)  
}  
make\_fake\_value\_column <- function(n\_obs) {   
  runif(n\_obs)  
}  
  
df.categories <- data.table(  
  idx=1:NUM\_CATEGORIES,  
  g0=make\_fake\_category\_column(100, NUM\_CATEGORIES),  
  g1=make\_fake\_category\_column(500, NUM\_CATEGORIES),    
  g2=make\_fake\_category\_column(25, NUM\_CATEGORIES),    
  g3=make\_fake\_category\_column(100, NUM\_CATEGORIES),    
  g4=make\_fake\_category\_column(5, NUM\_CATEGORIES),    
  g5=make\_fake\_category\_column(20, NUM\_CATEGORIES),    
  g6=make\_fake\_category\_column(10, NUM\_CATEGORIES),    
  g7=make\_fake\_category\_column(5, NUM\_CATEGORIES),    
  g8=make\_fake\_category\_column(150, NUM\_CATEGORIES),    
  g9=make\_fake\_category\_column(10, NUM\_CATEGORIES)  
)  
    
df.data = data.table(  
  idx=sample(1:NUM\_CATEGORIES, NUM\_OBS, replace=T),  
  val1=make\_fake\_value\_column(NUM\_OBS),  
  val2=make\_fake\_value\_column(NUM\_OBS),  
  val3=make\_fake\_value\_column(NUM\_OBS)  
)  
  
df.data <- merge(df.data, df.categories, by=""idx"")  
df.data\[, idx:=NULL\]  
  
speed\_test <- function() {  
  tictoc::tic(""Just counting"")  
  a1 <- df.data\[, .N, by=list(g0,g1,g2,g3,g4,g5,g6,g7,g8,g9)\]  
  tictoc::toc()  
  
  tictoc::tic(""Basic Stats"")  
  a2 <- df.data\[, list(sum\_v1=sum(val1), mean\_v2=mean(val2), max\_v3=max(val3)),   
by=list(g0,g1,g2,g3,g4,g5,g6,g7,g8,g9)\]  
  tictoc::toc()  
  
  tictoc::tic(""A more involved calculation"")  
  a3 <- df.data\[, list(s1=mean((val1\*val2)\^2-val3)),   
by=list(g0,g1,g2,g3,g4,g5,g6,g7,g8,g9)\]  
  tictoc::toc()  
}  
  
speed\_test()

&#x200B;

&#x200B;

\############ 

\## PYTHON

\#############

\#!/bin/env python  
  
import numpy as np  
import pandas as pd  
import random  
import time  
  
NUM\_OBS = 15 \* 1000 \* 1000  
NUM\_CATEGORIES = 150 \* 1000  
  
  
def make\_fake\_category\_column(n\_choices, n\_obs):  
labs = \[f""Category: {n}"" for n in range(n\_choices)\]  
return random.choices(population=labs, k=n\_obs)  
  
  
def make\_fake\_value\_column(n\_obs):  
return np.random.uniform(size=n\_obs)  
  
  
df\_categories = pd.DataFrame(  
{  
""idx"": range(NUM\_CATEGORIES),  
""g0"": make\_fake\_category\_column(100, NUM\_CATEGORIES),  
""g1"": make\_fake\_category\_column(500, NUM\_CATEGORIES),  
""g2"": make\_fake\_category\_column(25, NUM\_CATEGORIES),  
""g3"": make\_fake\_category\_column(100, NUM\_CATEGORIES),  
""g4"": make\_fake\_category\_column(5, NUM\_CATEGORIES),  
""g5"": make\_fake\_category\_column(20, NUM\_CATEGORIES),  
""g6"": make\_fake\_category\_column(10, NUM\_CATEGORIES),  
""g7"": make\_fake\_category\_column(5, NUM\_CATEGORIES),  
""g8"": make\_fake\_category\_column(150, NUM\_CATEGORIES),  
""g9"": make\_fake\_category\_column(10, NUM\_CATEGORIES),  
}  
)  
  
df\_data = pd.DataFrame(  
{  
""idx"": random.choices(population=range(NUM\_CATEGORIES), k=NUM\_OBS),  
""val1"": make\_fake\_value\_column(NUM\_OBS),  
""val2"": make\_fake\_value\_column(NUM\_OBS),  
""val3"": make\_fake\_value\_column(NUM\_OBS),  
}  
)  
  
df\_data = pd.merge(df\_data, df\_categories, on=\[""idx""\])  
df\_data.drop(columns=\[""idx""\], inplace=True)  
  
def speed\_test():  
t0 = time.time()  
a1 = df\_data.groupby(  
\[""g0"", ""g1"", ""g2"", ""g3"", ""g4"", ""g5"", ""g6"", ""g7"", ""g8"", ""g9""\]  
).agg(N=(""val1"", len))  
t1 = time.time()  
print(f""Just counting: {t1-t0}"")  
  
t0 = time.time()  
a2 = df\_data.groupby(  
\[""g0"", ""g1"", ""g2"", ""g3"", ""g4"", ""g5"", ""g6"", ""g7"", ""g8"", ""g9""\]  
).agg(sum\_v1=(""val1"", ""sum""), mean\_v2=(""val2"", ""mean""), max\_v3=(""val3"", ""max""))  
t1 = time.time()  
print(f""Basic Stats: {t1-t0}"")  
  
t0 = time.time()  
a2 = df\_data.groupby(  
\[""g0"", ""g1"", ""g2"", ""g3"", ""g4"", ""g5"", ""g6"", ""g7"", ""g8"", ""g9""\]  
).apply(lambda df: pd.DataFrame({  
's1': \[((df.val1 \* df.val2)\*\*2 - df.val3).mean()\]  
}))  
t1 = time.time()  
print(f""A more involved calculation: {t1-t0}"")  
  
speed\_test() | This article should provide more information on why Arrow: https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i | If you need pandas' functionality, but use numpy instead, then you're going to have to reimplement the parts of pandas that you need. | NumPy is ""closer to the metal"" than pandas, and so you will often experience better performance with it. But pandas is also doing more for you, so it's not really a fair comparison. If you need column labels, I would always prefer pandas. If I'm doing linear algebra and matrix computations, I would always prefer NumPy. Depending on your work, ""time to code"" can be much more important than ""time to run"". | Thanks for the kind words. At some point we had a [global sprint](https://python-sprints.github.io/pandas/) with 30 participating cities, and more than 500 people, improving the API reference. The list of people to thank for that is huge. :) | Thanks, I saw it yesterday in hacker news and read it, what I meant to say is that it seems that numpy dtypes are still an option, so I don't know if numpy is going away from the pandas core eventually or if it will remain part of it for the foreseeable future. | Will there still be .to_numpy() that does not copy? I am using numpy swig bindings to plot pandas dataframes with a c++ library, be nice if it did not become impossible with the new version"
1cy9vpt,Speed improvements in Polars over Pandas,"I'm giving a talk on polars in July. It's been pretty fast for us, but I'm curious to hear some examples of improvements other people have seen. I got one process down from over three minutes to around 10 seconds.   
Also curious whether people have switched over to using polars instead of pandas or they reserve it for specific use cases. ","I've been using polars for everything I do nowadays. Partially for the performance, but now that I've learned the syntax I would stick with polars even if there were no improvements at all on that front. Expressions are just *that good* for me: I can build huge lazy queries that can be optimized, rather than having to figure out all the pandas functions and do everything eagerly. 

I have got to the point that if I have to work with some codebase that does not support polars for some reason, I'll still do everything in polars and then convert the final result to pandas rather than doing anything in pandas. 

The two things pandas does better than polars is styling tables and pivot tables. Pivot tables in particular are so much better with pandas, especially when I have to group by multiple variables rather than only one. | So fast. I use pandas only in legacy code nowadays or with co-workers that don't know polars.

I've also experienced better memory usage due to LazyFrame (which is even faster compared to standard polars DataFrame).

But the aspect I love the most is the API. Pandas is old, inconsistent and inefficient, even with years of experience I still have to rely on an ocasional Stack Overflow search to grab a mysterious snippet of code that somehow works. I learned full polars in about a week and only have to consult the docs because of updates and deprecations, given it's still in development.

With that in mind, pandas still has a lot of features that aren't present in polars, table styling being the one I use the most. Fortunately, conversion to/from polars is a breeze, so no problems there.

Overall, I see no reason to learn pandas over polars nowadays. It's easier, newer, more intuitive and faster. | I built a local web app in Dash that loaded data from a variety of systems and did an ETL for further analysis. The system was a behemoth (>1.2 GB in libraries) and underpinned by Pandas. Data loads would take roughly 5 minutes. Combine that with distribution issues, it never lived up to its potential.

I rewrote the basic ETLs to run from an embeddable instance of Python with Polars (~175 MB) that I call from an Excel workbook via VBA Macro.

The Polars code feels exponentially faster. The ""batteries"" are smaller, and now my colleagues are actually using it! 

The only trouble I've run into is date parsing. Pandas seems to do much better at automatically parsing the date regardless of the format, which unfortunately is one of the main things I need my code to do. I've built a UDF to coalesce a long list of potential formats, but it just feels a bit ""Mickey Mouse."" Otherwise, I've got nothing but good things to say about Polars. | Had a weird Polars issue using postgres when reading/writing from a database. Switched back to pandas and it solved the issue.¬† | We're basically parsing [SLURM sacct job details](https://slurm.schedmd.com/sacct.html) (a shared university HPC cluster, so *tons* of activity), the original script was using pandas. I re-wrote this process to polars, and got the runtime of \~30 minutes down to less than 3 minutes, while increasing time domain resolution from 5 minutes to 1 minute.

Lots of this gain came from using `scan_csv()` and `LazyFrame` while using... uh, I forget the term, but the expression syntax that uses the `|` pipe symbol?

The original script was pretty slap-dash, but my rewrite isn't that great either... exhibited by the fact I need to stay on `polars==0.16.9` - anything newer and it breaks in new and exciting ways that I can't be bothered to debug. | I think a big reason why it‚Äôs so much faster (besides rust concurrency, lazy evaluation, etc) is that polars was built in rust and then bound to Python, whereas pandas was written in Python with C bindings for the tough spots. Polars is just a more cohesive approach, and the ecosystem is set up in a way that each rust crate has many dependencies, and if any one of them makes a speed improvement, all the downstream packages have the ability to benefit by just creating a new release, and PyO3 takes care of all the interfacing. I‚Äôm writing a lot of rust for a library with Python bindings right now, it‚Äôs so easy it‚Äôs almost magical | I'm currently working on optimizing some code  at my job. I chose Polars and the transition has been smooth. With 10 lines of code was able to shave off ~10min on the runtime. Not even close to finished to. Trying to get the Quants to start writing new code in Polars instead of Pandas. I think once I'm done, they will be convinced by the results. | I tried polars a few years ago when designing some qa software and duckdb was still faster so I stuck with that. I'll have to revisit it and see if it has indeed improved.
Pandas does have a lot of legacy support for data that isn't structured as expected, and it's reliable. I had backup functions written in it and expect to continue that until I see stability equalized. | I had a script whose processing time went from 20min to 90 seconds,  i do use polars a lot nowadays but just to join or concat converted pandas dataframes and convert it back to pandas (my team mostly uses pandas).  Cant convert a lot of other scripts as most of them are multiprocessing based and polars doesn‚Äôt love being inside multiprocessing, i get memory bugs which completely kills the entire program

I‚Äôm one of the weird people who likes pandas api especially like adding a column or a single static value to a column. But pandas lately has changed too much behaviour to be okay in production for me and trying to get everyone on polars. | Does it play nicely with sklearn? 

I‚Äôve always hear good things about polars but I know pandas so well and a lot of my custom modules uses pandas datafrmae that I never found the use case to move to polars.

My understanding is that polars don‚Äôt do things in memory, but plenty of ML packages train in memory.  Any ideas how well polars play with ML packages? | I primarily swapped from
pandas to Polars for remote execution of distributed dataframes in Ray. Pandas was causing out of memory errors (and incurs a copy of the arrow backed dataset) but Polars doesn‚Äôt which makes handling TB sized datasets much easier.¬†
Additionally I had a custom apply function written in pandas which took 20min but takes 30sec in polars which is a significant improvement. | I switched everything to polars except the things it is missing that I have to switch back to pandas for.  However, it wasn't really for speed, but for the syntax. | Basically null for me! But really, we get some huge and pretty gnarly (read=dirty) flat files from vendors and pandas handles them with zero issue. I‚Äôve attempted to get polars to handle them with no success thus far. There are a few implementations where I‚Äôll get the files read in and cleaned up with pandas, then send it over to polars, but even then, I don‚Äôt really see a huge speed boost. 

And for what‚Äôs it worth, I‚Äôm not a hater, actually love rust and the ecosystem, but as a data engineer by day, my superiors would frown if I spent too much time tinkering with a library instead of just being productive. IYKYK!

Just my anecdotal experience. Grace and peace mi amigos. | When I read things about going from 3 mins pandas to 10 seconds polars; It makes me think that you did not really write good pandas code to begin with, its less of a advertisement for Polars. I am sure you could write bad slow code for polars as well. | In one of my project, we were using pandas library, but then after knowing of polars, we switched to polars library.

But it wasn't as simple as changing the import statement. Lots of syntax had to be changed which caused us trouble and many equivalent functionalities weren't present for the same in polars.

So we just made the reading the file functionality to polars, and then changed the dataframe back to pandas df, this helped us reduction in our execution time. | I use polars as my daily driver, and every code revision I'm actively replacing as much of my old pandas code as I can. 

I have a project that reads from two different tables, 6 csvs and two xlsx files and compiles everything into a single table that is then shaped and sent to accounting for vendor rebates and it takes around 15 seconds to run. It's only 5-10k rows at output but it's so much faster than when I tried the same thing in crystal reports with some of the joins taking place in pandas beforehand (10-15 minutes).

I have a 5-7 minute pandas script I'm eying at replacing with polars as well but I went pretty deep into the features - it is going to take a while to unwind that one. It parses a heavily formatted xlsx and extracts out po data to be fed into several other reports. Row count is high enough that excel hangs for 10ish minutes before I can even open the file.

Only thing I struggle with for it is getting it to read complex json without a parser class or function helping it but I have a similar struggle with pandas. | Originally I was writing all of my data processes in Pandas and I felt like I was wrestling with indexing, slow file reading (as our data sat on a network drive -- something out of my teams' control), and I also wasn't a big fan of the syntax.

I had heard about Polars previously but chalked it up to hype. However, once I took the time to test Polars on a new project out of curiosity, I saw how much faster it was performing than Pandas -- so much so that I rewrote all of my existing Pandas processes into Polars and gained better performance across the board. I don't miss Pandas whatsoever.

Now whenever there is a situation that comes up where I actually need to utilize a functionality available only to a Pandas DataFrame, I just do convert my Polars DataFrame to Pandas using to\_pandas(). Beyond niche utility, there is basically no reason for me using Pandas over Polars. Realistically, unless Pandas was to be rewritten from scratch, it just cannot compete with the performance of Polars out-of-the-box. The only thing Pandas has going for it at this point is that it is a mature library that has a high adoption rate across the industry. | I tried it but didnt find much speed improvement compared to pandas with multithreading. Didnt try lazy dataframes though | I loved Polars the couple of times I used it. But installing it in a way that works cross platform is enough of a pain in the ass that I've reverted to Pandas.¬†


With Polars, I can write my code on one machine, commit to git, then pull on another machine, and the entire thing breaks because of Polars. Most frequently, it happens in Jupyter notebooks, where simply importing Polars crashes the entire kernel.¬†


I've tried installing the package meant for lower end devices, I don't remember the name off the top of my head, but that leads to the same issues.¬†


I can't for the life of me find a way to reliably add Polars to my dependencies and have it ""just work"" the way that Pandas does.¬†¬†


I'm also looking more at Ibis, but I just keep coming back to Pandas for the same reasons.. it's familiar, there are no surprises between machines when I try to pip install -r requirements.txt, and it's ""fast enough.""¬†¬†


If I could get Polars to reliably install and run without error on any machine and inside notebooks the way I can with Pandas, I'd be using it for everything.¬† | Just wondering, pandas 2.0 brings the Arrow backend to pandas (over numpy), so do you still see a significant difference? Are there other important factors that make polars faster? | Yeah it‚Äôs not perfect. I‚Äôve had some trouble with typing where I‚Äôve had to switch back to pandas | This makes me think how pandas became 1.0 only a couple of years ago | I think Wes actually understands and appreciates what they are doing with Polars and would do the same if he could start over with Pandas | out of interest, which pandas behaviour changes have been most painful? | Can't you do this in pandas with chunking? | Sklearn is leaning towards changing the default from pandas to polars in their docs. https://github.com/scikit-learn/scikit-learn/issues/28341

Also pandas team has a new triager that just seems intent on closing as many issues as possible without caring a world for UX. It's a huge turnoff for me to continue contributing anymore. | Would you mind sharing this custom function? I would like to replicate your use case and compare between pandas and polars. | i think many people write bad pandas and then complain about it, but polars is faster and harder to write slow code | Disagree mainly because Polars has several performance features that are impossible to replicate in pandas such as lazy evaluation and the query optimizer (among several others). Thats a bit hand wavy of you imo. 

Ive worked with pandas for several years and polars with like a month or two and already my exploratory rough draft Polars scripts dominates pandas scripts written with multiple peoples input and optimizations. 

Even if its a git gud issue why would I even care if I can write faster code as a beginner without even trying that takes domain experts in pandas to reach similiar performance | Yes, sure. Say I have an example like this.

    df = pl.DataFrame(
        {
            ""sex"": [""M"", ""M"", ""F"", ""F"", ""F"", ""F""],
            ""color"": [""blue"", ""red"", ""blue"", ""blue"", ""red"", ""yellow""],
            ""case"": [""1"", ""2"", ""1"", ""2"", ""1"", ""2""],
            ""value"": [1, 2, 3, 4, 5, 6],
        }
    ).with_row_index()
    

With Polars I have to do this

    df.pivot(values=""value"", columns=[""color"", ""sex""], index=""case"", aggregate_function=""sum"")

`index` is required, even if I don't care about providing one. The result is also quite unwieldy because having all the combinations of values on one row rather than stacked becomes really hard to parse really quick if there are too many combinations.

    case	{""blue"",""M""}	{""red"",""M""}	{""blue"",""F""}	{""red"",""F""}	{""yellow"",""F""}
    str	i64	i64	i64	i64	i64
    ""1""	1	null	3	5	null
    ""2""	null	2	4	null	6

With Pandas I have

    df.to_pandas().pivot_table(values=""value"", columns=[""color"", ""sex""], index=""case"")

and I get

    color	blue	red	yellow
    sex	F	M	F	M	F
    case					
    1	3.0	1.0	5.0	NaN	NaN
    2	4.0	NaN	NaN	2.0	6.0

where I can reorder the variables in `columns` to get different groupings, and the view is way more compact and easier to read. Pandas' version is also much closer to what I would build with a pivot table in Sheets, for example.

I have been working with data that I had to organize across 4+ dimensions at a time over rows/columns, and there's no way of doing that while having a comprehensible representation using exclusively Polars pivots. I ended up doing all the preprocessing in Polars and then preparing the pivot in Pandas just for that. | Yes. There is much more difference than the way we hold data in memory (arrow). Polars has much better performance. Here are the benchmarks against pandas with arrow support.



https://pola.rs/posts/benchmarks/ | Apart from the benchmark, iirc pandas doesn't have a lazy API, which can both increase performance depending on the pipeline and make it possible to work with larger-than-memory datasets. | [hot dang it's it only been 4 y](https://pandas.pydata.org/docs/whatsnew/v1.0.0.html) | Exactly. It was a win win because SQL in general is much more accessible IMO for those getting started in programming and we are in the midst of a significant change to open source. We also have two fairly large SQL dbs in our org that service a few thousand employees, so all of that knowledge can be leveraged.
 I just went with it originally for pure performance, but then came to love the simplicity, especially with the pandas integration. | Most painful is easily the string nan, changing it from np.nan to 'NaN' was one of the worst things they did for performance, ditching the numpy core pandas got popular with is a sure way to lose popularity for the future. Nans should be nans, or nulls. NOT 'NaN' | Honestly I don't really know how to improve the representation while relying exclusively on the polars structs formatting. This might be the only case where I found pandas' multi-indexes useful. 

Given that the issue is specifically with pivot tables, maybe it's possible to get around it by modifying how the table is displayed? Something like a \`pivoted.compress()\` method that changes the table display to something closer to pandas' version, including the multiple levels. Note that I have no idea how hard this might be to implement (though I think it'd be easier to do than having a full multi-index interface just for that use)."
12ahvyk,Pandas 2.0 Released,[https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html),"[Datetimes are now parsed in a consistent format](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#datetimes-are-now-parsed-with-a-consistent-format) glad to see that changed, this has got me bad in the past. | So why shouldn't I switch to pandas 2? How hard is it to migrate a project? | I might play with it, but I'm in the process of moving all work over to Polars. I like that Pandas is moving over to Arrow, but it came a little too late for me. Curious how benchmarks compare. | Polars author here, Your work will not be in vain. :)

I did run the benchmarks on TPC-H: [https://github.com/pola-rs/tpch/pull/36](https://github.com/pola-rs/tpch/pull/36)

Polars will remain orders of magnitudes faster on whole queries. Polars typically parallelizes all operations, and query optimization can save a lot of redundant work.

Still this is a great improvement on the quality of life for pandas. The data structures are sane now and will not have horrific performance anymore (strings). We can now also move data zero-copy between polars and pandas, making it very easy to integrate both API's when needed. | if the update is 100% drop in its huge for me even though im meh on pandas purely because of the sheer quantity of *other people's* pandas code that is inevitable in every data job. | I'm in the same boat. The announcement about the 25x (or whatever it was) speed increase with Pandas 2 came literally the day after I finished moving my project to Polars (and realized huge performance gains from that). | - pyarrow backend support (instead of numpy)
- seamless conversion from pandas to polar without copy. You can use pandas for its flexibility, and polar for its speed without loosing time doing in-RAM conversions
- Numerous smaller QoL improvements for a cleaner API | Yes probably. I, perhaps naiively, assumed Pandas would choose one format and try to parse all dates with the same format.

I'm in the UK, so dd/mm/yyyy is the go to.

From what I remember Pandas was trying the US mm/dd/yyyy first, then if that failed, it would try dd/mm/yyyy, but because some UK dates look like valid US dates it ended up interpreting different rows in different ways. | Hey Ritchie!  Really impressive work.  That benchmark graphic is enlightening.

I don't mean this disparagingly but you seem to be doing a little marketing (for lack of a better term) in these Pandas 2.0 threads.  Could you share a little more about your grand vision for Polars and how it will fit into the world of data science?  Are there any use cases that you feel Pandas is particularly equipped to handle?  If so, are you planning on ""competing"" in those areas or are you currently more focused on the features that differentiate Polars (performance, multiprocessing, etc.)

I'm still learning and growing in my data journey so I'm trying to get a better grasp of the landscape as a whole. | These two comments confuse me a bit. What's better than pandas, as a broad data handling package? | I just want to steer information a bit with real world benchmarks. There seem to be quite some hyperbole claims about pandas performance being equal or faster to polars now, which is not true.

> multiprocessing

We don't do multi-processing, but multi-threading. Not to be pedantic, but the performance implications of this is huge. In multi-threading we can share data between threads, in multiprocessing this needs to be serialized/deserialized having huge latency and compute overhead.

Every process also has to have its data in own memory, so it also has a lot of memory overhead.

> Pandas is particularly equipped to handle

Pandas has more IO readers/writers, plotting functionality and handy interop with timeseries and indexes (something polars will not aim to do). | If breadth is important, still pandas. If speed and resource efficiency is important, polars. 

If you need breadth and speed/lite resource use, use both. They‚Äôre interoperable. | i should rephrase. i like pandas fine. i use it all the time, but im a data eng, and pandas is often far from the best tool to do data engineering with. it seems to many analysts and data scientists this is crazy talk. | I have seen your work in one of the pandas announcements and thank you for such a tool. One particular issue with pandas is that appending new data to dataframe slows with the every append. Is Polars better in this regard?

Also is there a determined date for R port‚Äôs CRAN release? | Interoperable *as of pandas 2.0 with the introduction of arrow in pandas. | I'm something of a data scientist myself, and yes it sounded like crazy talk lol. I'd never heard of polars though.

The only non-pandas shenanigans I get up to is doing my more large-scale filtering and joining in arrow before converting to pandas. | > One particular issue with pandas is that appending new data to dataframe slows with the every appen

Yes, polars appends are very cheap, but this should also solved in pandas 2.0 with arrow dtypes.

Arrow allows for `ChunkedArray` types. This means that data doesn't have to be contiguous in memory, instead we can append the data chunk to the list of arrays. As the memory slabs are copy on write, we can increment only a reference count instead of copying data.

So appending will not be `O(n^2)` anymore. Chunking is not a silver bullet though. Every random access now has an extra redirection, so sometimes there has to be a `rechunk` to contiguous data.

> Also is there a determined date for R port‚Äôs CRAN release?

I am not sure. The R support of polars is entirely picked up by the R community and @sorhawell in particular. You can get certainly more information on that repo: https://github.com/pola-rs/r-polars | Sounds like a pretty good way to do things tbh. I rely on much less elegant, hacky pandas code all the time. My only tip to people Ive worked with is always exploit whatever database/storage query system you have. Of course this depends on access and architecture etc."
1byhpm7,Do folks ever use Pandas when they should use SQL?,"I think I see this a lot.

I believe SQL, including like dataframe SQL packages, should always be used over Pandas when possible unless one is doing something Pandas is more suited to or unless someone requires broader Python control structures or features.

I am a big believer in sticking to SQL as a baseline data manipulation framework and only using other frameworks if SQL is lacking. One of the biggest ways to accumulate technical debt is solving problems in multiple ways that are not standardized that not all people know.","SQL and Pandas are two very different things. I would argue that anyone who sees them as substitutes is probably misusing both of them. | I see this kind of mentality from devs all the time. It‚Äôs very myopic. It‚Äôs easy to look at any one thing in a business codebase and tear it to shreds. There‚Äôs almost always a ‚Äúbetter‚Äù or ‚Äúmore correct‚Äù way of doing things. I think devs often don‚Äôt realize that this is way more obvious than they think it is.

The problem is these things don‚Äôt account for the realities of a business. For example, the background and experience levels of the developers who work there, imperfect decisions made in the past, whether or not doing it the ‚Äúcorrect‚Äù way has any meaningful benefits at all, etc.

These things typically fall outside the remit of developers who think this way, and so they‚Äôre dismissed as being unimportant.

But the reality is, if you are working somewhere that is using pandas in a way where SQL would be more efficient, then that‚Äôs almost certainly because the familiarity and comfort levels using SQL are low or nonexistent amongst the developers working there. I‚Äôm saying this from experience having worked somewhere where this was exactly the case.

A senior technical decision maker saw that we were running into performance and resource issues, and decided that we‚Äôd use duckdb instead. It was a total disaster. Performance plummeted through the floor, code was terrible, buggy, inaccurate. People started using duckdb in the worst way you could possibly imagine because it made sense to them based on what they were familiar with.

The ‚Äúbest‚Äù solution is only the best solution if the people responsible for building and maintaining it know how to use it. It‚Äôs easy for devs to massively underestimate how little other devs know about things that they are familiar with. You mention tech debt - I guarantee that the worst cases of tech debt will be ones where a dev has unilaterally decided to use a particular technology because it was ‚Äúthe best‚Äù approach, despite it being poorly understood by the rest of the org.

In my example, I had to spend months patiently training people on how to work with SQL, and also how to build a mental model of delayed execution frameworks like duckdb, in order to slowly limp the organization back towards being even remotely efficient. | I mean, Pandas isn't some obscure Python package. But as an experienced SQL data engineer, I tend to gravitate to SQL. That being said, it's good to diversify your data tools and skills when opportunity arises. | There's plenty of times that exploratory research is being done using local data sets in a way that SQL or another database would be overkill. Pandas also interfaces so well with numpy and other data science and ML libraries that it's just more convenient and efficient to begin trying out ideas with pandas. | Depends on the volume of data and the complexity of the query / transformation. Almost certainly, it will be a combination of both. SQL to filter and pull the information out, and Pandas to deal with anything more complex and transform the data for visualisation. Can it it screwed by using either technique incorrectly? Absolutely!

Generally, I would use SQL to filter and pull the information out, possibly group too. But I am normally looking to do multiple transforms. So I'll use Pandas and visualise with bokeh or seaborne. | Should use or can use? People use pandas when they really should be using numpy if they cared about xyz (speed most likely but maybe something else). Hell people use python when they should use C if they cared about flops. Hell why aren't you using cuda?

Human resources are finite and sometimes people just need a low friction way to explore data. We have tools for different jobs and it's up to you to decide what to use. Need to explore a new dataset? Pandas. Need to run something daily where cycles count? Do it in SQL. | I really appreciate how quick it is to work with data in pandas (pretty great for working on early stage ML and data science tasks), but yeah I would say that there is most likely an overdependency on it | SQL is generally better for ETL (extract, transform, load) operations because it is heavily optimized by the server to be fast for those. Pandas is better for EDA (exploratory data analysis) because it is consistent, readable, flexible, and as of 2.0, reasonably optimized.

The ideal workflow is to use SQL for joins and filters to get your data into a single table at the granularity you need for analysis, then to actually do your analysis in Pandas.

Using SQL to do complex analysis is every bit as inefficient and ugly as downloading 20 base tables and trying to join and filter them in Pandas.

As others have said, use the right tool for the right job. | Pandas allows you to do things very easily that might not even be possible with SQL; even if those things are possible with sql, they would invoke so much involved code and joins that they would be very slow or extremely tedious , if done in a rdbs/sql. 

I find sql-style dataframes (pyspark, datafustion, poalrs) hard to maneuver for certain operations and logic that R-style dataframes(like pandas) is built for. The original r-style dataframe was developed in the 90's in S, as a native class for analytics and statistics. There is a reason why it has stood the test of time.

Keeping data in python vm/pandas has the advantage of being easy to integrate with all sort of python ecosystems. Its not uncommon to dump your data into a dataframe after aggregating it down.

That said, there are a couple of options for running sql on a pandas dataframe like duckdb or dasksql. | It's a bit confusing what you mean. Pandas is a data frame and SQL is language. If you are accessing data via a database or something like that then you are probably going to be using SQL to make those queries. If you have a need to convert that data to a dataframe then you will need to use pandas (or one of the popular alternatives). They aren't really interchangeable in the way you're describing. | If you are comparing these two things, then you clearly don‚Äôt know how either are used in practice. SQL is a language for managing complex *database* queries, and Pandas is a Python library for analyzing data that has been curated and organized into a dataframe already. They are used together in practice, at different points in the pipeline. | You're not wrong, but you're talking about one extreme. And you are right that you should lean that way when possible.

But do not forget this: what matters is getting the job done. There are plenty of cases when you will just slurp all data in Pandas and do all work there.

Only a Sith deals in absolutes, young Padawan. | I've seen it used for ETL -- wouldn't recommend for something long lasting as handling schema changes and typing isnt great in pandas. | I can hear what you said when it pertained to ‚Äúwhere do I process the data?‚Äù

Many developers in my team misuse pandas. They just dump the data from a SQL database and handle the processing in Python. These devs tend to know neither pandas nor SQL well enough. What they wanted was row-by-agonizing-row loops in Python, then make thousands more calls to the DBs in row-by-row fashion. That model is a lot easier for them to comprehend. But it usually produces a bottleneck in production, simply because dev has much lower volume of data, or that dev runs on local PC and does not suffer from the network lag. 

The better developers tend to most of their processing in complex SQL queries before bringing the results to Python. Sending things over the network introduces latency and the CPU is many times faster than the network. They save Python and pandas for things that are handled poorly by a database (calling APIs, upload to S3, join a relational table with some log files, etc).

That‚Äôs just one specific use case where I think more SQL should have been better. Pandas is great and very flexible. But it isn‚Äôt meant for billions of rows the way a relational DB like PostgresSQL is designed to handle. | I do not even use pandas these days, as I moved to polars. But, for this discussion, they are interchangeable. But, pandas and SQL are not, at least, not intended to be. 

I use SQL for interaction (query and upsert) on (usually OLTP) databases that support an SQL like interface, e.g. MySQL, PostgreSQL, Amazon Athena, Google BQ etc. I write some reasonably complex queries (usually aided by SQL alchemy) to ingest data from those databases, and for that interfacing, very often SQL is the only sensible method. 
 

Pandas or polars, on the other hand, are helpful for table-like data structures (more colloquially, dataframes) that already exist in memory, loaded from files or even a result of an SQL query. 

Once I get the data, to do all kinds of processing, pandas offers a lot of convenient tools that are nigh impossible or very ugly to replicate via SQL alone. I work as a machine learning engineer, and it is far cleaner to feed data into a machine learning framework via pandas/polars than SQL. 

So, the idea is to use both for their strengths. | I've seen the opposite mostly, people using a SQL DBMS when they could do it locally,  more efficiently,  using Pandas.  Like joining two datasets, that were just collected from some devices, and filtering out stuff.  No need to build tables, load tables, then make SQL connection and query for data that is transient or otherwise not valuable to keep. | Can you give an example of using Pandas when SQL could be used instead? | It's a loaded question because for most people using SQL always means you're running the execution on a remote server. 

If we can remove remote databases from this (e.g., using a in-memory database), then you can ask if you prefer to write queries in SQL or in the Pandas API.

To that I say, whichever tool gives me the easiest way to solve my problem.

PS: The comparison should rather be about Arrow API vs. pure SQL. | I think you have specific usages in mind and it would help to describe them. As a data analyst, SQL is amazing for data extraction, data aggregation and some things with window functions like lag and lead. But pandas is much better at pivoting, melting, statistics, get_dummies, etc. There's place for both and general rule for me is to how fast it runs for automated tasks and how easy it is for me to get results for ad hoc tasks. I have reduced the time from 1h45min pure MySQL query to 20 min using combination of MySQL and pandas. | The data scientists at my work are terrible for this. They immediately reach for pandas for any sort of data manipulation or calculations which can be easily done in python. They will even query the db with the pandas library rather than using a db library like psycopg. | I use pandas way more than sql because I‚Äôm way more familiar with pandas and computationally it‚Äôs about the same. I do more medium data than big data. | Maybe it's just because I never used SQL enough to become a ninja, but my method has always been ""use SQL to get the subset of data you need from the DB, and then use Pandas or other python code to do complex crunching locally"".

I just base it on what I see as far as how much time things take, but seems like trying to do joins on large tables and other complex stuff in SQL is much slower.  But then that may have something to do with me not knowing the right way to do it efficiently in SQL.  Data analysis is kind of a side gig for me so I'm sure I'd blush to show my code to any pandas or SQL experts. | Maybe you just like pain and suffering but I refuse to use SQL for any complicated data transformations. I will use either Pandas or Polars depending on how I feel but definitely not SQL.

For starters, SQL has multiple dialects with often widely different syntax. Maybe your company has things consistent but I don't want to jump around between Oracle, MS SQL server and Postgres and having to constantly look up dialect-specific syntax.

Also, errors in SQL in general are terrible so if you're trying to do something complex and you get an error, you're shit out of luck.

You can't debug SQL. Honestly, you can't do anything. 

At the end of the day, if you want to be terrible at your job go ahead and use SQL for everything, but I choose to use the tool that's most suited for the task. | Data science is often iterative. Pandas isn't an OLAP engine. So it runs everything imperatively. You should use polars for lazy execution, which will internally optimize an execution plan and run it when needed. | >I believe SQL, including like dataframe SQL packages, should always be used over Pandas when possible unless one is doing something Pandas is more suited to

This is basically it.

I use SQL to bring the data as close to it's final form as possible before i load it into python, \*within reason\*.

In-memory arrays are easier to work with and can be much more performance than SQL - Sometimes I have some esoteric where clause that would require a half dozen CTEs, union and partition by statements, and sometimes a massive bespoke index, only to exclude like 0.1% of the total dataset - in those cases I just bring the whole table across and do the filtering in python. | You can drive yourself crazy doing any analysis or manipulation of data without using pandas or a similar package. Python wasn't made for it. Very hard to drive a screw with a hammer. | I use Pandas for metadata, PySpark for actual data. | Use the right tool for the job.  Pandas sql-like functions tend to suck.  Yes, on occasion it makes sense to do them in Pandas, but in general anything that has a join in it is better in SQL. | I really hate writing sql code inside my python code. Just easier to use pandas.
There is polars that has an api kinda similar to sql | In the electronics industry their can be a lot of hierarchical data, with deep and varied structure. Pandas and SQL are limited in their support for hierarchy. | We reached the conclusion we are not good enough developers to write good code using pandas because our colleagues end up using it in the interfaces (as opposed to in the implementations of methods and functions) and this causes great maintenance issues when we lose track of the type and semantics of each column. Other teams have since adopted pandera and found that it is sustainable. We are now investing in SQL and SQL alchemy and pydantic as our priority is being in control of the data model. | Is your point that if you can help it query data in a format that suits your needs before loading it into python? If that's the case I kind of agree with you.

But generally most of the things you do in Pandas can't be done in SQL. Also what are these data frame SQL packages? | Yes, I‚Äôm those folks, I do most of the transformations in pandas, only use sql to fetch data or join tables and fetch data.

While sql is popular everywhere else my team is filled with scientists who know pandas way better. Also a ton of things around timestamp and datetime manipulation, boolean indexing and loads of other stuff is fucking great which is not possible with sql unless you‚Äôre a wizard. | Depends if the SQL is being sent to a shared database and if it is complex enough to eat up resources that it impacts other people.

If so, I'll pull some of the easier stuff into pandas and do the more complex stuff on my machine. | SQL to trim and join the data, Pandas to transform and analyse. For me, anyway. | I like Pandas for prototyping, but I wouldn't use raw SQL in production services.

SQLAlchemy queries broken down into unit testable chunks, added to a library of useful functions is way better than raw SQL IMO.  Great big cobs of untestable SQL in strings or separate files is often worse than dataframe magic, at least you can compose functions that take dataframes and write unit tests for the logic. SQL is generally integration tests only. | SQL for anything SQL can do that doesn‚Äôt require recursive logic.

&#x200B;

I also try to use Pandas as little as possible but its much cleaner for quite a few things (like calculating date differences with weekends). | I'm actually more of a fun of using vanilla python than pandas for most DE work.

But I would absolutely push back against using SQL rather than Python for most DE transformations.  Some things I think people need to consider:

   * How are you going to QA your code?  Ie, do you have a plan to automate comprehensive unit tests?  (note: dbt tests aren't QA or unit tests)
   * Can you run your code frequently enough to keep your users productive? Can they fix some data and see the results quickly, or do they have to wait a day to see the fixes?  Or is your compute cost on say Snowflake so high that you can only afford to run it 1-3 times a day?
   * Can you leverage third-party libraries to assist with transforms (ex: transform all ipv6 formats to a single format)?
   * Can you read your codebase after a dozen people have been adding to it for three years?
   * Is your codebase full of regexes to do basic manipulation?

SQL fails in most of these cases, and should really only be used for transformations if one is desperate. | to me, it's less about the framework and more about resource efficiency. yes, ofc the sql db is going to use its compute more efficiently than pandas would, but is that the best use of that particular compute for the business?

even a db server that is only ever used for reporting is likely still used by more than one person/team/dept. it's also non-trivial to scale up/out a db server. 

with most sql servers, readers should always be the same size as writers. so when you scale out, you're typically launching servers the same size as your biggest workload, which can be expensive to run and not very quick to scale (relatively speaking). 

so i think it's important to ask: is that really the best option? does it make more sense to do simple queries that fetch more data than is required and run the complex operations in python on disposable compute instances that scale up/out/in/down more quickly? this is, i think, a big part of the reason why Spark and Databricks are built the way they are and why they're so popular for complex workloads.

there isn't a right or wrong answer either way as long as the question is asked and considered. | I am retraining into DS and am just learning SQL and Pandas. I am curious, why do you have this opinion? Is it because the code is more efficient? Is it more in line with standard practice? What domain are you involved in?

Thanks! | I don't have time to design, create, and build a whole ass SQL server most times so yeah, I default to Pandas every time unless I'm building something for more permanent use | All the time. If the dataset doesn't fit in memory, you probably shouldn't be using pandas chunks to do EDA. You can do a ton of EDA quickly in SQLLite or Postgres. If you need to run similar queries repeatedly, you can create indices to speed stuff up. I mostly consider pandas chunking more for ETL-type jobs. | ‚ÄúIf your only tool is a hammer, every problem is a nail.‚Äù

I feel like a lot can be done in Pandas and a lot can be done with SQL. I think I am really efficient in 1 to do A/B/C, and really efficient in the other to do  X/Y/Z. But I generally find them, mostly, interchangeable. 

However - Pandas, imo, has never been efficient for update operations. Update statements in SQL are just so much easier. | ^ This, I use SQL if I need a relational database. I use pandas when I need a data frame.

Both have their usage cases. For example, I am not going to spin up a SQL instance to store a handful of addresses. A pandas data frame with suffice for that purpose. | That might be true from a technological standpoint, but syntactically they are both query languages for tabular data. Look at how much polars looks like sql (hell duckdb *is* sql) but still acts on dataframes. Equally tools exist that make using relational databases look like pandas. 

If you are more comfortable in one than the other, it's quite reasonable to extract the data into the one you're more comfortable with to work on it. 

It actually sounds to me like you don't quite understand the abstractions you're using. | This is getting a lot of upvotes but I'm not sure I agree -- ultimately, both SQL and Pandas are languages (pandas is probably distinct enough from python to call it a language, although disagreeing on this isn't all that important) for wrangling / querying data. For the vast majority of tasks, you can use either to accomplish the same goal (and generally very similar syntax). 

I tend to fall in the camp of ""if my data starts in a database, do everything I can in the database before pulling data down"" | SQL limited to select and analytic type queries `sum() over()` are a good deal closer to things like pandas.

In fact things like polars/spark blur that line even further. I would say that approaching things from the polars perspective is likely to give better and more maintainable code.

Pure SQL is too verbose, Pandas is too mutable. Polars/Spark is a great middle ground. | DuckDB entered the chat!

They can definitely be the same thing.  Pandas is meant to be a database with victor operations, as at the time all databases were row oriented so too slow for aggregation.  Pandas proved the need for victorised database and since many victorised databases shows up, including DuckDB which is powered by the same thing that powers Pandas, that‚Äôs Arrow engine. | The overlap for what they can do is quite large and not recognizing that doesn't really add to the discussion.

IMO pandas can be too verbose and harder to write/maintain code as a team. As it's python though your have the flexibility to scale out processes horizontally pretty easily depending on the size of the data, but this is going to add even more infra/code.

SQL on the other hand might have fixed hardware running the queries which could make working with large data slow. Some cloud providers let you scale up the compute resources adhoc so that could minimize the issues. If the tables/views being pulled from have indices set up well for your queries, you kinda get parallelism compute for ""free"", which could be good enough to get the output you want. Also it might be more cost effective doing joins on data in a data warehouse then exporting that to a model job vs exporting source tables and joining after. | > Using SQL to do complex analysis is every bit as inefficient and ugly

Would you mind providing a simple example of where using SQL to do complex analysis is less efficient or more ugly compared to doing it in pandas? | > Pandas allows you to do things very easily that might not even be possible with SQL; even if those things are possible with sql, they would invoke so much involved code and joins that they would be very slow or extremely tedious , if done in a rdbs/sql. 

I agree with this point, pandas is great for prototyping, it definitely favors momentum over performance. In my experience the issues arise when taking this code into production and complying with tight latency requirements. Even after this step is complete, keeping a low-friction pipe: iterating the prototype -> merging changes into the prod codebase can be painful. All in all pandas has saved me a lot more time than it has taken from me, and I am grateful for that. | You can also run a SQL query on pandas, and the data may have come from a SQL database. I think OP advocates a case where you do have a database, then may not need pandas, cannot think of other setting where OP's idea would hold. | I think he is talking about data transformations during extraction versus after extraction. For example, if you have a table with a column of strings that need to be split into sub-strings. You can do that with SQL prior to saving the data in a dataframe. Or you can query the data with SQL and then use pandas to do the string splitting. | There's a massive overlap between the capabilities of pandas and SQL. The company I work at has crazy amounts of BI/data eng/data science code using pandas distributed across dozens of departments, with virtually no central oversight. I see  overuse/abuse like op is describing every day. It comes about when pandas (via jupyter, streamlit, or similar tools) becomes the easiest interface for database extraction. It's actually very good at traditional ETL tasks for small datasets, but I hate to see it used like this because it's not scalable, the code is way less readable than SQL, and pandas has a ton of dependencies. So it might not be a good idea, but pandas is definitely used like op says in practice. | > pandas offers a lot of convenient tools that are nigh impossible or very ugly to replicate via SQL alone.

Would you mind giving some examples? | > should rather be about Arrow API¬†¬†

This ignores a huge portion of the pandas api for working with multidimensional array style operations (but to be fair everyone in this thread is ignoring that). Essentially, homogenous wide format frames/multiindexes. Which provides a lot of power that nothing other than numpy and xarray really provide. And pandas does it in a more accessible way than the other two I would argue.¬† | SQL is always going to be much faster, because you can run it on a database server with far more CPU and RAM than your application server. 

It's always a good idea to push as much work into SQL as possible. Unless the data size is small enough and won't scale up and it is faster to write in Pandas or otherwise more elegant to write in Pandas. But in general it is just sensible to do things via SQL. | ""A"" join is fine with pandas. It really reads like a sql join if the kwargs are there; even a couple is readable, but starts to get messy. any more than that, and I agree, just make a sql query already.

Most of pandas hatred is from people that maintain code written by academics experts; you know what I mean, code that haven't been linted, formated, and neither follows an internal style guide. This is not all academics, but having to rewrite shitty code from just one dude like that is enough to motivate such animosity.


Add that to a jupyter notebook with cells out of order, no venv definition, and no version control - you'll hate everything this guy does for a while. | There are speed advantages too. Running a SQL quest to drag your data into a Pandas dataframe is efficient when you are handling big data sets. If you do the filtering in Pandas, you can come across significant transaction times that could affect website performance (for instance).

SQL may be restricted in the complexity of the transformations. Panda has huge supporting packages for data analysis. So, it will usually be a combination of both. | It's mainly just that SQL is more widely known than Pandas, so code will be easier to maintain if written in a language more folks know. | I haven‚Äôt really used pandas much. What would that buy you over a list of dicts or Address instances, parsed from json? | Real talk, learn Polars ASAP. Basically Pandas but on Rust bindings. It sped up some of my df queries by 40x | Agreed here. I like pandas in a notebook for prototyping, and SQLAlchemy for productionized functions. | SQL is a fairly ubiqutous language but that's irrelevant to whether SQL and pandas are interchangable things to choose between. | Aye, but you really don't want to process 50 million rows of data in Pandas if you can filter the * selection processing useing a SQL WHERE command, and the process the (maybe 10k) rows using Pandas to find ( for example) the email inbox of a peadophile if you police search for him.

Imagine if that 50 million rows includes lists of images that are 2 GBs each...
.
First rule is filter out un-necessay rows and colums | Processing the data in Pandas was one function call; processing the data in SQL was 

    WITH table_0 AS (
      SELECT
        *,
        ROW_NUMBER() OVER (
          PARTITION BY role
          ORDER BY
            join_date
        ) AS _expr_0
      FROM
        employees
    )
    SELECT
      *
    FROM
      table_0
    WHERE
      _expr_0 <= 1WITH table_0 AS (
      SELECT
        *,
        ROW_NUMBER() OVER (
          PARTITION BY role
          ORDER BY
            join_date
        ) AS _expr_0
      FROM
        employees
    )
    SELECT
      *
    FROM
      table_0
    WHERE
      _expr_0 <= 1 | The deprecated language version as of January 2020? That‚Äôs not a like comparison at all. Your comparison would be like each ansi version of sql as they update the standard. 

Each database using sql brings their own language set features and supports some portion of the ansi spec, but not all. Which means your flavors of sql change based on the implementing database. Clickhouse, Snowflake, Postgres, MySQL all have different implementations and functions. 

The reason Pandas and Python are so ubiquitous is because they are easy to grok, have very tight optimizations for their use cases, have a small learning curve, are easy to read by humans, and the API rarely changes in a breaking way. | Sorry, No.  Pandas joins (even inner joins) are slow and memory-intensive.  I remember once trying a cross join and seeing 32GB of RAM blown through like it was nothing (and of course the machine locked up).  The same query in SQL used a couple hundred MB of memory.

Pandas does weird things with copies; as you are no doubt aware, appending data to a pandas dataset is massively inefficient because pandas makes a copy of the entire thing.  I am not very good at SQL but I recognize the immense amount of effort that has been put into planning algorithms and optimization for SQL operations.

Yeah a simple join or groupby is maybe more readable in pandas, perhaps, but I keep hoping that someone will make a lighter version of spark sql that operates directly on dataframes.  Maybe Duck DB is that thing already; I haven't had a lot of chance to play with it. | I see. Right now the DS I'm learning would use Pandas mostly for presenting tables, then creating datasets for further use. It does make sense to me that if you can extract the datasets you want from the SQL call, this seems to better way to go. But if you are extracting a dataset so that you can present it multiple ways to your boss or in a report, I think minimal SQL and maximal Pandas sounds better.

Basically, Maximal SQL/Minimal Pandas; or Minimal SQL/Maximal Pandas depending on your use. I think it looks most messy when you do some things in SQL, some things in Pandas in the same project for no good reason. 

But, I am a newbie.... | How is SQL more widely known? As in more developers are exposed to SQL? Yeah that‚Äôs probably true. However, how many of these developers know how to actual analysis with SQL. Most undergraduate db courses do not spend much time with complex data analysis using sql. 

If you look at the explosion in education for data science, and using programming to solve problems in academic areas, and some undergraduate statistics courses, you‚Äôll find Python and pandas. 

So while there may be more raw people exposed to sql as part of computer science undergrad, the number of people doing complex analysis with sql is much lower than pandas and Python.

My theory about why pandas is so popular is everyone knows spreadsheets as a visualizer. Combine that with knowing that you want to do a specific math function and can express it simply with Python means it will *always* win the mindshare of people actually working with data over developers. | Whenever you need some column operation you can use pandas, if your data is not a table, then iterating dicts. Pandas and polars make it easy to write out data transformations as a pipeline, so easier to trace and understand what happens to back table, some operations are vectorisrd, so happen faster. | DuckDB solves a lot of that with their friendly SQL. 

I haven‚Äôt touched pandas since I started using duckDB it does everything pandas and polars can faster and easier IMO.

Also SQL isn‚Äôt poorly designed it‚Äôs the result of 40+ years of research and is a staple for a reason. People much smarter than you and I have tried to replace it with very little success. 

If you understand the theory behind its design it makes perfect sense and it‚Äôs easy to express anything you want using it.

This is coming from someone who love coding avoids SQL for the most part but if it‚Äôs between SQL and pandas 1000% prefer SQL. | First off, neither pandas nor SQL is a replacement for good data modeling! 

That being said, what's the single pandas function call that does what your SQL is doing? | Table scans are extremely common in OLAP work. Still, I would never `select *` into a pandas dataframe. | Have you tried that before pandas 2.0? It sure sounds like it. A lot of optimizations came in with the arrow data spec.
It will still duplicate a lot of things; it's a trade-off between versatility and efficiency; you can find A LOT of decently optimized operations for many use cases (just look on the amount of methods a pd.dataframe has); there's no free lunch.

And if you are playing with pandas on GB sized data, even hundreds of MBs and doing cross joins, that's on you, my dude. Don't hammer a nail with pliers and complain about its ergonomics. | You can use it on polars dataframe and pandas too I think directly in the language or using duckdb.

First example that comes to mind.

I think I've also seen it used to query files from data warehouse but not 100% certain. | The specific operation I had in mind was dropping rows where the difference between two columns is negative. My dataset has 300m+ rows and ~470 cols so I have to do any operation in ~200 batches, meaning read/write is the big bottleneck. Files are in parquet format and compressed with gzip. Polars sped that up close to 50x checking back on it

But yes, just about anything you do with polars will be faster than pandas by default. Some operations will benefit from calling `df.lazy()` to get a `LazyFrame` representation of your data, calling dataframe methods on that as you normally would, and then calling `.compute()` only at the end to collect the results. It builds a computation graph so it can execute some parts in parallel.

Now I'm going to look into duckdb though, my mind is honestly blown that it gets better than this | Not OP but in my experience it's always faster than Pandas.
It's true that it doesn't really matter in a lot of use cases but I find the API to be clearer (even though more verbose) and it can allow you to refactor easier to account for larger than memory data if you ever come across those cases IMO. | Yes it was before pandas 2.0.  I have not really used pandas in a few years, but i am a huge arrow fan, so maybe these things are a lot better.  But next time I build a dataframe-based thing I am going to use polars, which can use arrow and is reputedly a lot faster than pandas..  

But FWIW the data I was cross-joining back in the day was not that big.  Maybe 1 GB tops.  We did fix it using SQL.  

Thanks for the update, though!"
1do71es,GeoPandas 1.0 released!,"A good 10 years after it's first 0.1 release, GeoPandas just tagged their 1.0 release!

* Release page: [https://github.com/geopandas/geopandas/releases/tag/v1.0.0](https://github.com/geopandas/geopandas/releases/tag/v1.0.0)
* Changelog: [https://geopandas.org/en/latest/docs/changelog.html](https://geopandas.org/en/latest/docs/changelog.html)
* 1.0 tracking issue: [https://github.com/geopandas/geopandas/issues/3201](https://github.com/geopandas/geopandas/issues/3201)
* 1.0 milestone: [https://github.com/geopandas/geopandas/milestone/4?closed=1](https://github.com/geopandas/geopandas/milestone/4?closed=1)

*About GeoPandas*

>GeoPandas is an open source project to make working with geospatial data in python easier. GeoPandas extends the datatypes used by¬†[pandas](http://pandas.pydata.org/)¬†to allow spatial operations on geometric types. Geometric operations are performed by¬†[shapely](https://shapely.readthedocs.io/). Geopandas further depends on¬†[pyogrio](https://pyogrio.readthedocs.io/)¬†for file access and¬†[matplotlib](http://matplotlib.org/)¬†for plotting.","Wow, as someone that uses GeoPandas a fair amount there are quite a number of additions in this update.  Certainly fitting that it is a v1.  Thanks to everyone that works hard on improving this library, it is a great resource! | If I already have pandas as a project dependency, how much more would adding geopandas introduce?"
10mezt9,Pandas Illustrated. The Definitive Visual Guide to Pandas.,,"Is anyone creating in 2023 with NumPy? What does NumPy do better than Pandas, if anything? | Great guide, in many(most?) aspects improves over existing pandas docs quite a bit.There are some things about pandas performance and non-obvious differences with numpy that maybe can be included in a separate article.

Example for a difference with numpy:

    # works pointwise, as expected
    test.values[((0,1,2,3,4),(0,1,0,3,3))]=100 
    # fills the whole quadrant of a DataFrame
    test.loc[('A','B','C','D','E'), ('A','B','A','C','C')]=100 
    # I guess when you how pd.DataFrame actually works this is not so surprising

Example for surprisingly slow performance, if I am not mistaken:

    test.replace({'_suffix': '_new_suffix'}, regex=True)

Also, can you tell how all of those images were generated?(if by hand, taking off my hat for your efforts, sir) | I haven't included sting and datetime functions deliberately, to keep the size of the article manageable. I plan to post a separate article on Pandas data types (also Int64, etc.). What is the format of your timestamps? Unix time (number of seconds since epoch) or something like 1900-01-01T00:00:00.000? Pandas is very flexible in converting anything to the datetime dtype. | Pandas date and time handling is a nightmare.

     df[""date""] > ""2023-01-01""

Would be totally valid SQL but pandas has a melt down and tells you it couldn't possibly compare that string to a datetime.

Worse, I'm relatively certain comparing timestamps to datetimes fails even though they seem pretty obviously equivalent. | Dataframes are not matrices.

numpy is about arbitrary dimensional matrices. It will have applications in numeric simulation, physics, etc... If you want to do something with a 5 dimensional tensor product, you use numpy. Numpy is really just a nicer way to work with fortran.

Pandas ultimately suffers from being a dataframe built on top of numpy. The difficulties encountered in that lead the creator of pandas to go off and create apache arrow which is optimized for the dataframe use-case.

And now things like polars are being built on top of arrow. | Numpy just has different use cases. It is great for number crunching as opposed to working with strings and dates. Upto 30x times faster than Pandas for basic operations. If you're building a kind of a GUI tool, rather than analyzing data interactively, Numpy is often times better. It has a more polished code to the extent it might become part of Python official distro one day. | Thank you so much for your response!

Yeah, that's a very subtle difference! Actually, when I've first encountered this kind of indexing in NumPy, I had the impression that it is some kind of tool from the 'plumbing' level (according to the git terminology: 'plumbing' vs 'porcelain' levels :) ), only supposed to be utilized by the libraries, not by the end users. Always thought it is an undocumented feature. For example, Jake VanderPlas does not mention it in the 'fancy indexing' [section](https://jakevdp.github.io/PythonDataScienceHandbook/02.07-fancy-indexing.html) (neither do I in Numpy Illustrated). Used it a couple of times (eg when working with [contours](https://scikit-image.org/docs/stable/auto_examples/edges/plot_contours.html)). Although, yes, I've checked now, it has been in the ""NumPy Manual"" (is anyone aware that NumPy has a ""Manual""?) at least since v1.13.

If I were faced with such a task I would probably slice the relevant columns (they supposedly have the same type to give sensible results), converted it into a 2d numpy array and proceed with numpy-style fancy indexing there. Or made a python-level loop with fetching elements one-by-one with \`.loc\` if indexing by labels is required.

Yes, regex can be slow if applied to a huge array item-by-item. Not sure why you need regex in this particular case. But the operation is slow even without \`regex=True\`: [https://stackoverflow.com/questions/41985566/pandas-replace-dictionary-slowness](https://stackoverflow.com/questions/41985566/pandas-replace-dictionary-slowness). Yes, that's a good example of low code quality I've mentioned in [this](https://www.reddit.com/r/Python/comments/10mezt9/comment/j654b9n/?utm_source=reddit&utm_medium=web2x&context=3) comment.

Here's another one [\#44977](https://github.com/pandas-dev/pandas/issues/44977) that I raised and that was mostly ignored with the formulation 'it is by design' :)

I made all the illustrations by hand in Google Slides. I've also implemented my own basic syntax highlighting tool for Google Slides that highlights text in the clipboard :)  Yes, is was a huge amount of work, but it intricately awarding when you finally find the simplest possible way of organizing a complex concept in a single image! | I had to revert from Unix to ISO 8601. I don't remember why I think it was mandetory for some backtesting framework, but I remember it having all kinds of problems with basic pandas functions further down the line when working with it. Where it said that index needs to be int. Or at least that was the gist of it. | I think it's fair enough, that's pretty dangerous and ambiguous code, because it's not clear what format your date is in. Comparing datetimes to strings without complaining leads to JavaScript-esque bugs, I'm glad the pandas authors didn't allow it. | The irony is that pandas datetime handling is better than python's. | In my mind python is to programming languages, as pandas is to python data libraries. For working with long format data what limited experience I have with polars seems to outperform it, for working with n-dimensional structured data, pure numpy and xarray make more sense. However, pandas is second best at both and often good enough to let you solve what you want quick and dirty in both styles, at the expense of optimized performance, which is often mitigated in other ways. | Well, I may not be following your exact issue, but if you use the reset_index() method for pandas and specify keep=True, then the timestamp index will be moved to a column and replaced by integer index values. Sorry if this isn't what you meant | As far as I'm concerned, Pandas has all the infrastructure for working with non-integer values in index, with strings and timestamps being a specialcase, implemented quite thorougly. You just need to convert to datetime from the string or integer representation. Here're several ways to do the conversion: https://stackoverflow.com/questions/40881876/python-pandas-convert-datetime-to-timestamp-effectively-through-dt-accessor | I believe I have encountered situations where pandas allows comparisons of different time classes, by just returning false everywhere. And that isn't so great either. | It's no more ambiguous than

     df[""date""] > pd.to_datetime(""2023-01-01"")

which would work so it's hardly a consistent design choice.

Pandas already assumes year, month, day unless specified so why not auto-parse a string date?"
lain0r,"Hey Reddit, here's my comprehensive course on Python Pandas, for free.","The course is called [Python Pandas For Your Grandpa](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa/) - So easy your grandpa could learn it. (It's the successor to [Python NumPy For Your Grandma](https://www.gormanalysis.com/blog/python-numpy-for-your-grandma/).)

## Course Curriculum
1. **Introduction**  
  [1.1 Introduction](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-1-1-introduction)  
2. **Series**  
  [2.1 Series Creation](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-1-series-creation)  
  [2.2 Series Basic Indexing](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-2-series-basic-indexing)  
  [2.3 Series Basic Operations](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-3-series-basic-operations)  
  [2.4 Series Boolean Indexing](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-4-series-boolean-indexing)  
  [2.5 Series Missing Values](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-5-series-missing-values)  
  [2.6 Series Vectorization](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-6-series-vectorization)  
  [2.7 Series `apply()`](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-7-series-apply)  
  [2.8 Series View vs Copy](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-8-series-view-vs-copy)  
  [2.9 Challenge: Baby Names](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-9-challenge-baby-names)  
  [2.10 Challenge: Bees Knees](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-10-challenge-bees-knees)  
  [2.11 Challenge: Car Shopping](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-11-challenge-car-shopping)  
  [2.12 Challenge: Price Gouging](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-12-challenge-price-gouging)  
  [2.13 Challenge: Fair Teams](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-2-13-challenge-fair-teams)  
3. **DataFrame**  
  [3.1 DataFrame Creation](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-1-dataframe-creation)  
  [3.2 DataFrame To And From CSV](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-2-dataframe-to-and-from-csv)  
  [3.3 DataFrame Basic Indexing](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-3-dataframe-basic-indexing)  
  [3.4 DataFrame Basic Operations](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-4-dataframe-basic-operations)  
  [3.5 DataFrame `apply()`](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-5-dataframe-apply)  
  [3.6 DataFrame View vs Copy](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-6-dataframe-view-vs-copy)  
  [3.7 DataFrame `merge()`](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-7-dataframe-merge)  
  [3.8 DataFrame Aggregation](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-8-dataframe-aggregation)  
  [3.9 DataFrame `groupby()`](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-9-dataframe-groupby)  
  [3.10 Challenge: Hobbies](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-10-challenge-hobbies)  
  [3.11 Challenge: Party Time](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-11-challenge-party-time)  
  [3.12 Challenge: Vending Machines](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-12-challenge-vending-machines)  
  [3.13 Challenge: Cradle Robbers](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-13-challenge-cradle-robbers)  
  [3.14 Challenge: Pot Holes](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-14-challenge-pot-holes)  
4. **Advanced**  
  [4.1 Strings](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-1-strings)  
  [4.2 Dates And Times](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-2-dates-and-times)  
  [4.3 Categoricals](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-3-categoricals)  
  [4.4 MultiIndex](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-4-multiindex)  
  [4.5 DataFrame Reshaping](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-5-dataframe-reshaping)  
  [4.6 Challenge: Class Transitions](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-6-challenge-class-transitions)  
  [4.7 Challenge: Rose Thorn](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-7-challenge-rose-thorn)  
  [4.8 Challenge: Product Volumes](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-8-challenge-product-volumes)  
  [4.9 Challenge: Session Groups](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-9-challenge-session-groups)  
  [4.10 Challenge: OB-GYM](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-4-10-challenge-ob-gym)  
5. **Final Boss**  
  [5.1 Challenge: COVID Tracing](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-5-1-challenge-covid-tracing)  
  [5.2 Challenge: Pickle](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-5-2-challenge-pickle)  
  [5.3 Challenge: TV Commercials](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-5-3-challenge-tv-commercials)  
  [5.4 Challenge: Family IQ](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-5-4-challenge-family-iq)  
  [5.5 Challenge: Concerts](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-5-5-challenge-concerts)  

Alternatively, view my [YouTube playlist for the course here](https://www.youtube.com/playlist?list=PL9oKUrtC4VP7ry0um1QOUUfJBXKnkf-dA). 

If you find this useful, please consider liking, subscribing, and sharing. It means a lot. You wouldn't believe how much effort went into creating this course. 

Thanks!","Great tutorial!

I wish pandas was easier though... the API is huge and there have been lot of changes and deprecations. | Thank you u/Neb519! Learning Pandas is certainly on my to-do list. This looks really helpful.

Raise your hand if you're like me and have a never-ending list of programming lessons to get to:

1) NumPy

2) Pandas

3) TensorFlow

4) Matplotlib

5) Pygame

6) JSON

7) OOP

8) Blockchain

9) jQuery

10) D3

11) Django

12) Flask

13) HTML

14) CSS

15) UI/UX

...

100000000000) Meaning of the universe | I'll take a look. I've learned most things from the pandas cheat sheet. [https://pandas.pydata.org/Pandas\_Cheat\_Sheet.pdf](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)

But this looks a lot more indepth. | Saving this to check out soon, thank you! I use Pandas a bit at work but I feel that I only know enough to get certain tasks or specific one-off needs. It would be nice to get to know it better. | Great course! Appreciate the effort that went into this. When I was starting out on Pandas, I found working with real data to be quite helpful. 

I run a [weather data API service](https://oikolab.com) where users can download time-series weather data easily ([sample code](https://nbviewer.jupyter.org/github/oikoweather/oikoweather-sample/blob/master/notebooks/oikoweather.ipynb)) and there is a free-tier - maybe something that might be useful for your learners? | I used pandas over the course of some months last year, but especially when using aggregations, I always seemed to end up with convoluted code. Your explanation of how to see the individual groups when using groupby() made perfect sense.

I'll be sure to return to your material. Thanks! | Subbed.

I am teaching myself Python and vlogging it on YT. Looking forward to running throughyour channel and possibly doing the challenges on my channel. Wanna learn both NumPy and Pandas... I esp want to learn Matplot lib. | Wait this is so funny you made a course now. I am legit starting to learn pandas for a project | Then, you've found the right course on Pandas my friend ;)

Not sure. I'm starting a big consulting project involving geospatial data, so maybe that. But I've been itching to get to Neural Networks. Finding time is the challenge. | You'll want the related course Python Pandas For You're Grandpa, since you are grandpa. | Yep. I think this is the downside to a package being entirely maintained by volunteers. In any case, Pandas is still the leading data wrangling package for Python. (I'm excited to see how [datatable](https://github.com/h2oai/datatable) evolves.) | I assume you mean `data.iloc[:, 0]`. I agree, it's not the best design but the reason is, Pandas supports labeled indexing and positional indexing. When you do something like `data[:, 0]`, it's not clear if you're requesting the column *named* 0 or the column at position 0. So, it's best to use `.iloc` and `.loc` to make it clear whether you're requesting a position or a label.

(Okay, you probably won't have a column *named* 0, but this argument makes more sense when think about requesting a row of data instead of a column of data.) | I think all of us use Pandas without knowing what we're doing, for a while. | Ideally, yes you should have at least \*some\* familiarity with NumPy before learning Pandas. Could you learn Pandas without knowing NumPy? Maybe but I wouldn't recommend it because you wouldn't understand some fundamental things. | You will definitely want to watch this awesome presentation on Geopandas.
https://youtu.be/PIPJAE-PXd4"
upbhyh,Pandas Tutor - visualize Python pandas code,,"I think there's a pandas GUI | Excel is pretty slow‚Ä¶and crashes very often‚Ä¶so I‚Äôm not sure how pandas is a downgrade from that. | I think about this all the time....excel has been around for longer than 25 I am pretty sure...and yet....they don't really seem to make improvements that are impactful. Still there are nightmares with data types. Still conditional formatting is both complicated and not that useful. Still so many times what i want to do, the only answer in the excel ecosystem is VBA. And so instead of VBA I'm just going to bring the data into pandas and export the results into a spreadsheet for my bosses, fully cooked. | Isn‚Äôt it written in C#? And pandas (if you use it properly) uses cython on the backend so really you are running C code anyway. 

Either way, it usually takes forever to operate on tens of thousands of lines in excel (which then crashes) whereas pandas is pretty efficient. | Sorry if there has been some confusion here, but I am not comparing pandas to excel. My comment said we are coming full circle and might have a Python based Excel. Python Excel would be a lot slower than Microsoft Excel (which is written in C)"
dnrp3b,Pandas got a new logo,,"pandas <3 | While I see the subtle pd, the logo should include two panda bears horsing around to tone down the ultra corporate feel.  While I harbor no ill will toward any software company this said, ""from Microsoft"" to me.

What's not to love about these:  [https://www.visitberlin.de/en/pandas-zoo-berlin](https://www.visitberlin.de/en/pandas-zoo-berlin) | We bigtime now boi's.  Nothing says pandas like non-descript vertical pong bars. | pandas is a good library | DAE not see the point of pandas? Basically as soon as I have a dataframe,  I convert the columns I'm interested in into numpy arrays. Working with a dataframe seems to invariably require ten times the effort to do anything.

I've also realised ""everyone uses it"" isn't good evidence of anything. | It's a lowercase p. *pandas*, not *Pandas*.

(It's even in the logo!) | https://raw.githubusercontent.com/pandas-dev/pandas/72206599f2d6dd2616520535343fd8722da75a4a/doc/logo/pandas_logo.png | Source to generate the previous one: https://github.com/pandas-dev/pandas/blob/34de308217a4c988a7655d69cfc2e2dd3d97d151/doc/logo/pandas_logo.py | IIRC pandas performs better than numpy for very large datasets. | What kind of stuff are you doing with numpy? There are a huge number of things pandas does that numpy can't | this was pandas logo?! | Numerical differential equations, Fourier transforms, image processing, plots and charts with matplotlib. Saving arrays to disk in HDF5 datasets. Basic arithmetic. Pandas puts the data in a table with named columns, I understand that. It fills in missing data with NaNs. I can make a table with named columns in numpy if I want, but it's usually not that useful and I just keep the data in separate arrays. The filling in missing data is usually not reliable enough that I need to do a manual step anyway, might as well be on a numpy array. I understand pandas has datetime datatypes, so does numpy. Pandas can load csv files, I see that too. So can numpy - again both are unreliable enough that I end up doing it manually.

Pandas has `groupby()`, I think numpy doesn't. Though it's trivial to implement and I don't remember the last time I used it. | From the [documentaton](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html):

>pandas is a Python package providing fast, flexible, and expressive data structures ...

---

>pandas is well suited for many different kinds of data: ...

---

>pandas is fast.

---

>pandas is the ideal tool for all of these tasks. | Even the `pandas` developers are inconsistent on this, though. See the [""Pandas ecosystem""](https://pandas.pydata.org/pandas-docs/version/0.25/ecosystem.html) page (the capitalization exists in the [source](https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/source/ecosystem.rst) as well) . | let's call it a website banner and pretend like pandas never had a logo! üôÑ | it's a pun:  
pandas  
p and ""a""s  
p and multiple a | I'm aware of that. Even Wes has messed it up occasionally. That said, the majority of the references are to ""pandas"" rather than ""Pandas"", the name of the package is *pandas*, and even the logo reflects this.

(I'm not sure why you would link to the page... and then the source of the same page.)"
11e99a2,pandas 2.0 and the Arrow revolution,,"It's quite amazing to see the synergy between the pandas and polars creators. I really didn't expect to see the presented example tbh. | I'm currently porting a data processing application from pandas to polars and while there's still a few things missing, it has been a really enjoyable process.

Don't get me wrong, pandas is great, but i'm starting to believe that we have reached a point where a complete rewrite like polars might actually work out better than teaching an old dog new tricks.

It feels a bit like when tensorflow 2.0 tried to make everything feel more like pytorch. Most people were much happier just using pytorch instead and leaving the old baggage behind.

In my experience, polars as a drop-in replacement is 7-10x faster. If you really optimize your pipeline to the polars mindset, it improves to 50-100x. It's stupidly fast.

One of two things will happen to pandas: a) they will never reach this form of acceleration. b) they use polars as a backend and rebuild functionality that is currently missing. | Does it mean that pandas will be as fast (or close to) as Polars? | Not certain I understand. Someone created a Python library called arrow? One that clears up/minimizes issue with pandas. | pandas ily | The original author of pandas is the co-creator of arrow.

Arrow is Wes McKinney's attempt to fix some back end issues with Pandas, but Pandas still has to deal with the mistakes made in the front-end API design. Polars gets to leverage McKinney's improvements to the back-end while providing a cleaner front-end. | In the free software community we're all friends. :) Our mission is to provide tools that are available to anyone. As a pandas core developer I'm happy to also contribute to Polars, and I'm happy to see it succeed. It solves things that pandas can't address, and for many use cases it's an improvement. For many others, pandas is still a better option. Polars is not as well tested as pandas, and it's mostly a one-person project.

I hope in the future we can share more code with Polars. It would be good to have I/O connectors, or the plotting extensions now in pandas being independent, and working for both projects, and other such as Dask, Vaex, Koalas...

So, different project, but same team. :) | cool. How stable is polars? ie, do you find any issues/bugs when moving from pandas to polars? | My guess is that the gains will be only in the in memory size of the data frames, since the speed of polars comes mainly from using a rust backend to enable parallelization and query planning. Theses optimizations are not coming to pandas right now from what I understand. | No.

Data interchange from pandas to polars and other libraries will be much easier.

Some elements of pandas will be faster.

Pandas will never be as fast as polars because of the immediate execution model and the fact that many operations implicitly copy dataframes. | Only in few cases. You need to explicitly use Arrow types first. Then it depends on the operation. Polars uses Arrow2 (rust) and pandas PyArrow (C++). Both implement some kernels (operations, such as sum,...), not sure which ones are faster, should be equivalent.

Then, Polars has a lazy mode, which allows, to be smarter than pandas, for example, if you do an operation and filter, for example  \`(df + 1).query(cond)\`, Polars is able to optimize this, and only do the operations to the rows not being filtered. While pandas will do this in two steps, operating in all rows first, and filtering later. | Arrow is a library for a format for storing columnar data in memory and functions for operating on said data, written in C. It can be used from various languages, including Python.

Arrow was written primarily by Wes McKinney, original author of Pandas, as a result of the pain points he encountered with in-memory data storage while writing Pandas. Polars was designed to use Arrow for its data, and Pandas 2 can now also optionally use Arrow as its in-memory data storage backend.

Wes's vision is/was that Arrow would become the lingua franca for columnar data, making accessing and operating on the same data trivial between e.g. R and Python. It's even used on GPUs for GPU-based data frame libraries.. | Which really makes this point rather funny (I took it to just be in jest):
> Besides just ignore Polars and use pandas | Yeah, Polars smells like C# to Pandas's Java. In a good way! | Hey I totally agree with you, but I think you‚Äôre underselling pandas‚Äô pros. Please take a look at some of my previous discussions on where I think the strengths of pandas vs polars lies.

https://np.reddit.com/r/Python/comments/11855fp/comment/j9h9psy/ | I don't currently use pandas, but that sounds like a wonderful idea.

My only concern is the overlap in module names with Python's arrow module, which is a wrapper/improvement on the standard datetime module.

Thanks for the 411, and happy coding! | It was clearly very much in jest. The entire objective of arrow is to enable this kind of data interchange. You aren't tied down to any one particular analytics engine, but can pick the best tool for the job.

There are some things that polars will be much better at than pandas, and there are some things pandas will continue to do better than polars.

With arrow you can pick the best tool for the job, but don't have to worry that in doing so you introduce time consuming and expensive steps that do nothing but copy memory around from one engines format to the others. | Yeah, but the question was will pandas be as fast as polars? the answer is no because of the reasons I described.

It will be faster, and is a great achievement. But polars has more things going on than only the arrow backend to achieve those speeds. | To me it looks like pandas 2.0 is something like <2x faster. Only the string operation probably uses some smart caching/hashing that arrow provides. Polars, in my experiments, is up to 100x faster than pandas if you use the lazy option and if you know what you're doing. You can create some simple examples that even show that. It's crazy. | They are making it better to share data between pandas/Polars. Just adding some support from the source.

Per the article. . .

>\[example use case\] . . . Besides just ignore Polars and use pandas, another option could be:  
>  
>Load the data from SAS into a pandas dataframe  
>  
>Export the dataframe to a parquet file  
>  
>Load the parquet file from Polars  
>  
>Make the transformations in Polars  
>  
>Export the Polars dataframe into a second parquet file  
>  
>Load the Parquet into pandas  
>  
>Export the data to the final LATEX file

`loaded_pandas_data = pandas.read_sas(fname)`

`polars_data = polars.from_pandas(loaded_pandas_data)`

`# perform operations with pandas polars`

`to_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True)`

`to_export_pandas_data.to_latex()` | So, when Polars will be more stable and mature, will there be a real reason not to use it over pandas? | In the example from the article, pandas was ""needed"" for reading SAS file(s) and exporting to LaTeX. For their use-case, the other operations are faster in Polars.

So, yes, if you need pandas you shouldn't use *only* Polars over pandas. If you don't need the speed, familiarity is probably best. | I'm trying to use Polars in my workflow more since it involves huge csvs and it's been great.

The one area where I'm always missing pandas is the IO.

The greatest accomplishment of pandas imo is the quantity of edge cases and weird data formats that pandas can import.

Making it easier and faster to move data from pandas to Polars is great for my usecase."
196jbms,Modern alternatives to Data Science Libraries like Polars with Pandas?,"I've been trying Polars and love them more than Pandas. In addition to performance, I find the API better designed (fewer ways to do the same thing) which, I think, allows memorizing the syntax faster, I would recommend Polars instead of Pandas to a new person.

Are there any modern alternatives for data visualization, algorithms, etc. that you are considering as an upgrade to your stack?","Pystore - data storage for pandas.
NiceGUI - Excellent frontend for Python.
KVRocks - On disk K/V store with a Redis API. | Criminally underated library from the creator of pandas is Ibis.


Very similar api to (although a little simpler than) pandas, but supports multiple back ends such as pandas, duckdb, SQL servers etc. You can change the back end to scale your code if needed withouy rewriting any transformations.


Performance wise, the duckdb engine ibis is very fast and pretty comparable to something like polars. | Seems like the functionality is quite reduced. Couldn‚Äôt figure out how to do a pandas series.shift(‚Ä¶) equivalent¬† | Having recently had to pick up more tidyverse, I understand why people like it, but it produces atrocious programming habits. The hoops you have to jump through to use variables instead of hard-coded variable names is nutty. It's nice for small and one-off scripts, but anything trying to approach robust behavior is more of a pain than it's worth. Pandas dot operators and flexibility blows it out of the water, even if the syntax is slightly more involved. | Pandas | \[\[var\]\] can be used in tidyR and ggplot to acces variables.  If you are used to it, than tidyR is better than Pandas will ever be.  Also Pandas dot operators don't work when the column you want to acces has a space.   
Working with external data where naming conventions might not be common, than the pandas dot operator breaks code. | It was weekend and I had karma to burn, so was just having a bit of fun.

But in all seriousness imho tidyverse is superior to pandas for data wrangling, and I think a python based data scientist looking for new tools, like op, would do good to consider learning R -- just like any R based analyst who is interested in machine learning or NLP should also learn python. | Sorry, meant dot chaining instead of simple dot operators, although those are nice. Where `df.series_names` fail, `df[my_col]` still works, which is the point I'm making.

[[var]] works sometimes, depending on what exactly you're trying to do, and which subpackage you're using. Other times you have to use tidyselect functions, and if you want to assign column names to dynamic names, you then have to delve into `!!` and `:=`, which is, in my opinion, insane complexity/knowledge ask. You shouldn't need an entire separate vignette about how to program robustly: if it's not baked into your framework, you should reconsider your framework. Especially when pandas, and even **base** R makes similar tasks incredibly easy, and don't change the rules just because you're trying to do something programming languages were made to do: handle code abstractly."
xidxvz,Pandas 1.5 released,,"As someone who started with python in 2013 (switched from MATLAB because of better ML capabilities at that time) pandas was essential to me - the notion of dataframe completely changed my view on data and data engineering concepts like map/reduce (probably R people will tell me that I am praising the wrong library) ...

Also this is where I started to love open source, you can look in each detail of the implementation and see into issues/workarounds of other developers... | [what‚Äôs new](https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html) for the lazy | I am so hyped for the stubs! I've come to completely rely on type hints and I never found a good one for pandas. | Love the tighter pyarrow integration. I have started to use pyarrow to read large CSV files because it is just so much faster than pandas, but once everything is converted to the right dtypes and serialized as parquet it's good to go for pandas. | Haha I had to download pandas 0.23.4 in a virtenv today | Pandas is such a blessing. I remember NumPy but never used it, seemed too esoteric. Pandas really worked for me.

It's interesting there's so many matrix math libraries out there that [there's a generic dataframe protocol now](https://data-apis.org/dataframe-protocol/latest/index.html). Pandas 1.5 adds support for it. | How do you update pandas in jupyter notebook? | I started with python in 2010 as a side language to Matlab which was taught in engineering schools. Back then i found that Python was superior and that it will be the language of the future.

When i discovered Pandas i had the same paradigm shift about data manipulation and it‚Äôs matrix representation in a Dataframe structure.

One day i hit the wall of Pandas of being very Memory hungry and slow compared to other implementations (generators and coroutines).
Also it was hard to interface it with the standard library or third party one (date64, float64, PyQt and its qObject, ‚Ä¶)

Now i use it at the higher/final stack of data/results manipulation for exploration.

Pandas is just a data exploratory/wrangling tool.

Now there is this library vaex that is very promising and resolves the afore mentioned limits of Pandas. | So is there any new stuff that's useful for someone with not a lot of knowledge about pandas, or is most of the new stuff pretty advanced? | I'm not 100% sure, but I think NumPy is a dependency for Pandas.  The Data Series in Pandas is very similar to a NumPy array, for example. | I wouldn't.  It's better to have a good, up-to-date requirements.txt or [setup.py](https://setup.py) and a virtual environment.  It's as easy as: 

* python -m venv --prompt \[projectname\] venv
* source venv/bin/activate
* python -m pip install -r requirements.txt

And you have a consistent set of libraries for which ever project you are working on, and it won't bugger your base set up.   Obviously, you can set the appropriate version of pandas in the requirements.txt, and if 1.5 doesn't work for whatever reason (like it's incompatible with other libraries), it takes about 20 seconds to switch back. | So many options. I'm pointing alot of my students and junior analysts to Modin at the moment. It let's you use the pandas API but switches the backend to Ray or dask.

Install the libraries and essentially you just need the following to use ""pandas"" for much faster speeds.

    Import modin.pandas as pd | Mostly rather advanced stuff.

For Linux users native [tar](https://pandas.pydata.org/pandas-docs/version/1.5.0/whatsnew/v1.5.0.html#reading-directly-from-tar-archives) support should be quite helpful | That's true, but I'm asking about feather vs parquet. Feather is an excellent format for pandas dataframes. I don't know why parquet would be chosen instead.

CSV is CSV, its pros and cons have not changed. | In the case of Pandas it existed long before type hints existed.

If you're not thinking about type hints when you start making a library you will often find that your code becomes very difficult to accurately type hint. 

Accurately type hinting can then become incredibly bloated, maybe adding just as much code that type hints as code that actually does stuff. It also might be a long time before you completely cover your code base. So one solution to this is to have stubs that you build up slowly over time."
q8fdfp,"""Give me one example of something you can do in pandas that you can't do in excel""",My friend the other day at work. He just got fired,"Source:  I have been a Excel user for 30 years.  I spent a year's worth of side time learning from every help file in Excel 2000.  I love Excel.  I still use it to view large amounts of data.  I still output to Excel or csv to view and check results in a spreadsheet for reasonableness.  But over the last five years, I have replaced more and more of my work in Excel with Pandas (often with parallel code in R!), but not a single task that I do in Python I have moved to Excel.

Over 1 million rows.

Run the same operations on thousands of separate worksheets at once.

Repeat the same operations an arbitrary number of times without inconsistency.

Configure graphs in a single command.

Timedeltas are correct.  For example, ""1/2/2021 11:00 AM"" - ""1/2/2021 7:00 AM"" == 4 hours will always be true in pandas.  Not necessarily in Excel - the timedelta will sometimes evaluate to 0.00000000071 or some idiocy.

Additional controls and parameters on anything.

Unlimited ways to join, merge, group, pivot table, or anything data.

Simpler database interfaces.

Statistical techniques in one easy step.  Simulations, bootstrapping, Monte Carlo.  Run every possible multiple linear regression, output results to a file, in 3-4 lines of code. | Its less about what you can and cant do and more about the speed and repeatability with which you can do things.

Like you can technically split a value in a cell into a few cells based on a delimiter with text to columns. But it makes you go through a whole wizard. With pandas its as simple as

    df[‚Äòcol‚Äô].str.split(‚Äòpattern‚Äô) | Technically excel is Turing complete which means you can do anything in excel you can do in pandas. The real questions are (a) how long it takes to develop and (b) how long it takes to run / how much memory it requires to run it. | I hate these implied false dichotomies. Pandas and excel are both great tools and can be used in very different ways. Use them both when the project calls for it. | Excel is an amazing piece of software that can technically do anything you want. So can Python, so can R with their respective libraries. Some tasks that involve iterating through millions of rows and automating repetitive tasks are much better done pandas. Doing advanced statistics feels very natural in R. Other tasks that require human interaction to see what's going on can probably be done better in Excel than in pandas+Spyder. Limiting yourself to one tool is a narrow-minded approach. | The last time I I had a larger table, I had the same thought.

But just for a very short time. The table had more than 20GB of data, about 240 Millions (240000000) rows....

It was a pain in pandas, but it worked....  Have fun with Excel.... | If I had a staff that knows Excel and doesn't know Pandas, the answer wouldn't matter. | There's one thing you can do only in excel and definitely not pandas or python. Be certain that the receiver can always execute the vba script and open the file and view the data. That means I can send a spreadsheet to the the Accounts payable dept of a local grocer who aren't computer literate and be certain that it will run on their computer. | When should I use pandas over excel? Specific examples? | > Do in Pandas, not in Excel.
  
1 million rows of data.  /serious | Pandas excel at fucking. | With python you can run a Pandas function over 100 million records. Try doing 100 million records in excel. LOL | Eat bamboo? What's pandas? | With power query and powerpivot there‚Äôs few things that pandas can do that excel can‚Äôt. | create a pandas file | There is a serious problem in genetics caused by excel. The auto correct text feature changes text codes that label gene sequences without the user realizing it. Those excel files go on to be published and used by other scientists causing confusion and errors. I read about this in Nature Magazine. 

Pandas will never auto correct your strings without you knowing about it!! | I‚Äôll give the opposite. I can‚Äôt see every number in the DataFrame / spreadsheet. Back when I was in college, some people used to excel for data analysis. I was amazed when they said they ran out of room on the first sheet and had to put some of the data on a second sheet. That was the day I knew I needed to learn how to code. Insane. Also, has anyone written a formula in excel on one line with 86 parentheses? I can‚Äôt do that in PANDAS. | Spend 99% of your time in the API docs and stackoverflow trying to figure out how to slice and dice your data.

I use pandas all the time but hoo boy that is one confusing API. | At this point [you can even do excel in pandas](https://trymito.io/) | Pandas is not the best data wrangling package in Python. | I think one example I can give is using 'for loop'. I recently had a situation at my work, where I have to see if a combination of numbers in a list can add up to a specific number.

For example, lets say my number is 15 and I have a list like [3,2,8,10,15]. I have to check which of these numbers can sum up to 15. In this example 15 can add up to 15.  [3,2,10] can add up to 15. AFAIK this would be nearly impossible to do in Excel. But in python, you can just itertools to get those answers in less than a second. Imagine repeating this for over a million cells. But pandas gave me the result in less than a second. | [he said no to pandas](https://youtu.be/X21mJh6j9i4) | For importing Pandas | How will that be of any advantage name examples? Pandas does just seems like Excel with extra steps... | It is! And there is a lot of overlap of what you can do in Excel and Pandas/Python. But there is an awful lot of *could* but not *should*.

I've seen some crazy spreadsheets that should be programs. They include tons of logic flow and are pretty powerful. But they also have really bad practices. For example, if you want to avoid scratch cells of intermediate variables, you need to make your equations super, super complex and nested. It is crazy what has to happen sometimes. | I was an early professional, and I kept telling all my friends that out of my experience, they should practice their excel, make stuff in excel, if you're in an interview and mention that you are quite comfortable with managing stuff and solving problems in excel, you've got a big head start. It shows skills, generally.

That is until you've attained serious pandas voodoo then you should mention that instead, ofcourse. | I assume you already know this, but it might be worth mentioning for other lurkers üëÄ: You can [load a large table by parts with pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html), which is useful if your computer does not have 20GB of free memory to spare. | Sir this is a python community and pandas is a python package | Eh thats a 30 second process, as opposed to a one liner i could type in 3 seconds. Not to mention it does the reverse of what i was saying. This is also very easy in pandas

    df[‚Äòc‚Äô] = df[‚Äòa‚Äô] + df[‚Äòb‚Äô]

What else do you got? Cause that reusability crap is less true for excel than it is pandas in general. And also if were talking speed we should also dive into computation speed, which pandas crushes excel in | This was a while ago, so I‚Äôm not 100% sure, but I don‚Äôt think the problem was easily vectorizable in pandas. I needed an output file of all the matches. Off the top of my head, I‚Äôm not sure how I would compare table1 to each record in table2 without looping | Thanks for this. It‚Äôs really tough for me to decide sometimes because I‚Äôm quite proficient in both excel and pandas (and Python in general.) 

I struggle with - should I spend the time to automate this or just one-off it with excel. | Ah, I see. What does pandas do? | He doesn't believe in under-pandas | In my position as an auditor I use both about 50/50. Excel is great for quick prototyping analysis, simple analysis with a certain amount of data, quick presentations, etc. When it comes to turning a project into an ongoing or more complex task, I bring out Python and pandas. Anything that I used to try to write vba or use macros for in excel are way easier, imo, to implement in Python. Python also seems to work with the data more efficiently than when I have multiple spreadsheets open in excel. Like you said, the best too for the job!

I‚Äôve grown somewhat biased against excel over the years because it tends to crash so often for me which is why I‚Äôm starting to move more and more work to Python with pandas

I don‚Äôt know enough about either to name a thing that one can do over another, but I do know from switching between Python and excel over the past 5 years that if I had to choose one to use the rest of my career, it‚Äôs Python. 

Lastly, sql is definitely worth learning anyways and I keep trying to push that onto everyone that I work with. Python combined with sql is unbeatable imo | Strictly speaking, there is nothing Pandas can *do* that Excel can't. Strictly speaking, there is nothing Excel can *do* that can't be done with paper + pencil. There are things you *shouldn't* do. An easy one is deal with 100mb+ of data in Excel. Don't do it. You can, but don't. The problem with this is that if they're not dealing with datasets of this size, well, it's not really a convincing argument.

It's just that certain workflows can quickly outgrow what Excel will excel at. I think the quickest I could come up with is some filter on the table based on conditions fetched from an external data source. There's a lot of ""Gotcha!""s in the way Excel will pull from external data, when it will update it's own model, and the order it will make that available for formulas and evaluate those formulas. If your workflow has a bit of a tight dev cycle loop where you're updating the datasource, then rerunning your filters, this is going to be an annoying workflow. You can do a lot to remove some of those pain points by writing a VBA script and attaching it to some control in Excel, but like, I doubt this is what anyone involved was thinking of.

If you keep expanding the above process, you can make a real pile of spaghetti, where it is very hard to organize the state of the system, and reason about what operations make sense.

To be clear, as you saw me outline, this is doable in Excel. I think it's easier to do it in Python/Pandas. It also scales better. Headless automation is easier. Integration with source control is way easier. Logging is easier. | You don't need to use a database much less learn SQL to use Pandas | Why does it matter if it's turing complete? Because of something called ""implementation details"" there is a clear difference with what you can achieve with pandas vs excel, and with performance and modularity. | Potentially related, my biggest gripe is when someone implements something in Excel that produces a hard-to-parse CSV and/or expects migrations from Excel to python (or R) to be easy or self-explanatory. 

I'm sure that I can implement the crazy multi-sheet wizardry you have in Excel as some Pandas operations, but I need you to explain to me what the Excel is doing. | where is my data? it was in an excel spreadsheet, backed up with revision history by office 365. If I'm not using that, it has to go somewhere. flat text file / CSV? that's fine i guess. Ill still keep it on sharepoint, and open it with excel if i need to do a simple query.  point being: if I'm not using excel any longer, there is a litany of other things that i also need to 'learn'.  

 and that takes us back to the original topic: what did i gain by learning python and pandas that i couldn't do before? | I will choose pandas over excel every day but your statement is incorrect.  Excel has power query which can concatenate multiple files like a breeze just like pandas. | You can resample using macros.  A long time ago we used an add-in package named PopTools.   Was wonderful, was also really, really fast, but was also 32-bit only.  Then, you can create a column for however you want to group, then use a pivot table or something.

However, this is a perfect example of why I use Pandas now.  In Excel, I have to structure everything within the framework, and 'get creative'.  

In Pandas, I would guess that most folks could do this task in 5-10 lines of code, less if you are really good at some of those little ways of expressing loops and such. | Yea you can use CSV. You can use Excel if you want. Lots of options: [https://pandas.pydata.org/pandas-docs/stable/user\_guide/io.html](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)  


Re: 1 thing you can do with Pandas that you can't with Excel- work with more than 1,048,576 rows | Where would your data come from if not a DB at some point? I don't know what SQL has to do with this at all, you could use pandas or excel with or without it. Like if you had an excel file to read you could read it in pandas. But unless someone typed out that csv/xls file by hand, it may have been queried by someone else who did know sql at some point. I have had to work with people who knew python decently well, including pandas, but were totally inept at all things sql, so the two don't always come hand in hand.


 I'm not suggesting you or anyone else invest time in learning python if you already have something that works or anything. I find that excel can be nice  for simple operations because you get to see it happen right in front of you. It's nice to just key over to a cell you want and not really think about too much syntax. But it's so freaking slow, I rarely have datasets where I can operate on them faster than in python. Doing more complicated manipulations are also easier in python imo because writing a script is much clearer than chaining operations in the formula bar, and I think vba is harder to make sense of. Seeking data is easier too imo since the pandas loc method let's you just specify columns by name and the values you want instead of all that lookup+index junk you have to do in excel. Having DataFrames talk to other DataFrames is also more convenient than working with data in multiple sheets because you don't have to type all that crap to reference data in another file, you can just open it in the same workspace.


Honestly, though, pandas is kind of a bad example. It has things I prefer over excel but it's still frustrating. With enough data pandas can also become very slow and it has more typing involved than most python packages. R turns up many of those advantages of python up to 11. I think if I didn't already know python and I had to learn something new for work I'd pick that. It can't do as much overall but for pure data analysis it is way cleaner. | That‚Äôs technically not true anymore as you can basically load any amount of rows into your excel data model and manipulate via pivot tables. I feel like python/pandas is a higher barrier to entry but very quickly becomes an easier/faster tool to do complex things | again, just playing advocate... but that just sound like a good reason to learn SQL.. use excel to query the SQL database to limit my dataset, and do all the same work i was doing before with excel.

I get what you are saying, and it fits with what i said originally: the argument should be what is the best tool for the job.

If excel is doing everything i need in one tidy package, what benefit is there to abandoning my extensive skillset and learning a completely new tool? potentially multiple new tools. i still need to find a way to display, present, and share my data. if im still going to do that in excel, and store my data in excel as you noted i could... leaving excel to manipulate data with python just to reopen excel to view it is a burden at that point. unless there is something cool i can do in Pandas, that cant already be done with excel. is there? | >Where would your data come from if not a DB at some point?

Excel WAS my Database and my IDE, also my chart/graph creation framework.   
If im not using excel any longer, im using a pything IDE... but no where to store my data.   


in context of the original post: the guy that got fired asked for 1 thing that pandas can do that excel cannot... in context it actually does less, as it doesn't self-host its own data. it also doesn't make pretty data.   


I'm not saying that Panda's, excel, or any other data manipulations frameworks aren't worth using: or that i don't know when an applicable use case may be...   
im saying its unwise to claim that something is ""superior"" when the truth is, its simply ""specialized"". depending on the argument/question: excel is superior. | You can use Python, along with many other languages, for excel. SQlite3 does export to Excel without the need for Excel binaries.  
  
And, of course, Pandas. | I just started using xlsxwriter instead, you can do your calculations, charts, formatting and everything in python, and then just decide where it goes in the excel workbook and save it. You can even save pandas dataframes directly to an excel worksheet by setting pandas writer to xlsxwriter. | If excel is doing everything you need then you have no need to switch ipso facto. And if you have a DB and plenty of time, I suppose- since Excel is technically Turing complete- there is technically nothing you can't do in Excel that you could with a general-purpose programming language. It just might be way more painful and time-consuming.   


Let's say you have a few million rows and want to compute some stat for rolling windows of size 100,1000,10000 and add them as columns. That's 2 lines of Python w/ Pandas and maybe a few seconds to run. I can't imagine how much time/effort that would take with your SQL+Excel workflow"
10a2tjg,Why Polars uses less memory than Pandas,,"I like polars a lot. It‚Äôs better than pandas at what it does. But it only accounts for a subset of functionality that pandas does. Polars forgoes implementing indexes, but indexes are not just some implementation detail of dataframes. They are fundamental to the representation of data in a way where dimensional structure is relevant. Polars is great for cases where you want to work with data in ‚Äúlong‚Äù format, which means that we have to solve our problems with relational operations, but that‚Äôs not always the most convenient way to work with data. Sometimes you want to use structural/dimensionally aware operations to solve your problems. Let's say you want to take a data frame of the evolution of power plant capacities. Something like this:

    plant  unit       date  capacity
        A     1 2022-01-01        99
        A     1 2022-01-05       150
        A     1 2022-01-07        75
        A     2 2022-01-03        20
        A     2 2022-01-07        30
        B     1 2022-01-02       200
        B     2 2022-01-02       200
        B     2 2022-01-05       250

This tells us what the capacity of the unit at a power plant changed to on a given date. Let's say we want to expand this to a time series. And also get the mean of the capacities over that time series, and backout the mean from the time series per unit. In pandas structural operations, it would look like this:

    timeseries = (
        df.pivot_table(index='date', columns=['plant', 'unit'], value='capacity')
        .reindex(pd.date_range(df.date.min(), df.date.max()))
        .ffill()
    ) 
    mean = timeseries.mean()
    result = timeseries - mean


Off the top of my head I can't do it in polars, but I can do it relationally in pandas as well (which is similar to how you'd do it in polars). Lots of merges (including special as_of merges, and explicit groupbys). I'm sure the polars solution can be expressed more elegantly, but the operations will be similar and it involves a lot more cognitive effort to produce and later decipher.

    timeseries = pd.merge_asof(
        pd.Series(pd.date_range(df.date.min(), df.date.max())).to_frame('date')
            .merge(df[['plant', 'unit']].drop_duplicates(), how='cross'),
        df.sort_values('date'),
        on='date', by=['plant', 'unit']
    )
    mean = timeseries.groupby(['plant', 'unit'])['capacity'].mean().reset_index()
    result = (
        timeseries.merge(mean, on=['plant', 'unit'], suffixes=('', '_mean'))
        .assign(capacity=lambda dfx: dfx.capacity - dfx.capacity_mean)
        .drop('capacity_mean', axis=1)
    )

The way I see pandas is a toolkit that lets you easily convert between these 2 representations of data. You could argue that polars is better than pandas for working with data in long format, and that a library like xarray is better than pandas for working with data in the dimensionally relevant structure, but there is a lot of value in having both paradigms in one library with a unified api/ecosystem.

That said polars is still great, when you want to do relational style operations it blows pandas out of the water.

u/ritchie46 - would you be able to provide a good way to do the above in polars. I could very well be way off base here, and there is just as elegant a solution in polars to achieve something like this. | I have to say, as someone coming from app engineering to ""light"" data science. Polars makes so much sense compared to the dog's breakfast of an API Pandas has | How is polar compatible with other libraries compared to pandas, such as matplotlib, plotly, numpy? | This may be a dumb question, but with these more performant data manipulation packages I‚Äôve found that the bottleneck that you STILL need to convert to Pandas at some point to plug it into many algos. So if you have a big one you‚Äôre gonna be hurting when you take that final step toPandas. Another bottleneck I ran into is going from Spark to DMatrix in XGBoost. You need an interim Pandas step because there‚Äôs no toDmatrix() in Spark. I guess I‚Äôm wondering when some of the main ML libraries will be able to ingest Rapids, Polars, and other new performant data formats, | I misread the title as ""Why polars use less energy than pandas"" clicked it as I thought weirdly interesting especially why one should use the term ""polars instead of polar bear"". Then got confused. | Generally agreed that the index functionality of pandas is where the real power of the library lies.

I think the challenge is that with so much implicit in the index, it isn't always clear what the code is doing.

In your example: `timeseries - timeseries.mean()` there are so many questions anyone unfamiliar with pandas might have about what the might be doing.

There are indexes on both horizontal and vertical axes of the dataframe. Across what dimension is ""mean"" operating? Is it computing the mean for unit 1 vs 2 across plants A/B or the mean for plant A vs B across units 1/2 or is computing a mean over time? If it is a mean over time is it the full mean? The running mean? How are gaps in the time series being treated? Are the interpolated? Is it a time-weighted average mean? or just a mean of observances? If it is time-weighted do we restrict to particular kinds of days (business or trading days)? And so on and so forth.

Ultimately you end up writing pandas code, observing that it does the right thing, and then ""pray that the behavior doesn't change.""

And then you have to deal with the risk that changes in the data coming in can propagate into changes of the structure of the index, which in turn becomes wholesale changes in what exactly pandas is doing. Which is a maintenance nightmare.

So I think we need something in between pandas and polars in this regard:

* Compel the developer to explicitly state in the code what the expected structure of the data is, in a way that polars can verify that the data aligns with expectation. So I say ""these are my primary keys, this is my temporal dimension, these are my categorical variables, this is a hierarchical variable, etc..."". Then tag the dataframe as having these attributes.

* Provide smart functions that work with tagged dataframes with long form names that explain what they do `polars.smart_functions.timeseries.running_mean` or something like that.

* Ensure that these tagged smart dataframes have limited scope and revert to plain vanilla dataframes outside of that scope to ensure that the declaration of the structure is ""near"" the analytic work itself. | I used `polars` while dicking around in Rust for advent of code and I'm immediately going to switch to using it as work as soon as I can (the Python wrapper). I could never understand pandas' insistence on having 5 ways to do the same thing. | Pandas suffers from its origins of pretending to be R, just as numpy and matplotlib have with MATLAB. It was also written in a time where python's dynamic nature was seen as a strength rather than a weakness, where convenience and shortcuts were seen as preferable to rigour and strictness. | That final step copy doesn't matter compared to what you would have done if you stayed in pandas. You would have done that internal copy, much more often in pandas. A reset\_index? data copy. Reading from parquet? Data copy. 

Polars needs a final copy when you convert to pandas, but you don't need the 5-10x dataset size RAM that pandas needs to comfortably run it's algorithms. | Btw, polars is based on arrow memory and this is becoming the defacto standard of data communication.

For instance spark, goes to pandas via arrow. | As others have said, the approach in your code doesn't limit RAM usage. There are other ways to chunk your data to use less RAM with Pandas though - for csv files you can use the [`chunksize` parameter of `read_csv`](https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas), for Parquet you can use [the `iter_batches` function in pyarrow](https://stackoverflow.com/questions/59098785/is-it-possible-to-read-parquet-files-in-chunks) and transform each chunk into a Pandas dataframe as you process it. Both ergonomics and performance are going to be a lot better with Polars, however. | That's way too much overhead for the kinds of things pandas and Polars (et al.) are generally used for (at least for me).

I love grabbing a csv file or list of dicts or web query or table from Wikipedia and doing quick interactive data analysis on it straight away using these libraries.

Shoot me if I have to construct a database first and run SQL queries against it.

To be clear, this doesn't replace databases. They just have different use case ""sweet spots.""

You could write the fastest code in FORTRAN! So why even use numpy, which seeks to be faster than pure Python, but isn't as fast as FORTRAN?!?! Just because a tool is the fastest possible doesn't mean it's better than another one that is _pretty_ fast AND enables an effective and efficient developer/analyzer workflow. (Spreadsheets falls into this category as well.) | usually I'm using pandas to build relational databases. | So we want to expand the frame from that compact record format to a timeseries. So from:

    plant  unit       date  capacity
        A     1 2022-01-01        99
        A     1 2022-01-05       150
        A     1 2022-01-07        75
        A     2 2022-01-03        20
        A     2 2022-01-07        30
        B     1 2022-01-02       200
        B     2 2022-01-02       200
        B     2 2022-01-05       250


The first pandas solution does this with multiindexes in a wide format.

    plant           A            B       
    unit            1     2      1      2
    2022-01-01   99.0   NaN    NaN    NaN
    2022-01-02   99.0   NaN  200.0  200.0
    2022-01-03   99.0  20.0  200.0  200.0
    2022-01-04   99.0  20.0  200.0  200.0
    2022-01-05  150.0  20.0  200.0  250.0
    2022-01-06  150.0  20.0  200.0  250.0
    2022-01-07   75.0  30.0  200.0  250.0

The second solution does this in long format, using merge_asof:

          date plant  unit  capacity
    2022-01-01     A     1      99.0
    2022-01-01     A     2       NaN
    2022-01-01     B     1       NaN
    2022-01-01     B     2       NaN
    2022-01-02     A     1      99.0
    2022-01-02     A     2       NaN
    2022-01-02     B     1     200.0
    2022-01-02     B     2     200.0
    2022-01-03     A     1      99.0
    2022-01-03     A     2      20.0
    2022-01-03     B     1     200.0
    2022-01-03     B     2     200.0
    ...
    ...
    ...

And then additionally reduces to the mean of the capacity of the unit over it's history, and subtracts the mean from the timeseries per unit. | Definitely agreed with risks and maintenance headaches that can arise, and yea there's always the tradeoff of abstracting away verbosity for ambiguity. Despite those issues the boost to iterative research speed is undeniable once comfortable with the different modes of operation.

> Ultimately you end up writing pandas code, observing that it does the right thing, and then ""pray that the behavior doesn't change.""

Agreed, and I think polars mitigates a good chunk of these problems by never depending on structural operations (where a lot of issues can arise), but it has a lot of the same issues around sensitivity to changes in data that alter the meaning of your previously coherent workflows.

I think xarray definitely needs to be brought into these conversations as well. Where polars is optimized for relational modes, xarray is optimized for structural modes. Pandas sits in between and is second best at both. | As someone who does a lot of data wrangling / manipulation in R, I've been hard pressed to find the motivation to switch to Python/pandas. I want to learn it for the sake of learning it, but question if it's worth the effort. | I think this take is missing a lot of context. See my comment here about the strength of the paradigms of working with data that pandas provides.

https://www.reddit.com/r/Python/comments/10a2tjg/why_polars_uses_less_memory_than_pandas/j453jjp/ | Pandas has its faults with silent failure, bloat, and type safety. It has a lot of convenient things wrapped into one imperfect implementation. I would like to learn polars for new projects.

As far as Numpy is concerned I do not think there is a more perfect library for what it does. You get a functional interface an an object oriented implementation of most functions. It is fast python wrapped C functions and can handle whatever the hardware will support with little overhead. It handles text and all types of numerical calcs. It is hands down the best standard lib package.

An analogous library is Scipy with numerical function wrappers on fortran and other scientific computing code, though not as consistent as numpy apis. It is relevant legacy software with limited scope and without a peer today. It improves by the maintainers keeping the interface modern.

I cannot defend the wack matplotlib api with two or three ways to do everything but I'd say you just need to figure out a few design patterns forget the rest of the docs and you get consistently good looking print plots. You can make any plot you dream of with enough customization. If you want javascript looking opinionated style web plots you instead use one of the revolving choices of plotting frameworks with their own ""modern"" interface. I just don't see matplotlib going anywhere because the results are extremely good for static 2d images. | Additionally in many instances those conversions to numpy/pandas can be zero-copy conversions. | When I included 'SQL' in the mix ('Or SQL.'), I intended to be emphasizing the 'plannable' and 'lazy' aspects in common with Polars and in contrast with Pandas. But I didn't use enough words.

The 'remote'-ness and 'oriented towards durable / permanent storage' aspects of traditional SQL are indeed drawbacks to one-off-ish and/or ETL-ish use cases where Pandas is used conveniently and effectively. But Polars ought to continue to steal mindshare from Pamdas for these efficiency reasons.

And then there's also DuckDB and SQLite for the intermediate approach. | Pandas is not necessarily better than R's dataframe, so don't switch on that account. But python as a language on the whole is better than R. R is a stats package with some general scripting capabilities tagged on as an afterthought; python is a programming language where one of its many capabilities is stats. Maybe it's not as good as R for stats, but for the rest of computing, it is better, in my opinion. | Yeah, that seems like a fair clarification of your original comment. Your point about Polars stealing mindshare from Pandas is where I'm at. It's taking some of the goodness of SQL and some of the goodness of Pandas if you choose to use the lazy API--but doesn't replace the use cases I have for bonafide DBs. Otherwise, using the eager API, it's still gonna be faster than Pandas. For me, Pandas ate mindshare from Excel and pure Python (possibly with numpy) and some from SQLite (but not much, as that is a really niche tool for me, personally). | These workloads generally involve:

* Reading data into memory
* Running lots of analytic functions on them to compute new columns
* Aggregating and formatting to create an output

Total memory footprint is often 2-4x the base data. Pandas is more like 4x, polars is more like 2x.

Challenges of SQL include:

* Difficulty expressing complex analytics... Things like regression models, or parsing strings are awkward in SQL
* data is often minimally structured, and there isn't enough value in declaring a full structure for a DB to store it
* Access is largely bulk and column oriented so traditional RDBMS are not ideal | I'm aware but you can usually prevent loading a lot of data with filtering or aggregating in the database. Since this is about memory, I think it does apply. 

I'm not saying pandas is super duper and polars bad, not at all but given the ecosystem you can't just ignore pandas or simply replace it."
zybjx8,I made a subreddit specifically for pandas!,"Hey all,

You can check it out here. Pandas conversation is a bit diffuse across a few subreddits, so i thought i'd aggregate here. 

https://old.reddit.com/r/dfpandas/comments/zyb9wk/welcome_to_dfpandas/","I‚Äôm using Pandas and SQLAlchemy to write some geocode information to a db table on a trigge. This is a temporary solution until we get Synapse in place :) but watch it turn into a permanent solution. :) 

I don‚Äôt have a problem with it but it‚Äôs fooking slow. | Learn Polars instead!

Pandas is:
 - single-threaded
 - extremely slow
 - consumes a lot of memory 
 - has huge memory consumption spikes for some operations like joins
 - doesn‚Äôt have lazy operations 
 - has weird type conversions, can literally break your types, the Int64 vs int64 is a pita 
 - API is often non-pythonic 
 - complex pandas code is an unreadable mess

Polars is the opposite for every item in this list. Written in Rust, extremely fast, multi-threading, type safe, great pythonic API, expressions API + query optimization,  everything can be lazy, even supports ‚Äústreaming‚Äù for processing extremely large datasets (>> RAM), complex code is readable thanks to expressions and pythonic API.

Most of the bad stuff Pandas has is only relevant for relatively big datasets tho. But typing bugs will haunt you in production anyway. 

Pandas was useful for a very long time, but I feel like it‚Äôs time for us to stop using it (at least in new projects). | i like pandas | Is pandas actually worth learning? | The reality is that pandas works so ducking well I have zero need for a pandas forum | Currently using pandas. Thank you! | It I‚Äôm using pandas to analyze data relating to pandas, should that go in r/dfpandas or r/pandas ? | I may be doing it wrong, but I have a bunch of ""applies"" in my pandas clean up code because it is going row by row using multiple columns to make a new column. 

Is this someone polars can do? (And more importantly, do it quicker?) | Why? Are there more subs about pandas? | An update in SQLAlchemy [broke pandas ability to form a dataframe from SA results](https://groups.google.com/g/sqlalchemy/c/frxgrzv3tBc/m/Q4_f-tKdBQAJ). I hope this gets sorted out. | You are taking the words out of my mouth. I dream of a world where polars can be consumed by all plotting libs etc. Might actually already be the case.

Also you forgot: in memory format is weird and inefficient for everything except e.g. a sum over a whole df. Since pandas' primary advantage over numpy is dealing with mixed dtypes ... Most notably, the in memory format of pandas sucks when it comes to loading/storing in any other format. Even converting from and to numpy can trigger realloc. Polars is the exact opposite as it grew from arrow, which is designed to be well behaved for everything anyone can ask of a format. | Pandas contributor here. I really like polars, but one thing I wish it had is indexes. I know the lack of such is one of the reasons that polars can get the performance that it does, but they‚Äôre really useful in certain cases, especially multiindexes. I‚Äôd actually prefer to do everything in long format, which is what polars encourages, but that‚Äôs not practical in many cases as it can result in much larger memory requirements. | I'm happy seeing people falling in love with Polars. I use in production pipelines, got consistent 95% time reduction. Even if you use Pandas + multiprocess the boilerplate needed makes Polars much more viable. | I agree there's really no reason to use pandas over something simple like numpy rec arrays. And if your problem requires additional tooling, I don't see what pandas will offer. In any case, pandas Series objects are backed by numpy arrays but it seems the bloat really is at least doubling runtime in some cases. | As soon as GeoPolars convinces me to switch from Geopandas I‚Äôm in. But until then.. | This thread is the most ""apples and oranges"" discussion I've read lately. To a pure data scientist Python with Pandas might be compared to SQL or Excel (or Power BI etc) somehow (I don't know how, but maybe). To an application developer absolutely not, as these tools have completely different ""reach"". This indicates it's crucial with a combination of data scientists (data/analysis/AI etc domain) and application developers (logic/UI/UX/scaling/compatibility/client/server etc domain) moving forward, including in terms of selecting the right tools. | Excel spreadsheets feel so slow and clunky to me after using pandas for a few years. There‚Äôs some areas where they‚Äôre certainly more suitable, but for speed, automation, and so much more, I find pandas to be exceedingly better. If you want to get started, go through Corey Schafer‚Äôs pandas series on YouTube‚Äîyou‚Äôll be extremely well-placed for further exploration of the data landscape in Python on your own. Can‚Äôt recommend highly enough. | Just to add‚Ä¶learning *DataFrames* is useful in general programming. Whether it‚Äôs *Pandas* or something else (like Polars) depends on your use case. | I was a hard core data scientist/analyst who only ever coded in Python/Pandas. 

I got dropped onto a software dev project in need of python engineers and they laughed at me when I suggested carrying data through our app in pandas data frames. Instead we all used dataclasses, dictionaries, lists, collections, json‚Ä¶ grabbing data from a postgres sql db.

My hottest take is that pandas/python/Jupyter is what data analysts/scientists who started out on Excel/PBI/Tableau are more comfortable using, but I wouldn‚Äôt count it as a robust way to manipulate data as a software dev in general. | Not enough pandas haters in this thread so here I am.

pandas is great for data science. If you want to be a data scientist or analyst you're going to encounter it at some point and you should familiarize yourself. 

pandas is great for simple file-to-DB or DB-to-file tasks, including simple data manipulations in between.

pandas is a crutch in most other cases. There's a lot of sunk cost mentality going on with programmers (and organizations) who code hacky workarounds to force all their scripts to flow through pandas. Define your own classes. Write your own SQL statements. 

That's not to say you should forget that pandas exists! It has its place. But you should always try to find the right tool. 

TL;DR: Yes, pandas is worth learning. But so are many, many other Python modules. Don't limit yourself. | hell yeah it is. I don't know what i'd do without it. Your choices are:

1. SQL to excel (sucks x2)
2. python (fine, but laborious)
3. pandas (rocketship)

Between 1 and 3, you may have to do a lot more work in sql to fit it into pandas. It is always most efficient to query in sql, but if you don't know quite what you're looking it, it can be good to grab a good slice of sql (without spending too much time writing insane queries!) and then chop it up / eda / preprocess / model in pandas | Out of all the various dataframe libraries across various languages, pandas is by far the worst. It's slow and has an awful API. It is literally only better than base Python. Polars is a better option in Python. | Polars author here.

You can use nested data instead of long format. We provide structs and lists for arbitrary deep nesting.

Polars will have indexes in the future but not in the way pandas has. They will merely be used as a auxillary data structure to speed up queries similar to how they are used in databases. The semantics of a query will not be influenced by the state of an index column. | Check out these benchmarks: https://h2oai.github.io/db-benchmark/

Pandas may not feel slow for small datasets, but Polars is definitely significantly faster.  I can confirm from personal experience as well. | Pandas is definitely slow if you want to compare it to something like numpy or polars, but you shouldn't be using it for things like that in the first place. | Of course pandas is slow. You didn‚Äôt notice it because you are not working with enough data. Check out the benchmarks in the other comment. 

Recently I refactored some old pandas code that calculates some aggregation features. It took 20 minutes to run that. 

After rewriting it (mostly) line by line to polars the code was running in under 15 seconds. 

And the dataframe was not even that big - I think around 2 million rows. We were calculating around 700 features (columns). 

Re: types - pandas awfully handles int columns with missing values as float by default. ‚ÄúInt64‚Äù has to be used instead of ‚Äúint64‚Äù to fix that. But pandas has some bugs because of this behavior - for example, I remember one time in production the type was changed from List[int] to List[float] when saving a dataframe (to parquet) and reading it back. It‚Äôs very time consuming to remember and cast all integer columns to Int64 when loading stuff with pandas. 

Polars doesn‚Äôt have these issues as it supports Null values for all types. And it‚Äôs type system is strict. 

Polars is basically a PySpark replacement for single node workloads. | Nah Pandas is frickken slow. Just time operations in numpy vs pandas.   Pandas is way slower by 2-10x sometimes. | Oh I 100% agree, pandas is abused for virtually all sorts of processing tasks that would be much better if they actually used native Python types and (gasp) iteration, to make the unit of work smaller and more easily understandable and testable.

A DataFrame is ultimately an opaque object, and it is hard to catch logic issues around it until run-time as a result. | Great answer. Pandas is the de facto standard, everything that revolves around data uses it or something like it. The abstraction and the freebies DataFrames provide for the work is awesome. | >Define your own classes. Write your own SQL statements.

Oh God no that's why I came to Python. Why write 87 lines of code when you can write one line of Python/Pandas code? | Every pro-pandas comment is nebulous like this. Do you have a concrete example of how it improves things? | Why is Polars better than Pandas, first time I've heard of that module. | Thanks for the tip. I don‚Äôt think this would quite satisfy the use case, since you‚Äôd still be using the same amount of memory as in the long format, and you lose the granular atomicity of the data in the structs and proper df normalization.

I think a more apt replacement is taking your ‚Äúmeta‚Äù columns put them into their own df, drop duplicates, assign each row a unique id. Then remove those columns from your main df and replace it with a column referencing the unique id. So you have a main df and a meta df. (This is essentially what a multiindex in pandas is at its heart).

There‚Äôs also other benefits to multiindexes. For one with long format only all your data manipulations need to be done through relational operations. However if you take advantage of multiindexes you can manipulate your data through dimensional/structural operations, which can be easier to reason about in many cases.

That said I don‚Äôt think polars needs to worry about this use case. It‚Äôs very good at what it does (better than pandas), but I don‚Äôt think it‚Äôs a drop in replacement. | > The semantics of a query will not be influenced by the state of an index column.

I'm just starting to learn more about polars, but I feel like certain things look easier with pandas than polars. In particular for many of our datasets that are pulled from different sources there is a natural join condition (e.g. date) and pandas indexes make things more convenient and less verbose.

So you might have code that looks roughly like:

    df1 = read(source1, index=""date"")
    df2 = read(source2, index=[""date"", ""identifier""])
    df3 = df2.concat(df1.lag(-1).fillna(0), axis=1) # 
    df3.groupby(pd.Grouper(freq=""M""))....

Obviously this can all be done with polars, but it seems to be a lot more verbose and from what I am seeing online it appears you would have to repeat the column name.

I think what I really want from an index is more like a context which can be used to ensure that all the queries executing on these related dataframes have the correct defaults and prevent me from accidentally omitting a column. Something like:

    with pl.context(df1.key_context(sort_key=[""date""], key=[""account""]), df2.key_context(sort_key=""date"", key=[""ticker"", ""exchange""])):
       # within that context things like:
       df2.shift(1) # implicitly grouped by account and sorted on date before shift
       df3 = df1.join(df2, how=""inner"") # implicit join across the date key as it is the only common key
       df3.key_context(sort_key=[""date""], key=[""account"", ""ticker"", ""exchange""]) # is implicitly added to the existing context
       df3.groupby([""date"", ""account"", ""ticker""]) # would raise an error because you dropped exchange while still in the context
       df3.groupby([""date"", ""account"", ""nyse_ticker"", ""exchange""]) would raise an error because your context indicates that ticker was the column to use not nyse_ticker
     df3.groupby([""name""]) # is allowed once you leave the context

Because so much of what pandas does when an index on a dataframe exists is dictated by the index, it can be really helpful in avoiding bugs when refactoring.

The main problem with these indexes is that they persist indefinitely with the dataframe and are not readily visible in the code. I think something like a with-block might make provide the best of both worlds. | I've made a living off refactoring shitty pandas code that some schmuck wrote while they were going to night school for data science. Like this happens a lot. Really speaks to how ready managers are to hand their processes over for automation to wholly unqualified people if they demo one neat ETL script. | There's things that I'm sure can be fine without, but I've just found to much easier with it. Even pretty simple things. 

For example, my first real script took 6 or 7 spreadsheets and combined then into 1. They had several columns in common, but not all in the same order. Pandas made it very simple to combine them all, dedupe the list and output the new list.

My second example again can 100% be done without it, but I found it easier/nicer to do with it. I've made a few discord bots for games I play that save user input in a SQLite database. When people use a command to see a list from thata database, I use pandas to pull the info, transform it into the format I want to show, then output it as markdown.

I havn't used the more advanced stuff, but I found it very intuitive and quicker to learn how to do what I wanted to do. | ‚ÄúThe tool I like, and know how to use, is the best tool.‚Äù

I am an MLOps engineer who supports a data science team using pandas.

Pandas js not my preferred tool for anything. It has its niche. I‚Äôll tend to process data in SQL first. For actual data engineering, pandas architecture is not a great approach‚Ä¶ so tools like Spark tend to get used instead, or cloud services. | if you asking about vs. python, dataframes are amazing and so easy to play with. Also, just about any function you want to do, that you might need to script for in python, already exists in pandas. Even something so simply as ""groupby,"" to whatever kind of matrix or shape transformation you want to make, or whatever. 

If you're asking about excel, don't use excel | Automation and scaling is the first thing that popped in my head. (Automating repetitive tasks).

IIRC, Excel has PowerBI or some tool that can enable the user to automate stuff, but python with pandas can scale significantly more.


I'm still new to this, so take my opinion with a little grain of salt.

One of my projects right now involves extracting data from SQL, manipulating said data a lot of times, which results to some interpreted data, all happens in just one button/execution of code.

Also, it's way faster than doing it in Excel afaik. | Faster, better memory management, and (subjectively) better API. Check out their [website](https://www.pola.rs/). I honestly just hate pandas. I came from R where both tidy style and data.table are a billion times better. | Adding on to what the others have said, Polars also does lazy queries, meaning it only calculates things when you ask for a result. This sounds bad until you realize that it allows query optimization like a full SQL database does, decreases memory usage and makes parallelism easier for the library devs. It‚Äôs usually 5-10x faster than pandas on the same data. | Edit: they got downvoted so they blocked me üòÇüòÇüòÇüòÇüòÇ

>Haha, all of the numpy and OpenBLAS users are giving you the side eye right now.

I'm convinced the people who like pandas have literally never used anything else.

>
>It's idiosyncratic and carries a lot of bad design decisions from its LONG pre-1.0 phase but it's not as bad as you're describing. 

Any library that has seventeen different ways to select rows or columns is bad and should feel bad. They regularly ship broken functions like pivots. It's slow as molasses and uses all the memory in the known world.

R is a weird abomination of a language but some of this just isn't true.

> End of the day, at least it's not R where every library installed is ALWAYS imported,

Simply false.

> your coworkers fill up the global namespace for fun at the beginning of the scripts,

Get better coworkers.

> deserialization ALSO fills up the global namespace, 

Not sure what context you're referring to.

> and undefined variables get special contextual treatment.

Non standard evaluation is literally the best part of R. Like with data.table, filtering is literally just df[column>0]. So succinct ü•∞ü•∞ü•∞ | Neither pandas or polars are executing Python code for the actual computations, they are instead using complied C/C++ or Rust code. That‚Äôs the case with all the Python data-related libraries, otherwise they won‚Äôt be even remotely useful. Python just acts as a convenient (and very good) frontend for the lower-level languages. | no. he mispoke. SMALL data. NOT big data. do NOT use pandas on big data. you will regret it. | That sounds more fitting for a database stored on secondary storage, and even then PB is a lot.

SQL and Pandas in combination would be great for extracting parts of a database (via SQL) and doing all kinds of analysis and operations on that data (via Pandas etc). | Well I feel dumb. My university courses never taught us anything else. I do ELT (json to BQ) with pandas. Is this wrong? Is the a better way? | Excel has improved greatly over past several years with the advancement of power query and it's language M. Power BI / Power Pivot can scale very well these days. It also makes it simple to hook into databases. You can do alot of the same things you can do in Pandas in Excel these days without much issue.
Otherwise for more data engineering tasks you can pretty easily hook into some 3rd party library like PYODBC/mysql-connector to load/download data.

edit: Not to say you can't use Pandas for these things, and if it works for you..stick with it | I don't need 200 lines of code, but just ""pandas has this feature, which means you can do operation X in one call rather than 10 lines of python like this"" is what I'm after. | Pandas isn't for vectors, matrices or tensors. Are you thinking about Numpy there..? | I don't know how you are using pandas, but I bet you haven't made an effort to write efficient code in pandas. | They blocked you for insulting them over a discussion about pandas. You‚Äôre way out of line and a huge asshole. | Both, to greatly differing degrees. I got my start coding when I was a ""supply chain analyst"" (glorified spreadsheet reformatter). There was this guy everyone thought was a computer god because he automated a bunch of their reporting. When he quit, everything broke. So they asked if anyone wanted to learn Python. I said yes, then spent the better part of six months learning by rewriting his thousands of lines of busted up pandas code. I parlayed that into automating my own stuff and fixing other random scripts people wrote and abandoned.

I've done a few jobs on Fiverr that involved refactoring. They weren't all as pandas-centric, but there's a common thread of ""someone else wrote a Python script for me and it doesn't work anymore"" and finding out that person learned just enough pandas to be dangerous then left for greener pastures.

Now I'm an ETL engineer so I'm actually in an established code base, not messing with one-off scripts people wrote, but I still find pandas shoehorned in random places every now and then. Dug in like ticks. | Sorry, I didn't mean to discourage anyone who is currently using pandas. It's a fine tool for simple ETL tasks, which may or may not describe what you're doing. I've never worked with BigQuery so I can't speak to what's available there, but if you're basically just reading data from JSON files and loading it straight into BQ, pandas should be completely fine. If you're doing something more complicated than that in between, and especially if you start hitting confusing bugs or if others have trouble running your code, I'd recommend looking up alternative ways to accomplish what you're doing.

I'd need to know more about your workflow before I could recommend anything specific. | my dude is in /r/python picking a fight for excel. good luck to ya

yes, you can do all these things without pandas. Yes, we could still operate our economy on trains and steampower. But like cars, pandas offers flexibility and fills a large niche in the data management process, even if it isn't strictly speaking necessary. YMMV | While that's true, there are some stuff that you still can't do in Excel.

As I've said I'm still new to this so I'm only touching the surface, but if you use Pandas (and by extension Python) you can use other libraries as well so that you can for example send your data (pandas dataframe) into a Google Sheet, or directly inject it in your SQL database. (I haven't really familiarised my self with power query, if it can do these as well then I guess all my arguments are moot). | Lmao feel free to look up literally any benchmark. [sucks to suck](https://datascience.stackexchange.com/a/40532). Unless you think I'm literally using the built in functions like reading or joining wrong üòÇüòÇüòÇüòÇ

Edit: Downvoting me won't make pandas faster :) | Lol, im actually the guy writing the shitty pandas code for you, and deploying it into production. Thank me later :D | I didn't ask the original question, but I'm not trying to pick a fight. I still use Pandas and think it's a great way to quickly look at your data. I'm just answering to the ""Do not use Excel"" portion. I just think people hear Excel and they just think of the standard rows/columns on the screen and how slow it can be when used that way. When used correctly, to analyze data, it can be nice and simple use. | Please feel free to provide me a benchmark for groupby stepwise interpolation/or anything with a bit more statistical than standard sldeviation and mean. half of them can't even do them in a line or two, whereas pandas can in a single line. It really depends on what type of work you are doing. If you are an SQL monkey, then yes pandas can't compete there. | Pandas simps are so fucking annoying. You just got proven wrong so you immediately shift the goalpost. And then throw in that anybody who proves you wrong is just a ""SQL monkey"". Fucking loser

There's even more cases including more complex ones [here](https://h2oai.github.io/db-benchmark/#explore-more-data-cases) that pandas completely fails üòÇüòÇüòÇüòÇ | let's keep it civil, and let me explain again, there are use cases where pandas shine and there are some where it doesn't shine but still does a better job (compared to base python). Then there are NumPy amd SciPy functions which are integrated with pandas and make life even simpler. Let's take an example of spark. Do you know how to apply a custom groupby function (lets say KS stats over different groups) similar to you can write groupby apply function in pandas? Not without fiddling in Java. | > let's keep it civil

üôÑüôÑüôÑ says the guy who immediately came in condescending. Loser pandas simp | Literally our first interaction:

> I don't know how you are using pandas, but I bet you haven't made an effort to write efficient code in pandas.

Condescending and wrong! | reality hit a bit too hard? there are job functions other than a data analyst that use pandas. there is no point arguing with a loud mouth. good bye"
td9fzp,I made a video tutorial about speeding up slow pandas code. I wish I had known this when I first learned python and pandas.,,"Unfortunately the functions I typically apply to my data sets involve opening an excel and grabbing some information or querying a database and returning a result based on the output, so I don't think I can go from using `.apply()` to vectorized functions (correct me if I'm wrong)

One quality of life feature that I love when doing very long `.apply()` that could take minutes or even hours is to use the `tqdm` module to display a progress bar. An example would look like this:

    import pandas as pd
    from tqdm import tqdm
    tqdm.pandas()  # This gives you access to the progress_apply method
    
    df = create_some_dataframe()
    df['result'] = df.progress_apply(some_function, axis=1)

There is almost no overhead to using tqdm and it helps you know right away if your function is going to finish in 2 minutes or 2 hours. It will update in real-time letting you know how many iterations/second it's doing, how long it's been running, and the estimated time to completion, as well as a nice progress bar that advances from 0% to 100%. | Great tutorial haven‚Äôt done a whole lot with pandas, something I want to do more of. Since I am not familiar can you explain how you got the times you used to plot the differences, I‚Äôd be okay if that was in a separate video. But when I ran I got the following using the `%%timeit`

Looping 1.71s + 6.68ms per loop
Apply 91.4 ms + 534 Œºs per loop
Vectorized 3.14 ms + 14.5 Œºs per loop

Just trying to understand how to convert those properly for the mean and std

Thanks again for the great tutorial | Not to be that guy but if you want your code to run fast you should just use numpy. As someone who learned pandas first and used it extensively at work, it took a while to accept this. | Vectorized just means that looping is happening at the c-level instead of at the Python level. 

Additionally, the arrays being operated on are unboxed homogenous typed- which is fancy talk to say that all of the values in the array are the same type, occupy the same amount of memory space, and are explicitly typed (instead of being Python objects).

Since each value in the array has the same amount of memory space reserved, the code at the c-level can loop over each of these values very quickly by skipping the number of bytes one value occupies to get to the next value.

This is different than a Python list- which is a heterogeneous container of Python objects. When you loop over a Python list (or any Python iterable) the code needs to retrieve the object the reference points to, inspect it, and the operate on it. The code has no knowledge of how large each item in the list is as it does with a pandas series or numpy array, which limits its efficiency when looping. | Great question! I'm not an expert on the backend of vectorized functions but I know that they are optimized in the C code level. In the numpy/pandas world they are optimized to work on arrays. This article goes into more detail: https://www.pythonlikeyoumeanit.com/Module3\_IntroducingNumpy/VectorizedOperations.html | Agreed. Working with raw numpy typically will speed things up because it doesn't require the overhead of pandas. Also I've had success with np.vectorize in the past. | Generally, loops are only ""bad"" when you're dealing with large amounts of vectorized data. For most Python users (at least to the best of my knowledge), this pretty much only happens with pandas/numpy, so you don't need to be afraid of loops. | I love using pandas query. But i don‚Äôt think it would work for this use case because we are creating a new column based on conditions from other columns. When filtering down a dataframe based on conditions I use query exclusively. | I never knew this, thanks! Even though, I always find np.vectorize to be significantly faster than .apply in pandas even on pandas objects. 

Numba is also a great package, I have significantly sped up some optimazation problems using it. You have to write your code carefully with a restricted set of functions though. | pandas is built on top of numpy, thus numpy will always be as fast if not (significantly) faster. | https://penandpants.com/2014/09/05/performance-of-pandas-series-vs-numpy-arrays/

numpy is indeed faster, but many times I will be working with data that contains many different types and I just find pandas to be much easier to work with in that situation."
1andqab,Why is pandas not compiled with -o3 by deafualt?,"Is it just me or does that seem simply bizzar. 
Like if you have gcc u can just get so much extra speed from doing that simple trick and as far as I can tell from their cython setup they don't seem to... 

Why is that?","I suggest you build it yourself with -O3 then you compare it yourself to the official build and do your own profiling.

For numpy and scipy they have extensive unit tests that are easy to run.

I suspect Pandas also has similar tests. So you could profile using Pandas overall built in tests.

---

I would also check to see if Intel python distribution has a panda package. Then test the Intel version. Their releases are very performance oriented. | because it might be perceived as bias towards pandas when there could be better alternatives | Ik intel has modin for faster pandas. Probably worth checking out how they are doing that.

I also have some connections to intel so I can maybe talk to my mentor and he will find whoever made that extension and ask.

Thx for the idea. If I would be looking deeper into it I will start there | Yes okay so I am a noob too but people here explained it fairly well. 

So when u compile with c there is what's called optimization level which is usually a number from 0 to 3 that tells the compiler how much time to work on making the code faster. 

-o0 means ""take this code give me the first assembly that works"" whilst-o2 and -o3 mean something like ""so this is gona run everywhere for a while do the best job u can making it fast"" 

Now pandas uses -o2 which aperently is standard? (Just learned that myself I was always thinking bigger is better but aperently 2 and 3 are as fast) 

Now I also noobed out thinking they are using -o0 because the build system wasnt the most obvious 
And that seemed just wild but they are not doing that I just missed the part they define it in their code.

Now does this actually matter for writing python? Probably not. I only care because I wana start cython so I am looking into how people build. | That was my first thought too. :D

And why isn't Pandas written in Rust, hm? ;) | I belive thats the deafualt cmake uses... 
Which is the most common c build tool. 
Also most c totriouls do it for realease builds again for similar reasons.

Can u share some sources of people discussing the trade offs? A pandas specific 1 would be nice. | Pandas uses Meson as a build system, in release mode, which should use -O2 or -O3 depending on the compiler."
f4oaag,"Bank statement analyzer GUI with pandas, matplotlib and PyQt5",,"Source code : https://github.com/arpanghosh8453/programs/blob/master/myprojects-Python_3/Bank%20Balance%20Analyser/Bank_GUI_open_source.py

Requirements : PyQT5, Matplotlib, Pandas | i want need in source code Bank statement analyzer using with pandas | I have it in my private GitHub because it has some sensitive info.

I can share the source code privately if you really want it ( I will remove the sensitive data )

Requirements: PyQT5, Matplotlib, Pandas, 

Re: For now it only supports standard Indian SBI statements(mine) and some german bank statements (for my brother). you may need to edit the code to make it work with your statement. ( you can send me your statement if you are unable to reformat it, I will try my best ) | I have it in my private GitHub because it has some sensitive info.

I can share the source code privately if you really want it ( I will remove the sensitive data )

Requirements: PyQT5, Matplotlib, Pandas

Re: For now it only supports standard Indian SBI statements(mine) and some german bank statements (for my brother). you may need to edit the code to make it work with your statement. ( you can send me your statement if you are unable to reformat it, I will try my best ) | I have it in my private GitHub because it has some sensitive info.

I can share the source code privately if you really want it ( I will remove the sensitive data )

Requirements: PyQT5, Matplotlib, Pandas, 

Re: For now it only supports standard Indian SBI statements(mine) and some german bank statements (for my brother). you may need to edit the code to make it work with your statement. ( you can send me your statement if you are unable to reformat it, I will try my best )

**I have made a .exe for windows. that. you may get** [**that. you**](https://that.you) **will lose the editability** | Requirements: PyQT5, Matplotlib, Pandas

Re: For now it only supports standard Indian SBI statements(mine) and some german bank statements (for my brother). you may need to edit the code to make it work with your statement. ( you can send me your statement if you are unable to reformat it, I will try my best )"
120mci9,pandas 2.0 is coming out soon,"pandas 2.0 will come out soon, probably as soon as next week. The (hopefully) final release candidate was published last week.

&#x200B;

I wrote about a couple of interesting new features that are included in 2.0:

* non-nanosecond Timestamp resolution
* PyArrow-backed DataFrames in pandas
* Copy-on-Write improvement

[https://medium.com/gitconnected/welcoming-pandas-2-0-194094e4275b](https://medium.com/gitconnected/welcoming-pandas-2-0-194094e4275b)","Thinking of moving some of my workload over to Apache Spark, previously just used NumPy.

Good timing by pandas, otherwise I would have had to switch to polars | Pandas is great for prototyping, but as a data engineer it‚Äôs a pain in the ass to keep changing code to handle interface changes or outright removals of functionality every couple years. I avoid using it in production where ever possible because of this. If I had one wish it would be that this project would change less. | Is Pandas ever going to implement a new API that isn't a pain in the ass to deal with? I find it impossible to tell what functions modify in place vs return a new dataframe as well as what things are functions vs attributes. Seems incredibly unintuitive and requires memorization, which sucks. | Using pandas feels like cheating. So great. | You should switch over to polars anyways if you're willing to rewrite legacy code, because in all benchmarks I've seen pandas is still \~3-4 times slower than polars. | This update won‚Äôt make pandas any close to polars. The pyarrow backend will only improve memory consumption and data read speed. Also maybe remove some weird behavior with types that pandas has. It won‚Äôt affect computations efficiency and speed. | It is a pain in the butt, yes.

But you can enable CoW right now. You don't need Pandas 2.0 for that. Use one of these three methods:

    pd.set_option(""mode.copy_on_write"", True)

    pd.options.mode.copy_on_write = True

    with pd.option_context(""mode.copy_on_write"", True):
      ...

More details:

https://towardsdatascience.com/a-solution-for-inconsistencies-in-indexing-operations-in-pandas-b76e10719744 | I just started getting into coding and this feature for me already seems very useful. Thanks to the team for working on this, any advice to someone trying to become more familiar with parsing through this field?

Also, how does one become a dev for an open source library like pandas? | > A long-standing issue in pandas was that timestamps were always represented in nanosecond resolution. As a consequence, there was no way of representing dates before the 1st of January 1970 or after the 11th of April 2264. This caused pains in the research community when analyzing timeseries data that spanned over millennia and more.

Though it seems like those dates aren't quite accurate on my machine. Probably some reason I'm not aware of. | Regardless of the performance points: polars is sooooo much more pleasant to use that I'd try to avoid pandas whenever possible really. | There are examples of some operations being faster. E.g. I think some string operations are noticeably faster. Of course, don't use pandas for 100M rows. | This would be a valid usecase for pandas: https://en.m.wikipedia.org/wiki/Astronomical_year_numbering

""""""Astronomical year numbering is based on AD/CE year numbering, but follows normal decimal integer numbering more strictly. Thus, it has a year 0; the years before that are designated with negative numbers and the years after that are designated with positive numbers."""""" | Have used both. I wouldn't worry about it for smaller data, depending on the particular operations used. Developer productivity will trump any improvements in execution for simple operations on a few million rows or fewer (assuming you'd have to learn polars). I do prefer the polars API (more functional and elegant) but am much more familiar with pandas so mostly still use it. | Yes, pyarrow is a memory model. It may improve some operations a little. Polars is superior thanks to (1) parallel execution (2) query optimization. None of this is coming to pandas (and can‚Äôt come without rewriting the package form scratch and breaking all the APIs)."
p98nmh,When Excel fails you. How to load 2.8 million records with Pandas,"Hey I'm back again with another quick and easy Python data tutorial loading 2.8 million records with Python and Pandas overcoming the Excel row limit.

https://youtu.be/nDixZvbhQZQ

I also encounter two additional errors with this one that we overcome. 1) Delimiter of the CSV being tab and 2) UTF-8 encoding error.

Feedback always welcome. Thanks for your ongoing support.","What module would you use if the csv file is so big that Pandas cannot handle (or maybe handle too slow)? I search google and people say that Vaex is a good module. Sometimes I come across a 12GB csv file, and that's terrified :((((( | > How to load 2.8 million records with Pandas

You don't. There is no reason to have that much data loaded at once. You need to save it to disk and only query the records needed, as needed. Use a SQL database or similar. If you are simply doing per-row operations, you can even do just fine with `csv`. | Don't need Python or pandas:  https://sqlite.org/index.html | Pandas.load_excel() | Having used both, I prefer the syntax of pandas. However it is a bit of a heavy dependacy, so for a small project `csv` is a better option. | Totally! Upcoming tutorials are going to be using Pandas to do a bunch of neat data analysis. | Pandas has a row chunking feature, if you don't need all the data at once you can chunk the process | Remember CSV is a text-based format that is relatively easy to read for humans. That means it is space-inefficient. Try compressing it to see how much space it really takes up.

I don't have a csv as large as 12GB handy, but I found a random one on my system that is 17Mb in size. I can compress it to around 3.5MB. That is about 20% the original size. So if we assume that compression ratio (this is a big assumption because I have no idea how repetitive the data is) we would have a zipped file of about 2.4GB which is pretty reasonable.

Ultimately, it really depends on the ram of the system - if the data will fit into memory. If it can, Pandas should be able to handle it. If not, then you have to use Pandas 'chunking' features and read part of the data, process it and continue until done.

Remember, the size on the disk doesn't necessarily indicate how much RAM it will take. You can try this, read the csv into a dataframe and then use [`df.memory_usage()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.memory_usage.html). That will tell you how much memory it takes.

Another thing to better understand, create a csv file. Load it into a spreadsheet and save it in the spreadsheet native format. Check the differences, the csv will be smaller. | Same here. I prefer Pandas but I cringe every time it‚Äôs presented as ‚Äúa solution for Excel‚Äù *for the wrong reasons*. | Hard to put an absolute number on this kind of thing, it depends and varies from case to case. Even in your cases it's very likely that you only need half as much RAM as you think you do when using pandas (potentially even just a quarter or less of the RAM depending on the precision you need). Pandas by default loads everything in as int64, float64, 64bit strings, etc. it's not very often you actually need that much precision/values of the magnitude this provides. Depending on your data you can specify to use dtypes that are a much leaner fit for your data (e.g. np.int8, np.float16, etc. and pd.CategoricalDtype for non-arbitrary string data).

e.g.

    pd.read_csv('file.csv', dtype={'a': 'category', 'b': np.float16, 'c': np.int8})

This example could provide a memory reduction of 83% over the default pandas load, and will often result in a dataset much smaller than even the size of your csv file (which as others have mentioned here, is a space inefficient file format). | If you have experience with Pandas I might start with Dask as the API is very close.

If you have access to Azure or AWS, trying a out a spark cluster is suprisingly straight forward and then you can use either python, SQL or scala to query the data.

There are also tools like Athena and Synapse which allow you to query a CSV file in place.

In the end it depends what you are trying to do. | > but critically it can handle input sizes greater than your system memory automatically).

Ohh I didn't know this. I thought we use spark to leverage mutliple machines with more memory than what your single machine would have. So theoretically, if a 16GB machine is having difficulty running pandas to open/manipulate a csv file that's like, say, 10gb, on the same machine spark/pyspark can do it? 

(Sorry, we only briefly tackled spark in class, and our datasets are usually very small so we didn't really use it a lot) | > If it's long term data it should be...

I didn't say **everything** should be stored. I said ""If it's long term data it should be..."" Don't be so quick to want to disagree that you don't bother to consider what I'm saying. 

Let me ask you this, do you process your log files with Pandas? That's what's being discussed. That's the type of data I was thinking about.

So i didn't consider your use case. Log files don't enter my mind when I think data. They are garbage files. Like you said, take what you need and get rid of it. | Yes\* but keep in mind that once Spark needs to start swapping data to disk things slow down drastically. In production environments you'd generally want to avoid that situation. Also, I'm not very familiar with Pandas but from other comments below it seems that Pandas also has options available to handle datasets larger than available memory.

---

\* This doesn't always work. Spark splits data into partitions for performing work, and while not all partitions need to be in memory, a single partition does need to fit entirely in memory. If any of your operations happens to produce a very large partition then the task will crash out. Some fine tuning of Spark's parameters may also be required."
tjodin,"LPT: Pandas DataFrames have a ""to_clipboard"" method","For those that don't know, Pandas has very useful to_clipboard and read_clipboard methods that make it easy to drop a DataFrame into an Excel sheet or to move it across python sessions without having to read and write CSV files. This is really useful for me and I hope it will help you too!","pandas clipboard integration is awesome! I also really like the `.read_clipboard()` method. You can use it to copy a table from an Excel worksheet and quickly get it into a DataFrame. | > drop a DataFrame into an Excel sheet 

could use [dataframe.to_excel()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html) | If you read a csv with pandas, then write to csv, pandas will write a new ""index"" column. The output csv is different from the input csv. | Absolutely. DataFrames have an implicit column for index, that it uses for finding rows in its tables. When you save the DataFrame to a CSV, the default parameters in the DataFrame.to_csv() method creates an explicit column with the index.

read_csv() however, just reads the column as data, meaning that you now have a new implicit index column that doesn't match what you had when you saved the DataFrame, and the previous index is now a data column in your DataFrame.

This makes sense when you think about it; reading a CSV file into a DataFrame without specifying that one of the columns is an index could either not use any of the columns as an index, or decide to choose one for you. They went with the not using any. Further, when saving a DataFrame as a CSV file you probably don't want to lose the index column, so it is saved next to the rest of the data. CSV doesn't have any concept of index, so it's just there as text.

_HOWEVER_, this means that if you do:
    
    df = load_some_dataframe()
    for _ in range(10000):
        df.to_csv('data.csv')
        df = pandas.read_csv('data.csv')

you will have 10,000 columns of garbage in the dataframe.

This example is a bit contrived, but it's caused me some unnecessary garbage in data just because I save a jupyter notebook at some intermediary step and reload the data. 

It's also one of pandas many stupid fucking quirks that make it super annoying to work with. | If only you could use a kwarg to set the index in Pandas when you read in a csv."
12fgyui,Pandas 2.0 (with pyarrow) vs Pandas 1.3 - Performance Comparison,,"lol, 14 times **SLOWER** to read the csv... I will stick to [polars](https://www.pola.rs/), I doubt I will move back to pandas. | I've always had issues with pandas groupbys. Find them very slow for the kind I need to do. Lots of optimisation  can be done, and pandas is very slow in a bunch of corner cases. Was forced to write our own approach to groupbys that reproduces the pandas 1.3ish result. Best case is minutes faster and worst case is seconds slower, and will not ever encounter the out of memory errors in pandas. | > But, surprisingly, aggregation operations were SIGNIFICANTLY (10X) SLOWER. The following aggregation took 40 seconds with Pandas 1.3.5, but ALMOST 7 minutes with Pandas 2.0.0.

My oh my. | How different is the api for polars compared to pandas? | You can also read the file with pyarrow directly:


    import pyarrow as pa
    pa.csv.read_csv(""hn.csv"").to_pandas(types_mapper=pd.ArrowDtype)


finishes in less than a second. | The ""Eager"" APIs are sometimes close, but most of the operations are different. But after a while you get used to the news APIs.

You can also convert from/to pandas dataframe anywhere, so you can move to polars progressively. | It's much much much better imo, it reminds me a lot of the tidyverse ecosystem in R. I really dislike pandas API, it's very messy | Just to give a different perspective than what others are saying here: the Polars API is much more cumbersome than Pandas, and this is something one needs to take into account. Sure, its API makes ""separate Polars expressions [...] embarrassingly parallel"" as the [documentation says](https://pola-rs.github.io/polars-book/user-guide/dsl/expressions.html), but there's a cost in terms of usability and understanding other people's code.

See these examples [from a post on HackerNews](https://news.ycombinator.com/item?id=35429555):

```python
Bump prices in March 2023 up 10%:

    # pandas
    prices_df.loc['2023-03'] *= 1.1

    # polars
    polars_df.with_column(
        pl.when(pl.col('timestamp').is_between(
            datetime('2023-03-01'),
            datetime('2023-03-31'),
            include_bounds=True
        )).then(pl.col('val') * 1.1)
        .otherwise(pl.col('val'))
        .alias('val')
    )

Add expected temperature offsets to base temperature forecast at the state county level:

    # pandas
    temp_df + offset_df

    # polars
    (
        temp_df
        .join(offset_df, on=['state', 'county', 'timestamp'], suffix='_r')
        .with_column(
           ( pl.col('val') + pl.col('val_r')).alias('val')
        )
        .select(['state', 'county', 'timestamp', 'val'])
    )
``` | What can't you do in polars that you can do in pandas? I haven't run into a single problem and we use it for relatively complex ETL in biopharma.

edit: wtf bro instantly blocked me üíÄ | For me, my only experience with r was when I did a stats class in my undergrad, I have much more practical experience with pandas and there‚Äôs quite abit of documentation/answers on stackoverflow that I can solve most of my problems fairly quickly. But if polars documentation is up to snuff, I wouldn‚Äôt mind switching if it really is that much of a speed up that I could forgo my familiarity with pandas. | Ah ya the api style seems to be abit more explicit and less intuitive (this is subjective) compared to pandas | There are a significant number of use cases where pandas likely won‚Äôt be usurped by polars anytime soon. Namely for modeling of financial, econometric and physical systems. 

I expand on my thoughts on this a bit more here:

https://np.reddit.com/r/Python/comments/12b7w3y/comment/jeyowci/

That said for things like data/feature engineering tasks polars is probably the best way forward. | If pandas works for you, then use pandas. You only really have to worry about the speed difference if you're doing large data stuff. If you are, polars is a 3-4x boost, but requires you to likely refactor. | > Bump prices in March 2023 up 10%:

    # pandas
    prices_df.loc['2023-03'] *= 1.1

    # polars
    polars_df.with_column(
        pl.when(pl.col('timestamp').is_between(
            datetime('2023-03-01'),
            datetime('2023-03-31'),
            include_bounds=True
        )).then(pl.col('val') * 1.1)
        .otherwise(pl.col('val'))
        .alias('val')
    )

Perhaps another way to write it could be:

    polars_df.with_columns(
        pl.when((pl.col('timestamp').dt.year() == 2023) & 
                (pl.col('timestamp').dt.month() == 3))
          .then(pl.col('val') * 1.1)
          .otherwise(pl.col('val'))
    )

Which is still quite verbose.

You could create some helpers:

    def dt(name, **kwargs):
        return pl.all(
            getattr(pl.col(name).dt, key)() == val for key, val in kwargs.items()
        )
        
    def loc(df, cond, expr):
        return (
            df.with_columns(
                pl.when(cond).then(expr).otherwise(pl.col(expr.meta.output_name()))
            )
        )

    pl.dt = dt
    pl.DataFrame.loc = loc

Allowing:

    df.loc(
       pl.dt('timestamp', year=2022, month=12), 
       pl.col('val') * 1.1
    )

Could probably be done better, but it's a potentially interesting example of what is possible. | That‚Äôs pretty nice. If polars adopted some standardized utils around this kind of syntax that would be a huge selling point. I‚Äôve also suggested (either here, or HN can‚Äôt remember) allowing a sort of pseudo index where you define ‚Äúmeta columns‚Äù and certain operations like arithmetic operators can do auto joins to get the desired `a + b` behavior.

I think if polars essentially provided a suite of utils to enable operations that ‚Äúlook‚Äù like they‚Äôre being done in a wide/indexed format, but on the backend is just doing long format operations, that would be huge for adoption in the pandas holdout use cases I‚Äôm talking about.

cc u/ritchie46"
12hixyi,Pandas or Polars to work with dataframes?,"I've been working with Pandas long time ago and recently I noticed that Pandas 2.0.0 was released ([https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html))   
However, I see lots of people pointing up that the (almost new) library Polars is much faster than Pandas.  
I also did 2 analyses on this and it looks like Polars is faster:  
1- [https://levelup.gitconnected.com/pandas-vs-polars-vs-pandas-2-0-fight-7398055372fb](https://levelup.gitconnected.com/pandas-vs-polars-vs-pandas-2-0-fight-7398055372fb)  
2- [https://medium.com/gitconnected/pandas-vs-polars-vs-pandas-2-0-round-2-e1b9acc0f52f](https://medium.com/gitconnected/pandas-vs-polars-vs-pandas-2-0-round-2-e1b9acc0f52f)  


What is your opinion on this? Do you like more Polars?  
Do you think Pandas 2.0 will decrease the time difference between Pandas and Polars?","I rewrote some old code that used Pandas to graph statistics recorded at high frequency over long periods. Think around 50 columns, and millions of rows. But the catch? Each row was a span of time, not an instantaneous measurement - and the time spans were variable and overlapping. (specifically, job logs on an HPC cluster).

The pandas 1.x code took about half an hour to run - mostly because of the majority of the work being stuck in a single thread.

I rewrote it in Polars (with the help of an expert on the Polars discord) and it brought that down to *three minutes.* In contrast to the original code, this was able to leverage the cores available properly. I did not touch anything directly to do threading, like reading files in a pool or such. 100% the doing of Polars, there. | From the little I've seen, Polars looks good, but I'm sticking with Pandas for now...  I do a lot of work with [GeoPandas](https://geopandas.org/).  When they release a usable version of GeoPolars, I'll take a look. | Disclamer: I'm a Rust fanboy.

I prefer Polars for the fresh new APIs that are well designed. Also if you have a beefy computer, it's nice to see all CPU cores working.

>Do you think Pandas 2.0 will decrease the time difference between Pandas and Polars?

Yes, there are a lot of potential to ""catch up"". Pandas 2.0 and Polars both use Apache Arrow, so hopefully pandas 2.x will reduce the gap. | Personally, I switched to Polars for most tasks and rewrote a lot of codebase in Polars instead of Pandas where performance mattered. I enjoy the speed.

As for other things, Polars is relatively fresh and doesn't come with as much baggage as Pandas does so the API and workflow is more consistent.

Some things are easier done in Pandas/Numpy compared to Polars and also Pandas community is larger.

I don't think Polars is yet accepted by major machine learning libraries so if doing a lot of ML you might want to wait a bit, although you can always convert Polars DF into Pandas or Numpy.

Overall, I think Polars is better ATM. | Depends what you're doing.

If you're doing data manipulation polars will let you parallelize your operations across multiple cores.

If you're using libraries designed to work with pandas, then you'll  need pandas.

If you're doing both it might make sense to do manipulations in polars and convert to pandas to use other libraries. | Depends, does speed matter to you? If your workflow runs in 10 seconds, learning a new library is probably a waste of time. Do you have memory or real performance problems with pandas? Switching might be worth it (there are other alternatives as well though).

Your environment matters as well, if your company wants to use pandas for example. Personally, polars isn‚Äôt mature enough yet for me and the maintenance model isn‚Äôt really clear yet. Also, pandas is fast enough for my use case | Well my team is all in on Pandas so that is what I use at work.

However, at home, I toy around with Polars to stay relevant in the industry. 

At the end, for me, either of them works as long as I get paid and I can go outdoors to do my things.

In my personal experience, working with day to day data, I have not felt any difference in terms if performance. For me, tge major factor is collaboration and business requirements. | (I‚Äôve been reposting variations of this comment several times)

Polars totally blows pandas out of the water in relational/long format style operations (as does duckdb for that matter).
However, the power of pandas comes in its ability to work in a long relational or wide ndarray style. Pandas was originally written to replace excel in financial/econometric modeling, not as a replacement for sql (not totally at least). Models written solely in the long relational style can be near unmaintainable for constantly evolving models with hundreds of data sources and thousands of interactions being developed and tuned by teams of analysts and engineers. For example, this is how some basic operations would look.

Bump prices in March 2023 up 10%:

    # pandas
    prices_df.loc['2023-03'] *= 1.1

    # polars
    polars_df.with_column(
        pl.when(pl.col('timestamp').is_between(
            datetime('2023-03-01'),
            datetime('2023-03-31'),
            include_bounds=True
        )).then(pl.col('val') * 1.1)
        .otherwise(pl.col('val'))
        .alias('val')
    )

Add expected temperature offsets to base temperature forecast at the state county level:

    # pandas
    temp_df + offset_df

    # polars
    (
        temp_df
        .join(offset_df, on=['state', 'county', 'timestamp'], suffix='_r')
        .with_column(
           ( pl.col('val') + pl.col('val_r')).alias('val')
        )
        .select(['state', 'county', 'timestamp', 'val'])
    )
Now imagine thousands of such operations, and you can see the necessity of pandas in models like this. This is in contrast to many data engineering or feature engineering workflows that don‚Äôt have such a high degree of cross dataset interaction, and in which polars is probably the better choice.

Some users on Reddit (including myself) have provided some nice example utilities/functions/ideas to mitigate some of the verbosity of these issues, but until they are adopted or provided in an extension library pandas will likely continue to dominate these kinds of use cases.

I‚Äôd also recommend checking out duckdb. It‚Äôs on par with polars for performance and even does some things better, like custom join match conditions. | I'm proficient in pandas (I learned python to access numpy and pandas) but I think polars is more straightforward and would be easier to learn from scratch. It is of course demonstrably faster, but I think the accessibility of the API (compared to pandas) is really where polars shines.

I think pandas will continue to exist for quite a long time because it is entrenched but there is now a superior alternative library in polars. As mentioned elsewhere in this thead, it is trivial to transfer data from polars to pandas so that is also an option where pandas is required. | I find Polars enjoyable, a feeling i dont get with pandas | Pandas. Because they are cute. Especially the red ones. | > I also did 2 analyses on this and it looks like Polars is faster

The size of the csv file in that benchmark is rather small - using a larger dataset would make it more interesting.

For example if we concat it 150_000 times:

    pd.concat([pd.read_csv(""taxi+_zone_lookup.csv"")] * 150_000).to_csv(...)

On my machine, the pandas version takes `40s` - the polars version `10s`.

The polars code uses `.read_parquet()` and `.read_csv()` which both read all of the data into memory.

It fails to use one of the main features of polars - the [Lazy/streaming API](https://pola-rs.github.io/polars-book/user-guide/lazy-api/intro.html) which supports larger than RAM datasets.

If we change the `polars_performance.py` to use `.scan_parquet()`, `.scan_csv()`, and `.sink_parquet()`, it runs in `3s`.

    start = time.perf_counter()

    parquet = ""yellow_tripdata_2021-01.parquet""
    csv     = ""taxi+_zone_lookup.csv""

    df_trips = pl.scan_parquet(parquet)
    df_zone  = pl.scan_csv(csv)

    (df_trips
     .select(""PULocationID"", ""trip_distance"")
     .groupby(""PULocationID"")
     .mean()
     .join(df_zone, left_on=""PULocationID"", right_on=""LocationID"")
     .select(""Borough"", ""Zone"", ""trip_distance"")
     .filter(pl.col(""Zone"").str.ends_with(""East""))
     .sink_parquet(""out.parquet""))

    print(time.perf_counter() - start)

Some other features of polars I've found useful are that its columns are ""strictly typed"" and it has actual list/struct values, semi/anti-joins.

Calling polars ""a faster pandas"" is an over-simplification and ignores many of features it actually has.

It's also not a ""drop-in"" replacement for pandas, so trying it for yourself would be the best thing to do if you are curious about it. | I can see that this is definitely not a easy discussion.   


I see people talking about ""if you want speed then use PySpark"" but that implies the installation of Spark. 

Do you really think that Pandas 2.0 is that increase in speed regarding 1.x ?

From what I see so far it looks like the ""conversion"" to Polars doesn't seem complicate. 

Even some functions looks like the same between Pandas and Polars.   


But, by other hand, it looks like Polar more a ""hype"" stuff than a huge improvement on working with dataframe.   
And like someone said here: Lots of other libraries are made under the scope of Pandas.

I think I will keep on working on Pandas and sometimes to try out Polars to see if it matters to change or not. | Sometimes I suspect some vested interest is pushing this issue. You know how many faster-than-pandas data frame libraries have failed to gain critical mass? I wouldn't waste my time on polars. 

If you really need speed, use pyspark, pandas 2.0, or multiprocess + pandas. Or just use numpy...at least you're still using a common library without having to learn something totally new and annoy your colleagues | pandas takes the liberty of interpreting your data. if it looks like a date it will convert it to a datetime object. no matter that dates are inherently ambiguous (usa vs. the world...). I don't know about polars and I don't care about performance that much, i just want a library that does NOT make assumptions for me. | Is pandas a large package? I wanna use it in my program but I really just need one of its functions.
I don't wanna make it a dependency if it's a large package | That's very interesting lol.. I'm working with pandas but I think I've reached the max ceiling of optimizing it. 100+ columns and prob about a million rows. I'm gonna look into Polars now! | Same. I can't wait for GeoPolars to get here/be usable. Until it does though I'm stuck with GeoPandas. | And Polars also works in Rust, which Pandas doesn't, so there's a shared API there. | This is not entirely correct. You can still do most of the work on Polars and then use `.to_pandas()` when needed | I disagree a bit on some of your comments.

Regarding long vs wide format, you can only multiply an entire pandas dataframe by a factor if you do it to every column.

You cannot selectively increase all the columns of float type by 10% and increment int types by 1. You cannot (directly) select columns by pattern matching the column name, etc... All of which are supported by polars.

So polars support for wide data is actually more powerful than pandas. Pandas can only do these big operations on the wide dataframe if it is a pivot of long format.

Certainly the polars examples are more verbose, but that can be handled with utility functions. The challenge is getting an API for these operations that makes sense for a broad class of use cases. | > But, by other hand, it looks like Polar more a ""hype"" stuff than a huge improvement on working with dataframe.

I think you are ignoring the significance of having an API that treats dataframes as immutable. The value in that is enormous and mutability causes all kinds of problems for pandas code.

Mutability:

* makes optimizing pandas operations very hard.
* causes a lot of bugs in programs written in pandas.
* and it turns the API into an inconsistent mess.

Polars and spark have significantly cleaner APIs almost entirely because of the design decision to require immutability. It does make certain tasks a little more verbose and a little harder, but it is worth it if your objective is to write code that will be supported for a long time as part of a larger program.

Depending on how you write your pandas code, converting to an immutable dataframe could be a very minor change or a major change. The difficulty is you don't know how exactly it was written from a cursory glance at it. Plenty of people go out and write pandas code that treats dataframes as immutable everywhere except for one line... and finding that line is as much work as rewriting the whole thing from scratch.

------

If you are writing one off routines, pandas is probably fine.

If you are writing as part of larger programs or with a view to maybe moving to large clustered systems, you should be thinking in terms of immutable dataframes and using either polars or spark to compel you to implement in terms of immutable structures. | >You know how many faster-than-pandas data frame libraries have failed to gain critical mass?

Can you name a few which had as many users as Polars? | Dealing with datetime formats in pandas has been the bane of my existence for years. 

(If you want it to just treat dates and datelike columns as strings when reading in data, I‚Äôm pretty sure there‚Äôs an option for that. It was the default behavior for a long, long time.) | Dataframes for the operations that I'm describing aren't formatted in the way you seem to be describing. When I say ""wide format"" all the values in the dataframe would be of a homogenous type and represent the same ""thing"", and your columns and index levels would represent the different dimensions over which that thing exists, and the labels in those levels would represent the coordinates in that multidimensional space. You can think of this as an numpy nd-array with labeled coordinates instead of positional coordinates. You would never distinguish a column by its type in this format. You potentially could want to pattern match the dimension labels (which is possible with pandas, not sure why you say it isn't) but that's normally not ideal, and I'd argue that's an anti-pattern in these use cases. You'd normally want structured access to columns through a proper multiindex hierarchy. | Polaris has functions for exporting data to other formats, such as Pandas `DataFrame`s

https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/export.html | That is what I am talking about.

Example:

* Long format where columns are date, stock ticker, close price

* Pivot to wide and columns become date, ibm, msft, ....

With long you compute moving averages using grouping which is similar with polars and pandas.

With wide you do it as a moving averages down columns

No big differences so far.

----

But now consider adding volume to the long format.

Wide becomes: date, ibm_px, ibm_vol, msft_px, msft_vol

If I want to do different things to vol and px that is harder with pandas than with polars. | It's definitely interesting to see competition in the data analysis library space, with Polars showing promising speed improvements over Pandas. However, I think it's important to consider other factors besides just speed, like ease of use and availability of resources. Do you think there are any downsides to using Polars over Pandas? | Totally makes sense but often the to_Pandas or to_dmatrix step has become the bottleneck in my personal experience, negating some of the preprocessing speed gains. | If you're going to perform detailed statistical analysis, Pandas might come with all the bells and whistles you need, Polars might not.

Ease of use, I personally find Polars' syntax to be more intuitive. | From my (very limited) use if Polars, I appreciate that Pandas is more clumsy when it comes to types (and especially mixed types). Sometimes you just want to load some data and look at it, not clean up mixed types (or specify columns to be string/utf8 to work around it). However, the cost is that the code runs a lot slower. | The conversion to pandas is now zero copy in pandas 2.0.

And before that it was cheaper than a reset_index in pandas. 

Did you benchmark this? A memcpy is almost never the bottleneck of an OLAP pipeline. | > Pandas is more clumsy when it comes to types

I would expect that to have changed with pandas 2.0 (due to use of Apache Arrow) - do you know if that's the case? | I think we'll just end up agreeing to disagree, but I'd like to illustrate my use case a bit more anyway. We have many teams, each team develops and maintains around ~2-4 models. One of the main models my team maintains is ~1000 unique functions, forming a function dependency graph, with an average of 20 lines per function. Generally, not always true, but a good rule of thumb, each function returns a single dataframe in this homogenous wide style. 

For example it might look something like this:

    def pizza_cost():
        dough = dough_cost()
        sauce = sauce_cost()
        toppings = toppings_cost()
        return dough + sauce + toppings

    def sauce_cost():
        tomato = tomato_cost()
        water = water_cost()
        return tomato + water

    def tomato_cost():
        tomato_retail = tomato_retail_cost()
        discount = bulk_discount_rates()
        my_discount = _get_discount_rate(tomato_retail, discount)  # this function not part of function dependency graph
        return tomato_retail - (tomato_retail * my_discount)

We have thousands of these types of functions, much more complex than this obviously, but this gives you an idea. You can see how the verbosity can become very unmaintainable at the scale of code I described above, and how utility functions where you need to explicitly specify meta columns becomes an additional maintenance burden. Also breaking up the problem into small modular chunks like this mitigates a lot of the issues you describe with hidden behavior. When things are broken down like this, identifying where an issue is becomes trivial. With this structure in fact you even mitigate a lot of performance issues through distributed graph execution, to the point where the difference between pandas and polars is negligible, and ease of reading and quickly expressing your ideas becomes more valuable. Sure it would still be faster if you wrote it all in polars and executed the way we do, but at that point the marginal performance benefits don't outweigh the maintenance costs and added cognitive load.

By the way I want to plug the awesome fn_graph library (https://fn-graph.readthedocs.io/en/latest/usage.html) which we have forked internally for our orchestration platform. For parallel graph runs you can see examples of how to do this in a similar library called Hamilton (https://hamilton-docs.gitbook.io/docs/extensions#scaling-hamilton-parallel-and-distributed-computation).

All that said there's no denying that polars performance blows pandas out of the water, and we do actually use it in several places for easy performance wins that still end up being a bottleneck in our models. | I found it similar to Pandas in my usage. But I must admit, I haven't used it extensively. Can you give an example? | Checked this right now, seems like it accepts it but gives a warning. Its the best way to deal with it for my use at least.   


    import pandas as pd
pd.__version__
    
    >'2.0.0'



    pd.read_csv('filewithmixedtypes.csv', delimiter=';', decimal = ',')
    
    >/tmp/ipykernel_447640/2659548876.py:1: DtypeWarning: Columns (12,14,16,19,21,22,23,25,26,27,30,39,91,92) have mixed types. Specify dtype option on import or set low_memory=False.
  pd.read_csv('filewithmixedtypes.csv', delimiter=';', decimal = ',') | [Here's some of the polars code in a report script I ported from Pandas pre-2.x.](https://pastebin.com/j4smt3uU) `df` is a Polars LazyFrame. I should note this was my first rodeo with Polars and my experience doing this sort of thing previously was almost nothing, so I may be doing things stupidly.

I'm posting it on pastebin so you get syntax highlighting, and it's of enough length and breadth that Reddit's formatting would *suck*. | These systems aren't  quite rebuilding significant portions of dask (which I am a contributor of several features to, mainly dask.distributed, which is more concerened with the distributed compute side of things rather than the data api. Just added to say that I understand the scope of the library well) or spark, or Ray. They are just using them for the underlying execution. You could say that it's rebuilding a significant portion of something like the dask.delayed interface, which could also be leveraged to build similar functional dependency graphs, but the fn_graph approach is significantly distinct, and is invaluable for scenario and sensitivity analysis. With the dask.delayed approach you need to explicitly link specific functions to be dependent on other specific functions. In my pizza example you would need to do something like this:

    tomato = tomato_cost()
    water = water_cost()

    dough = dough_cost()
    sauce = sauce_cost(tomato, water)
    toppings = toppings_cost()

    pizza = pizza_cost(dough, sauce, toppings)

This is nice, but it requires an explicit imperative function call chain to build the graph. What if I want to price a pizza with bbq sauce instead of tomato sauce, I would have to update this workflow and switch out the function and argument passing. With the fn_graph approach, the dependency graph is built implicitly and if you want to switch an implementation out you can just do `composer.update(sauce_cost=bbq_sauce_cost_function)`. Or if you wanted to run a suite of discount rate distributions, you can make several composers with a simple `composer.update(bulk_discount_rates=lambda x: x + 0.01)`. You could build some machinery around dask.delayed to do something like this too, with a config approach, without having to do the explicit imperative workflow, but obviously it would need to be built as it's not provided in dask.

> there is obviously a lot of structure in your models that are enforcing through policy not code

What you call enforcing through policy here, is an industry standard practice in the quantitative/financial modeling space. It's good model design, and to not use modular components in this style will lead to hard to maintain code whether you use pandas or polars.

> It is critical to the well functioning of your code that every function return identically indexed dataframes otherwise those additions don't do what you expect them to

It is not a requirement that they be identically indexed, but rather have identical schemas (e.g. same index/column levels). I do see your point about unexpected broadcasting if your levels don't align the way you initially intended, but again these issues are not as common as you seem to suggest when you have a very explicit lineage of data operations and sources.

> I wouldn't advice others to rely on arithmetic operations between independent dataframes because it just obfuscates the critical need to ensure that indices align

This style of operation has been one of the fundamental bases of numeric computing for the past 60+ years. I don't think I would necessarily suggest that people shy away from a concept that has backed an entire quantitative field because it takes a bit of extra discipline to do properly. Even now, no one is suggesting not to use numpy where it's appropriate. 

Anyway my main original point was that polars is definitely a better choice in many (maybe even most) data engineering/science workflows. But there are some fields in which it would need to implement some convenience features/wrappers to gain a foothold (which it totally could). The fact of the matter is that based on my observations and conversations with those in this industry, these functionalities are a strict requirement, and most people won't switch over completely in these fields (or at for these kinds of modeling tasks in these fields) without them."
xkjs92,Storing Pandas data frames efficiently in Python,,"Can pandas lazy load data?  I haven't used it much, but I was playing with about 700MB of parquet with pandas and it took like two minutes to load. Super painful. Wondering if arrow or duckdb might be better than pandas for parquet. | fwiw, you should never save to a csv if you can help it, because it mangles type data which can cause bugs in code.

Instead either save to a pickle file or a [parquet file](https://stackoverflow.com/questions/41066582/python-save-pandas-data-frame-to-parquet-file) is the recommended solution. | .feather is also a good format, in some cases faster than parquet. Developed by the founders of Pandas and R.

https://link.medium.com/Ef07kGGpxtb | I don't think pandas can lazy load. But I've had the opposite experience--much faster read and write with parquet over csv in pandas. 

Duckdb is nice if you want to avoid ingesting the files, yeah. Very convenient. | I could, although I'd add some complexity. But even a simple aggregation like COUNT(col1), COUNT(col2), etc still took ~12 seconds to evaluate, vs ~1s in parquet + pandas."
vuznid,"Why should I choose Anaconda if I can install pandas, numpy and jupyter notebook already in Python?","No shade towards people behind Anaconda but all I see is it is just Python that comes with things like Pandas, Numpy and Jupyter Notebook built in. What special will Anaconda do that I cannot already achieve in base Python?

It may be necessary for Math professors who do not feel it being their part of the job to setup development environment and want to go straight to coding. But for us folks who know how to use pip, what practical benifit does Anaconda bring?","Intel packages its own optimized version of Python, NumPy, SciPy, numba, tensorflow, modin, pandas, and others. They are optimized to run more efficiently only on Intel architect.

Intel has partnered with Anaconda for the distribution of those packages. You can access them using `yum` & `apt` also.

More can be read here:

* [https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-for-python.html](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-for-python.html)
* [Installation using conda](https://www.intel.com/content/www/us/en/developer/articles/technical/using-intel-distribution-for-python-with-anaconda.html)

So if you use core Python numerical & scientific packages on an Intel CPU you might wanna check it out.

Intel optimized packages on Anaconda:

* [https://anaconda.cloud/intel-optimized-packages](https://anaconda.cloud/intel-optimized-packages)
* [https://anaconda.org/intel/repo](https://anaconda.org/intel/repo) | I‚Äôve used it primarily for installing libraries like rasterio, gdal, pyproj and geopandas. | If I was doing data science on the daily, I‚Äôd probably use conda, it‚Äôs overkill and cumbersome for what you‚Äôve described above.  I love using Jupiter lab for iterating on scripts, I use pandas for most manipulation of data where application load time isn‚Äôt a problem, and otherwise I‚Äôm a python developer and infrastructure guy so conda gets in the way of my day to day workflow and is pretty bloated for what I need.

If data science was my day to day instead of application and tooling, I‚Äôd probably be recommending conda for that. | Good package management and the conda repos usually give you precompiled packages. I've had problems before getting things like Geopandas to install on windows using Pip because one of the deps is a bear to compile, but anaconda ships the precompiled libs."
x4u928,Level up your Pandas skills with query() and eval(),,"That was great! I‚Äôve been using pandas for years and I always have to have 50 windows tabs up with various search terms any time I ever need to filter a dataset  by a particular column! | Insightful! Incidentally, when i first started using pandas (& python), i used .query() all the time before i understood how to filter with brackets. But honestly haven‚Äôt used it much since ‚Äî good reminder to utilize it in more contexts! | Thanks for this. I was wondering why more people didn‚Äôt use query() in tutorials. I‚Äôm glad there‚Äôs no good reason not to use it. 
Also, thanks for the comments, I‚Äôve learned that the built in eval() is different than pandas eval. | Nice! I am a beginner in pandas this was very helpful! Thanks! | Oh cool, didn't know that! You can always learn something new with pandas haha. Still less concise than eval though. | Thanks for reading, always something new to learn in pandas :) | Pandas eval is not the same as the builtin eval. | What aspect of pandas syntax do you find impenetrable? | Question wasn‚Äôt directed at me, but I have the most problems with remembering how many brackets to put around things and how to do if/then logic with pandas (ie, if ‚Äúusername‚Äù == ‚Äúd_composer‚Äù then ‚Äúability_to_ever_understand_git‚Äù = False) | Thanks for the clarification. It should be hardcoded. I'm not talking about the standard python eval() function in the article but the pandas.DataFrame.eval()-function. What you say is true for the standard eval() but the pandas equivalent can be used with hardcoded values and be useful. | You can read up on this at 

https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

Or check out Joris van den Bossche‚Äòs talk about this at Pydata Berlin

https://www.youtube.com/watch?v=aBeEN2klZQE&list=PLGVZCDnMOq0p0Fal8_YKg6fPXnf3iPtwD&index=3

This isn‚Äôt something you can explain comprehensively in a couple of sentences.
Indexing is really powerful but also takes some time to really get into it. Depending on the use case, there is a big difference between a regular __getitem__ or __setitem__ and doing a similar operation with loc | It will be slower, pandas is migrating to differentiating those two in their shallow copy (or not) behaviour | Depending on your use case, you could also check out Dask

It mirrors the API of Pandas (and other libs) and allows for parallel computing. That could cut your 6 minutes down to a few seconds, if not less."
n8l35m,Iterating though Pandas DataFrames efficiently,,"If you're looping in pandas, you're almost certainly doing it wrong. | Does learning these library help in getting job like pandas,numpy,scikit cause i found nowdays companies have their own tools and software to tackle data related problems these are now become basic knowledge?
Well i am new to this side of python i am flask guy or little bit django.
Plsss anyone clear me out can these things help in analyst job? | Ah yes you are correct there! I've modified the function to make it a little more comparable: 

    def using_iteritems(): 
        data = create_data() 
        for index, row in data.iteritems(): 
            for val in row: 
                sum = val + val

Here is how long it takes to run each one 100 times(rerun them as recording slows them down):

List Compr 2.329638

to_list Loop 2.4328289

vec 0.6680305000000004

Pandas itertuples 7.0313863

Pandas iterrows 518.6045999999999

Pandas iteritems 3.724092200000001 | Lots of them will borrow ideas from pandas and it gives you a frame of reference, so yeah it's a good idea. I suppose there will be the occasional person who will complain that it ""taints"" freshman people into thinking that's the only way of doing things but I think that's a moot point because a good programmer is always going to have to be able to learn new things and new paradigms. | Agree that absolute statements are not helpful, but from my experience, the vast, _vast_ majority of cases where people use loops on pandas DataFrames there are vectorized equivalents.

Does it matter in a one-off script where the DataFrame has 1000 rows? Maybe not. But shouldn‚Äôt you want to learn the more efficient and concise way to do it? | Hm not necessarily, in those cases it‚Äôs good to use ‚Äòdf.apply‚Äô or ‚Äòdf.applymap‚Äô

‚Äòapply‚Äô isn‚Äôt necessarily any faster than for loops, but it aligns with the standard pandas syntax (transformations via chained methods) so most people seem to prefer it for readability | If it was a blanket statement, I would have said something like ""looping in pandas is always wrong"", which you'll notice I didn't. | In the case of pandas I think this blanket statement is valid. There are cases where there‚Äôs no good vectorized way to do something, but those cases are rare. Vectorized operations should be the default way of think IF you‚Äôre serious about writing proper ‚Äúpandonic‚Äù code. And anything else should be a last resort. If you‚Äôre just messing with small frames or don‚Äôt care about speed then sure no need to vectorize, but it would still be good practice to. | This may be a case where you may want to use shift, like 

    mask = df[[‚Äòfoo‚Äô]].join(df[‚Äòfoo‚Äô].shift(), rsuffix=‚Äò_shifted‚Äô).apply(your_condition)

https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html | [https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6](https://engineering.upside.com/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6) | I'd say avoiding it is mainly useful in the long run, a  lot of times you loop through the df because you don't have time to look into another way of achieving the goal and don't worry about whether the implementation will have to eventually scale with time. 
 
I've had to rewrite some stuff made using iterrows because when it was written, scalability was not taken into account. For some of the rewrites, it took quite long, because you have to condense several lines of logic in those for loops into few pandas methods, making sure you're not introducing any new pathways for bugs. If you take the time to do it with vectorization since the beginning, it's way more unlikely you'll have to go back to it ome day to make it faster. | Standard python with dictionaries and lists are way faster and straight forward to implement for that use case.

You should try to stick with bulk operations with pandas because that's where it shines. | Pandas and numpy have lots of precompiled operations in their libraries, so if you do things to whole dataframes & series, you're typically running at the speed of compiled C.

If you're iterating by hand in Python, you're going up to Python level after every operation, and that can be ten or a hundred times slower.

If it's a small dataframe, then the difference between 0.06s and 0.6s doesn't matter much if you're only doing it once. But it starts to add up with big dataframes, and it adds up even more if you have a more complex algorithm that isn't just looping once through the whole thing (eg if you're writing a sorting algorithm by hand) | Is pandas `apply()` similar to `apply()` in base R? | In this case, if you're sticking to pandas, probably not. | This was mentioned above, but consider creating a copy of the array/series of interest but shifted by the relative amount needed.
> https://pandas.pydata.org/docs/reference/api/pandas.Series.shift.html | Yeah sure, although it may not be faster.

Define your function on the row:

    def row_func(row):
        csv_file = ...
        ... do stuff

Use [`apply()`] (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) along rows:

    df.apply(row_func, axis=1) | Thankfully that‚Äôs no longer true, they changed that since it was causing some bugs:

https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.1.0.html#apply-and-applymap-on-dataframe-evaluates-first-row-column-only-once | Well there's nothing wrong with using pandas if it works for you. What is the nature of models you're building? | Apparently they fixed this behavior about a year ago, so it‚Äôs not true for current versions (and tough to find documentation)

But you can see it in the changelog here https://pandas.pydata.org/pandas-docs/dev/whatsnew/v1.1.0.html#apply-and-applymap-on-dataframe-evaluates-first-row-column-only-once"
13pjzsz,Polars vs. Pandas in 2023,"Need to switch from r-data.table to Python in my new job. 

Is polars ready for prime time? Should I study it as a replacement for my previous API or *pandas* will still be the standard in 2023-2024-2025? 

I would say I never used the most advanced features of data.table. 99% of the time is pivoting/groupby/filter/aggregations","If you're familiar with pandas, learning polars shouldn't be too hard. In my experience, it's faster and handles larger datasets better than pandas. Plus, the Rust language it's built on is dope. Give it a try! | Two important benefits of Pandas: 1. Good integration with data stack, such as sklearn, plotting libraries, awswrangler‚Ä¶ 2. Everyone knows it (at least everyone thinks they know it). Including GitHub Copilot‚Ä¶

Other than that, I have to say I love polars. It‚Äôs faster and much more powerful than pandas (window functions!). I‚Äòve been using Polars for almost a year now and my experience is entirely good.

By the way the discussion largely revolves around performance, but Polars is really a much more powerful and well-designed library that will speed up your data wrangling (I mean the time to write code). So in my opinion, it‚Äôs valid to say use Polars unless you need to use Pandas. And other than its popularity, there aren‚Äôt many areas where it excels. | I personally would just stick with Pandas. Every time I've tried to switch any of my work from Pandas to Polars I hit some snag and have to switch back. I'm sure for some sufficiently simple code the speed benefits are worth it but for anything even remotely complex it becomes a big exercise just to replicate the same functionality that a few lines of Pandas will do. | A lot of people here are talking about speed needs and dataset size. But as someone very experienced in pandas and recently learned polars, I find polars way of doing things (expressions and operations) WAY more intuitive than pandas.
Considering this and the fact you mentioned your needs are basic, I‚Äôd go with polars. There are some corner cases that right now are much easier to solve with pandas, but you can always convert to a pandas dataframe in the middle of your pipeline, convert it back to polars and go on. | My stack is primarily Parquet, PyArrow and DuckDb, and then whatever is required by the rest of my pipeline. 

While Polars is better than Pandas, both ultimately are an endless convoluted chain of API trivia. 

I end up with a lot of pipelines that need numpy data types, so Pandas is usually somewhere in the pipeline.

So, my advice is to minimize your Polars/Pandas code and use duckdb for most of the heavy lifting. Its shocking how good it is (I‚Äôm unaffiliated, just a huge fan) | Working in finance the big thing pandas ha going for it still is superior date manipulations with calendars and business date logic. If I did not need that I think I would switch. | I love polars and use it whenever possible. In the ""worst"" case of using polars when pandas is required, you can easily convert dataframes back and forth using polars. | Pandas V2 can use PyArrow as a backend. It's still not as fast as polars but it's close | As probably one of the heavier pandas users here and a contributor of several features and bug fixes to pandas, I would say start with __polars__ too. While pandas does still have a significant place in the python data ecosystem (beyond just seniority and wider integration/compatability, it does handle significant use cases that polars does not, and does not plan to) most people won‚Äôt be dealing with those use cases and would most likely be using the features that polars excels at.

For more detail on some of those use cases see my comment thread here:

https://np.reddit.com/r/Python/comments/12hixyi/comment/jfrxti6/ | Depends on a bunch of factors - what type of data, how large is it, how fast do you need it to be etc

Personally, I‚Äôm sticking to pandas. More mature ecosystem around it, support for seaborn plotting and easy to use. | Answer me this and I'll be impressed: can you perform a group by but the aggregation requires using more than one column of the subset? Pandas would be

pd.grouby('player').apply(lambda x : x.loc[x.winner.eq('yes'), 'score'].mean())

If not then Polars isn't ready for show time. | After playing with pandas, that's exactly my major annoyance: too many different ways to achieve the same. I get it makes sense to the seasoned user (that's my relationship with many of the R facets: I know it could be improved, but I got used to it).
Do you think integration with other packages will be an issue? | >there isn't 50 different ways of achieving the same thing.

Agreed. There are things that are still easier to do using pandas but I find it healthy to ask myself if those things even need to be done.

My code is far easier to understand when I go back and look at it without pandas. | I despise the pandas API. It‚Äôs what happens when devs try to accommodate R, matlab and others while also being pythonic. Matplotlib has a similar issue. 

It can‚Äôt be done IMHO. It‚Äôd be better to fork pandas than have what we have. | Is Polars faster for doing math operations on columns and things like that - I thought pandas did that with numpy (might be wrong) | Agreed on the recommendation to start with polars. But disagree with pandas‚Äô popularity being its main strength. See my comment thread here on how pandas is still highly relevant in the python data ecosystem:

https://np.reddit.com/r/Python/comments/12hixyi/comment/jfrxti6/ | This is not a good solution in pandas. Apply is slow, filtering before and then doing the mean is significantly faster | I would at least create a short list of packages that you know you will need. If one of them complicates the workflow using Polars, stick to pandas. I don't have any of these cases. 

I use parquet for storage and the Polars implementation is even more native. I also like writing ""stupid"" Polars code which is still 10x faster than pandas and then improving it when I need to speed it up by likely another 10x.

I'm sold on polars. I'd rather use a modern, fast and light API that might miss a functionality here and there, instead of dealing with this heavy old machinery of something that I never quite enjoyed in the first place. Some things in pandas are just frustratingly slow... Plus, the devs at Polars are so quick in fixing bugs or adding new things. Just open an issue, stand your case and if it's a good idea it might be live 4 weeks later. | If you run into a integration issue with polars, just use .to_pandas() and continue as normal. You can convert it back later if needed. | Yeah coming from R data.table to pandas was huge source of frustration for me. Even now sometimes when I am doing some personal stuff I load data in data.table to run some quick summary etc.

On the other hand pandas has more mature ecosystem and if you want to do anything other than just wrangling data locally, you'll come across these issues. So if you are working all by yourself, I would say go with polars. On the other hand if you are working in a team and you don't have to optimize some analysis, then stick with pandas. | I mean, I still work with a lot of Fortran but I wouldn‚Äôt expect a python library to have column-major, 1-based indexing option just to make me comfortable. Although now that I say this, pandas devs please add this! :/ | This is just based on my personal preference but if you don't have any problems with the speed of Pandas, and don't expect your dataset to grow in the near future I would do the switch later on. I did the switch but just as I did I also started using DuckDB and see that I do much of the work in DuckDB, so I might as well keep using Pandas. In any case, both of these are good, and if you just enjoy learning new stuff I would go ahead with Polars. You'll very soon find out if you think it is the right fit for you. | If performance is critical for you then polars, otherwise I‚Äôve used pandas for similarly sized data with no issues.

I don‚Äôt think you‚Äôll really have trouble, more plotting libraries will probably add support for polars. Right now it‚Äôs limited to just plotly express. | You can totally do this in polars, and is way more readable than the pandas way.

Edit: I think it would be something like this:
`df.groupby(‚Äúplayer‚Äù).agg(pl.col(‚Äúscore‚Äù).filter(pl.col(‚Äúwinner‚Äù) == ‚Äúyes‚Äù).mean().alias(‚ÄúMean score when winning‚Äù))` | So, a few things. First I wouldn‚Äôt call it the ‚Äúpandas‚Äù way or the ‚Äúpolars‚Äù way. You could do what you call the ‚Äúpolars‚Äù way, in pandas, albeit slower. You could also do it in duckdb, sql etc. it‚Äôs the relational/long format way. And you could do what you call the ‚Äúpandas‚Äù way in numpy, xarray etc it‚Äôs the ndarray/wide format way. 

That said, I do completely 100% agree that the ‚Äúpolars‚Äù way is more robust. But that‚Äôs not main determining factor in whether to write your models that way. If you read my linked comment thread, you‚Äôll see that these models get to tens of thousands of lines of code. To switch to the ‚Äúpolars‚Äù way would mean 100+ thousand of lines of code, and those lines are also harder to intuitively understand what they‚Äôre doing from a mathematical sense due to the extra boilerplate. Also as I mentioned if you need to add some dimensionality to a dataset, you need to go find every downstream operation of that dataset and update every single one with the new dimension, until that dimension is reduced out. This just isn‚Äôt a feasible way to develop a highly iterative/dynamic model.

Also I wouldn‚Äôt say the ‚Äúpandas‚Äù way is ‚Äúextremely‚Äù inviting to bugs. There definitely are some gotchas with the ndarray style way. For example adding a dimension to 1 of 2 frames in an operation, might sometimes still yield a valid albeit nonsensical result, but these resulting frames can not usually propagate too far since the resulting structure is often ‚Äúall jacked up‚Äù. And like I mentioned in my comment thread. Proper modularizarion/organization of models makes these cases extremely easy to identify and fix if they happen.

Keep in mind this is a very specific use case of dataframe libraries. The vast majority of data problems don‚Äôt have these problems, and polars is likely the best solution for most of those cases. But it‚Äôs just not quite viable __yet__ in this specific use case at this specific scale. | With pandas 2.0 switching to pyarrow, does this change the performance improvement aspect? | They have not yet fully embraced arrow.compute so the compute functions should still be pandas implementation, which means for most cases they should perform similarly. If they will move toward acero compute engine or in general rely more on arrow compute capabilities the performance should increase accordingly."
12b7w3y,Everything you need to know about pandas 2.0.0!,"Pandas 2.0.0 is finally released after 2 RC versions. As a developer of Xorbits, a distributed pandas-like system, I am really excited to share some of my thoughts about pandas 2.0.0!

Let's lookback at the history of pandas, it took over ten years from its birth as version 0.1 to reach version 1.0, which was released in 2020. The release of pandas 1.0 means that the API became stable. And the release of pandas 2.0 is definitly **a revolution in performance**.

This reminds me of Python‚Äôs creator Guido‚Äôs plans for Python, which include a series of PEPs focused on performance optimization. **The entire Python community is striving towards this goal**.

# Arrow dtype backend

One of the most notable features of Pandas 2.0 is its **integration with Apache Arrow**, a unified in-memory storage format. Before that, Pandas uses Numpy as its memory layout. Each column of data was stored as a Numpy array, and these arrays were managed internally by BlockManager. However, Numpy itself was not designed for data structures like DataFrame, and there were some limitations with its support for certain data types, such as strings and missing values.

In 2013, Pandas creator Wes McKinney gave a famous talk called ‚Äú10 Things I Hate About Pandas‚Äù, most of which were related to performance, some of which are still difficult to solve. Four years later, in 2017, McKinney initiated Apache Arrow as a co-founder. This is why Arrow‚Äôs integration has become the most noteworthy feature, as it is designed to work seamlessly with Pandas. Let‚Äôs take a look at the improvements that Arrow integration brings to Pandas.

## Missing values

Many pandas users must have experienced data type changing from integer to float implicitly. That's because pandas automatically converts the data type to float when missing values are introduced during calculation or include in original data:

```python
In [1]: pd.Series([1, 2, 3, None])
Out[1]:
0    1.0
1    2.0
2    3.0
3    NaN
dtype: float64
```

Missing values has always been a pain in the ass because there're different types for missing values. `np.nan` is for floating-point numbers. `None` and `np.nan` are for object types, and `pd.NaT` is for date-related types.In Pandas 1.0, `pd.NA` was introduced to to avoid type conversion, but it needs to be specified manually by the user. Pandas has always wanted to improve in this part but has struggled to do so.

The introduction of Arrow can solve this problem perfectly:
```
In [1]: df2 = pd.DataFrame({'a':[1,2,3, None]}, dtype='int64[pyarrow]')

In [2]: df2.dtypes
Out[2]:
a    int64[pyarrow]
dtype: object

In [3]: df2
Out[3]:
      a
0     1
1     2
2     3
3  <NA>
```

## String type
Another thing that Pandas has often been criticized for is its ineffective management of strings.

As mentioned above, pandas uses Numpy to represent data internally. However, Numpy was not designed for string processing and is primarily used for numerical calculations. Therefore, a column of string data in Pandas is actually a set of PyObject pointers, with the actual data scattered throughout the heap. This undoubtedly increases memory consumption and makes it unpredictable. This problem has become more severe as the amount of data increases.

Pandas attempted to address this issue in version 1.0 by supporting the experimental `StringDtype` extension, which uses Arrow string as its extension type. Arrow, as a columnar storage format, stores data continuously in memory. When reading a string column, there is no need to get data through pointers, which can avoid various cache misses. This improvement can bring significant enhancements to memory usage and calculation.

```python
In [1]: import pandas as pd

In [2]: pd.__version__
Out[2]: '2.0.0'

In [3]: df = pd.read_csv('pd_test.csv')

In [4]: df.dtypes
Out[4]:
name       object
address    object
number      int64
dtype: object

In [5]: df.memory_usage(deep=True).sum()
Out[5]: 17898876

In [6]: df_arrow = pd.read_csv('pd_test.csv', dtype_backend=""pyarrow"", engine=""pyarrow"")

In [7]: df_arrow.dtypes
Out[7]:
name       string[pyarrow]
address    string[pyarrow]
number      int64[pyarrow]
dtype: object

In [8]: df_arrow.memory_usage(deep=True).sum()
Out[8]: 7298876
```

As we can see, without arrow dtype, a relatively small DataFrame takes about 17MB of memory. However, after specifying arrow dtype, the memory usage reduced to less than 7MB. This advantage becomes even more significant for larg datasets. In addition to memory, let‚Äôs also take a look at the computational performance:

```python
In [9]: %time df.name.str.startswith('Mark').sum()
CPU times: user 21.1 ms, sys: 1.1 ms, total: 22.2 ms
Wall time: 21.3 ms
Out[9]: 687

In [10]: %time df_arrow.name.str.startswith('Mark').sum()
CPU times: user 2.56 ms, sys: 1.13 ms, total: 3.68 ms
Wall time: 2.5 ms
Out[10]: 687
```

**It is about 10x faster with arrow backend**! Although there are still a bunch of operators not implemented for arrow backend, the performance improvement is still really exciting.

# Copy-on-Write
Copy-on-Write (CoW) is an optimization technique commonly used in computer science. Essentially, when multiple callers request the same resource simultaneously, CoW avoids making a separate copy for each caller. Instead, each caller holds a pointer to the resource until one of them modifies it.

So, what does CoW have to do with Pandas? In fact, the introduction of this mechanism is not only about improving performance, but also about usability. Pandas functions return two types of data: a copy or a view. A copy is a new DataFrame with its own memory, and is not shared with the original DataFrame. A view, on the other hand, shares the same data with the original DataFrame, and changes to the view will also affect the original. Generally, indexing operations return views, but there are exceptions. Even if you consider yourself a Pandas expert, it‚Äôs still possible to write incorrect code here, which is why manually calling copy has become a safer choice.

```python
In [1]: df = pd.DataFrame({""foo"": [1, 2, 3], ""bar"": [4, 5, 6]})

In [2]: subset = df[""foo""]

In [3]: subset.iloc[0] = 100

In [4]: df
Out[4]:
   foo  bar
0  100    4
1    2    5
2    3    6
```

In the above code, subset returns a view, and when you set a new value for subset, the original value of df changes as well. If you‚Äôre not aware of this, all calculations involving df could be wrong. To avoid problem caused by view, pandas has several functions that force copying data internally during computation, such as `set_index`, `reset_index`, `add_prefix`. However, this can lead to performance issues. Let‚Äôs take a look at how CoW can help:

```python
In [5]: pd.options.mode.copy_on_write = True

In [6]: df = pd.DataFrame({""foo"": [1, 2, 3], ""bar"": [4, 5, 6]})

In [7]: subset = df[""foo""]

In [7]: subset.iloc[0] = 100

In [8]: df
Out[8]:
   foo  bar
0    1    4
1    2    5
2    3    6
```

With CoW enabled, rewriting subset data triggers a copy, and modifying the data only affects subset itself, leaving the df unchanged. This is more intuitive, and avoid the overhead of copying. In short, users can safely use indexing operations without worrying about affecting the original data. This feature systematically solves the somewhat confusing indexing operations and provides significant performance improvements for many operators.

# One more thing
When we take a closer look at Wes McKinney‚Äôs talk, ‚Äú10 Things I Hate About Pandas‚Äù, we‚Äôll find that there were actually 11 things, and the last one was **No multicore/distributed algos**.

The Pandas community focuses on improving single-machine performance for now. From what we‚Äôve seen so far, Pandas is entirely trustworthy. The integration of Arrow makes it so that competitors like Polars will no longer have an advantage.

On the other hand, people are also working on distributed dataframe libs. [Xorbits Pandas](https://xorbits.io/), for example, has rewritten most of the Pandas functions with parallel manner. This allows Pandas to utilize multiple cores, machines, and even GPUs to accelerate DataFrame operations. With this capability, even data on the scale of 1 terabyte can be easily handled. Please check out the [benchmarks results](https://xorbits.io/benchmark) for more information.

Pandas 2.0 has given us great confidence. As a framework that introduced Arrow as a storage format early on, Xorbits can better cooperate with Pandas 2.0, and we will work together to build a better DataFrame ecosystem. In the next step, we will try to use Pandas with arrow backend to speed up Xorbits Pandas!

Finally, please follow us on [Twitter](https://twitter.com/Xorbitsio) and [Slack](https://join.slack.com/t/xorbitsio/shared_invite/zt-1o3z9ucdh-RbfhbPVpx7prOVdM1CAuxg) to connect with the community!","Isn't Polars still faster than Pandas + Arrow? I thought I remember a Reddit post within the last couple of weeks that benchmarked a handful of different approaches, and Polars consistently came out on top. | Sorry if this is a dumb question, but with respect to ML libraries, will the OG‚Äôs like XGBoost, Torch, sci-kit, etc be able to ingest data that‚Äôs PyArrow formatted? I hate having to do conversion back to vanilla Pandas, numpy, etc at the very end. Becomes a bottleneck. | Many scenario Polars can be proved that it can process billions of rows. But I cannot enjoy good experience after using Pandas 2.0.1 pyarrow.parquet. So I post this issues to Pandas [https://github.com/pandas-dev/pandas/issues/53249](https://github.com/pandas-dev/pandas/issues/53249)

My belief is the use of bytearray is much better than arrow to keep table in memory. It save space and computing time. Byte-to-byte conversion I do a lot of billion-row experiments for Distinct, GroupBy, JoinTable and Filter, it proven work and achieve outstanding processing speed. So it is strongly recommend Pandas and Polars to consider this in-memory implementation model.

Implement bytearray, cell/column becomes independent of data type (integer, float, text, date...). Only when doing computation e.g. GroupBy aggregate it will treat bytearray as real number automatically, if cell contain abc, obviously abc must be equal to zero in this case. if cell contain (123.45), it is -123.45.

For data filtering, user can operate it by text or floating. Account number has different length, it shall be operated by text type. By default, all column are text type for data filtering except user specify which column of number is float for particular filter processing. They can change it data type (without changing the in-memory table) to adapt different filters.

One week later you can do above experiment using the trialapp.

I will recommend Apache Foundation, to unify in-memory storage format for exchange using ByteArray instead Arrow when users found the trialapp is much faster than Pandas. | I think it would be in the future. Since Arrow backend is definitly more suitable for pandas, compared to NumPy backend.

But this feature is quite young and may have some **compatible issues** or **performance regression** under certain situations. It may not be able to act as the default backend right now.

If you want to enable arrow backend in your workflow, my suggestion is to benchmark 2.0 version with your own workload before making any changes to the prod env. | Polars author here.

Polars is a multi-threaded, vectorized, out-of-core query engine. It does query optimizations and is written for performance and especially multithreading from the ground up. We can control performance tightly in our code base, not in a third party lib.

Pandas is eager and therefore makes huge materalizations that are unneeded. I would take phrases as a revolution in performance with a grain of salt.

Here are TPC-H benchmarks showing polars against pandas backed with arrow dtypes: https://github.com/pola-rs/tpch/pull/36

Even when all operations dispatch to arrow, there is no reason to assume it will generally have better performance. It would still make materializations, not have parallelism in its engine design, needs all data in memory, and have an API that invites you to do expensive things.

And last, different implementations, have different performance characteristics. Adhering to the same memory format doesn't change that. | (Copied from my hacker news comment)

If you were to say ‚Äúpandas in long format only‚Äù then yes that would be correct, but the power of pandas comes in its ability to work in a long relational or wide ndarray style. Pandas was originally written to replace excel in financial/econometric modeling, not as a replacement for sql. Models written solely in the long relational style are near unmaintainable for constantly evolving models with hundreds of data sources and thousands of interactions being developed and tuned by teams of analysts and engineers. For example, this is how some basic operations would look.
Bump prices in March 2023 up 10%:

    # pandas
    prices_df.loc['2023-03'] *= 1.1

    # polars
    polars_df.with_column(
        pl.when(pl.col('timestamp').is_between(
            datetime('2023-03-01'),
            datetime('2023-03-31'),
            include_bounds=True
        )).then(pl.col('val') * 1.1)
        .otherwise(pl.col('val'))
        .alias('val')
    )
Add expected temperature offsets to base temperature forecast at the state county level:

    # pandas
    temp_df + offset_df

    # polars
    (
        temp_df
        .join(offset_df, on=['state', 'county', 'timestamp'], suffix='_r')
        .with_column(
           ( pl.col('val') + pl.col('val_r')).alias('val')
        )
        .select(['state', 'county', 'timestamp', 'val'])
    )
Now imagine thousands of such operations, and you can see the necessity of pandas in models like this. | There are still a bunch of operators not implemented for arrow dtype backend in pandas 2.0.0. I believe the performance is going to be improved gradually in following versions. | I also benchmarked RC0 on TPC-H: [https://www.reddit.com/r/Python/comments/11ts7rv/pandas\_20\_rc1\_has\_been\_published\_have\_you\_tried/](https://www.reddit.com/r/Python/comments/11ts7rv/pandas_20_rc1_has_been_published_have_you_tried/)

RC0 performed even worse than pandas 1.5.3 due to a corner case. I reported the problem and the community fixed it.

I think the 2.0.0 should perform better now, and the following versions are gonna keep making progress.

I agree that different implementations have different performance characteristics. That's why we build \[Xorbits\]([https://github.com/xprobe-inc/xorbits](https://github.com/xprobe-inc/xorbits)) to handle TB-level data processing. | If you dont mind me asking - I tried out Polars as it seems like a great fit for me. However it seems like it requires quite clean data - at least columns with uniform datatypes. What is the recommended workflow for working with messy data? Opening it in pandas to familiarize myself and clean the data seems to defeat the purpose. | I'm a dummy when it comes to python and tried polars... I got stuck with creating connections for all my servers for polars: Postgres, Oracle, Snowflake, and SQL Server.

I think my python skills are too rudimentary to grasp how my engine connections work for pandas but not polars, what the URL connection method really is.

I'm not even sure where to learn. I feel like the introductions to libraries are at a level I'm not yet to. | I updated the benchmark to include pandas 2.0 and latest polars as well. | Yeah true story, but who are even working with perfect data? Maybe I'm just a bit ""biased"" from my type of work. The data I tried using with polars was some government data in CSV's, and lo and behold someone had snuck a string into an float column or something like that. In that case it feels like Polars isn't anything for me, or at least until I have progressed so far that I do some automatic processing of clean data? Given how I use pandas (to explore and clean data) it feels like Polars is not an alternative to Pandas but something else."
oz5erv,I created an Excel Add-in to generate Pandas Dataframes right inside Excel,"I am working as a Data Analyst. In many cases, the Excel Files I am dealing with are pretty 'messy'. Often the Excel files are containing headers, comments, additional (unnecessary or blank) columns.

If I want to perform analysis using the pandas library, first I need to transform the Excel file into a pandas DataFrame using 'pandas.read\_excel(""ExcelFile.xlsx"")'. Pandas offers different parameters to read in 'messy' Excel files, such as *usecols*, *skiprows*, *nrows*, etc.

Yet, I found it tedious always to specify those arguments if I just want to perform a quick analysis. That is why I have created an Excel Add-In, which does all the tiresome work. As shown in the gif below, after I select the data I want to transform into a pandas dataframe, the add-in will create a python file in the workbook's directory. The VBA code will translate the cell range into the necessary pandas arguments:

* io *\[File Name\]*
* sheet\_name
* skiprows *\[Number of lines to skip (int) at the start of the file\]*
* usecols *\[Excel column letters and column ranges (e.g. ‚ÄúA:E‚Äù)\]*
* nrows *\[Number of rows to parse\]*

[Demo of 'Create Pandas Dataframe' Button](https://i.redd.it/1aclg7d4aqf71.gif)

Perhaps this add-in might be also helpful to you. I also added some other neat features into the add-in to expand excel capabilities. With the add-in, you can add images to Excel comments, transform text to checkboxes, easily create Drop Down lists with one click, remove empty & blank spaces from cells, and much more.

Here is the link to the tutorial:

*(The Python-specific part starts at 8:40 min)*

[https://youtu.be/PmJ9rkKGqrI](https://youtu.be/PmJ9rkKGqrI)

You can download the add-in for free here.

[https://pythonandvba.com/mytoolbelt](https://pythonandvba.com/mytoolbelt)

It would be great if you could share your feedback with me. Any suggestions regarding additional features or improvements? Please let me know :) **Enjoy!**","This is cool, I am wanting to make an excel add in but don't know where to start.

One thing that I found it is pandas has a read_clipboard function, so you can also skip a lot of steps that way | Thank you! I deal daily with converting to csv‚Äôs for easy tasks (why isn‚Äôt there a skip row function in excel, I need every 2nd row yet need to punch it through pandas to do it for me). Will be sure to look into this. | Glad it is helpful! Do you mean transforming the df back to Excel? You might wanna have a look at the 'xlwings' library:

```
import xlwings as xw
import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])
xw.view(df)
``` | Obviously there are programmatic ways to get data out of Excel (read: Pandas) but VBA is still the best way to work with data inside a workbook. 

You can either use VBA to clean the data and insert into a DB or use Pandas to read the file using various arguments to clean the data.

What OP has created is a way to auto-generate the arguments necessary for Pandas to clean the data when it is read.

Edited to add: Creating a DB to stage the data seems like over-engineering and may not even be possible if the developer does not have permission to create a DB which is common in IT."
18b5bsg,Will Pandas have streaming in Future??,"As Pandas has switched to arrow backend from version 2.0, is there a possibility that we can see Lazy Evaluation or streaming in Pandas so that we'll be able to process datasets larger than memory on a machine as we have in Polars??","Polars' API is much more streaming-friendly than pandas'. And you can easily dip back into pandas when necessary because they can just pass an arrow memory buffer between them. | I think the main issue with that would be that pandas has an eager execution model and several parts of the api (I'm thinking of stuff like setting values by index slicing) wouldn't make sense without it.

Dask is a project that's lazily executed distributed pandas, so I think streaming would probably be more likely to show up there first (I haven't heard any plans of including it though)

That said, never underestimate the sheer size and backing that pandas has. There's a mammoth community behind it, so I guess anything is possible.

EDIT: oh woah! Dask actually prototyped this a few years back so looks like it's 100% possible: https://blog.dask.org/2017/10/16/streaming-dataframes-1 | Most of the pandas API isn‚Äôt compatible with streaming or lazy evaluation so it would take some major reworking. | Awkward does lazy evaluation and there is an awkward-pandas library: https://awkward-pandas.readthedocs.io/en/latest/

So in theory you can have lazy evaluation by doing an extra step. Or am I misunderstanding the library? | Lazy is likely to land in pandas in the future, streaming is unlikely for now, that‚Äôs not a focus at the moment | Is the arrow backend default now in pandas 2.0? | The perspective python backend might work for some streaming use cases https://github.com/finos/perspective it can output arrow so you've nice compatibility with polars/pandas these days | lol same, i'd love to see some pandas | Polars has the additional benefit of a saner,more consistent API as compared to the Pseudo -DSL offered by pandas"
18676xd,"What's up Python? New args syntax, subinterpreters FastAPI and cuda pandas‚Ä¶",,"My apologies it was my misunderstanding of how the sub-interpreters worked and after reading the proposal in detail I have a better idea of what is being proposed, in my excitement I jumped the gun! If these sub-interpreters have their own sys.modules I suppose and I could define my own custom import loader for each then that could work in my case. The use case is imagine I have different versions of a pandas based report that gets sent out, I want to test different versions of the report and potentially different versions of pandas or other libraries to ensure that the values are the same, e.g if we are calculating risk or P/L we want to ensure that the numbers being output are the same and developers haven‚Äôt inadvertently broken something. Currently this is being done with venvs and then the results are compared"
1dqm1y4,Atollas - a column level type system for pandas,"Hey folks!

I do a lot of stuff professionally with pandas and dask, and I always *reeeeaaaly* wish that they had a column level type system. I feel like a lot of bugs like, one-to-one joins on non unique columns, or just plain old incorrect source data would be quicker to find if there was one.

So I've written one - or at least started to. It's pretty early stage, but I'm pretty excited about it as an idea. Would love some feedback people (especially ones that work with pandas a lot)!

[So here's my little project, hope it's interesting to someone!](https://github.com/benrutter/atollas)


## What my Project Does

Provides a column level type system for pandas, to catch bugs earlier and check for things like, join operation validity.

## Target Audience

People looking to put pandas code into production with better reliability that pandas provides out the box.

## Comparison

I don't know of any 1-2-1 comparison, things like polars have good type checking internally, but don't make any attempt to type check columns. Pandera is the closest project, which gives a decorator to enforce schemas at type boundaries)
","This point gets mentioned in the readme- tries to do something a little differently, i.e. type data frames and then validate operations on that basis. Whilst pandera gives you functions to check schemas are valid at key points.


That said- this is nowhere near ready for production or anything like that, so if you're looking to *actually type* pandas dataframes in production, pandera is gonna be a better choice across the board. | I love polars! But I'm pretty sure it (like pandas) doesn't have a column level type system. You can choose to give a schema at read, but it isn't enforced later on in your code, and doesn't inclide expression of things like uniqueness or nullability? | Really cool.  I feel like there is a pretty deep well to try to build in the whole pandas API, like how a `.stack` or `.concat` might ruin your unique or how a `.drop_duplicates` might establish a unique where there wasn't one before, though I don't see why you'd need to support all of the pandas methods.  The user may just need to explicitly re-establish types after operations that aren't considered in your api.

In my work, a lot of my joins are done on combinations of fields, so being able to declare a combinations of fields as unique could be handy for some. | Thanks, although my thinking is that those aren't too bad when they're the *only* thing being tracked. Concat isn't supported yet, but you can use drop_duplicates on a single column and it's smart enough to know that the column is now unique.


The targetted API surface area is just pandas method chain operations for which there's only 20 or so. I guess the tricky bit is whether that's a bet that pays off or if you end up needing to use pipe all the time whilst manually specifying schemas."
187l96b,Using Polars in a Pandas world,,"Polars is awesome. Pandas can now run on Cudas, but since I'm mainly using a laptop without Nvidia GPU, for me Polars has proved to be much faster. And not having index has turned out to simplify things quite a bit. | I‚Äôm interested in Polars. A lot of the work I‚Äôm doing with Pandas involves data exported in Excel. I can tell you that I haven‚Äôt found a way to get Polars to handle Excel well.

I‚Äôll likely try it the next time I get data in csv which I prefer anyhow even in üêº | I've switched to polars about a year ago or something, and now I use Polars almost exclusively, unless I really need to use Pandas for some reason (I work mainly with csv/parquet files). 

I find Polars' expressions so much more convenient to use than Pandas' syntax, and I can build far more complex pipelines with it. Lazy operations are also a godsend when I'm working with millions of rows at the same time. 

I usually do all my data prep in Polars and then convert the df to Pandas for plotting. I only ever need Pandas for formatting and for preparing pivot tables (multi-level indexes are way better in Pandas than Polars). | > not having index has turned out to simplify things quite a bit

Depends on what you‚Äôre doing. Indexes have a lot of value that polars ignores (they have admittedly  become less vocal about this point though).

Bump prices in March 2023 up 10%:

    # pandas
    prices_df.loc['2023-03'] *= 1.1

    # polars
    polars_df.with_column(
        pl.when(pl.col('timestamp').is_between(
            datetime('2023-03-01'),
            datetime('2023-03-31'),
            include_bounds=True
        )).then(pl.col('val') * 1.1)
        .otherwise(pl.col('val'))
        .alias('val')
    )

Add expected temperature offsets to base temperature forecast at the state county level:

    # pandas
    temp_df + offset_df

    # polars
    (
        temp_df
        .join(offset_df, on=['state', 'county', 'timestamp'], suffix='_r')
        .with_column(
           ( pl.col('val') + pl.col('val_r')).alias('val')
        )
        .select(['state', 'county', 'timestamp', 'val'])
    ) | You can reas it from excel using pandas and then switch to polars | You can tell that they planned out Polars at least a pit before they started making it, as opposed to Pandas where they just went for it | It's been around for a couple of years.

I think the recent press release from a few weeks ago was that it now requires ""zero code changes"".

https://developer.nvidia.com/blog/rapids-cudf-accelerates-pandas-nearly-150x-with-zero-code-changes/

https://rapids.ai/cudf-pandas/ | That was one of the suggestions I got from an LLM. I couldn‚Äôt make it work. And, I came to the conclusion it probably wasn‚Äôt worth it in my use case because I need it to open Excel and save to Excel. Having to switch back and forth probably wasn‚Äôt going to be worth the time savings in my use case. Pandas works great with Excel out of the box. 

I‚Äôm still looking forward to using Polars where I‚Äôm working exclusively with CSV | Oh mind messaging it to me? I‚Äôm primarily using pandas for workflows but polars looks intriguing on many levels for future projects. I‚Äôd love to see what other people are doing with it | The full pandas support was only released this past November in cudf v23.10. Cudf in general has been around for a while though. | I'm still using pandas most of the time because I'm more used to its api and the performance is acceptable for my use case so cannot argue with this! | >The full pandas support was only released this past November in cudf v23.10.

  
Just as a note. Full pandas support doesn't mean your whole pandas script runs on the GPU.  
It supports the full pandas 1.5 API, but it hasn't implemented the full API. Routines that are not implemented fall back to running pandas itself."
12n0wfk,I discovered that the fastest way to create a Pandas DataFrame from a CSV file is to actually use Polars,,"Interesting study. My data needs are trivial and I'm still learning. I've worked with Pandas enough to have stepped on several of the rakes, so Polars sounds intriguing more from the API standpoint than the need for speed. Either way though, I'm still working out how to break it into intelligible functions rather than chaining as far as the eye can see. | Are we due a backlash against all these ‚Äújust use Polars‚Äù articles at any point? The pure speed of Polars vs pandas goes against the grain of the argument for Python in the first place: developer time costs more than compute, which is why we‚Äôre not working in C++ to begin with.

I‚Äôm not denying that Polars looks great, and its API is better thought out than pandas, but in industry, *boring always wins*. People evangelising for Polars in this sub are a) getting pretty irritating, and b) ignoring the realities of adding new dependencies and learning how to use them in a commercial software development. | These are click bait articles who really cares.

Performance for this does not matter. 
If you were truly needing to optimize performance don‚Äôt use python and pandas at all.

This falls into two categories:

1. I am loading it into a data frame to explore in a notebook.
2. I have completed my work and this is being done as part of a pipeline.

For number 1. Performance doesn‚Äôt matter at all it will either be an amount of time that while annoying isn‚Äôt really impactful or if it‚Äôs actually really long take that as a sign you are using the wrong tool for the job. Worst case scenario you work on something else and let it take time.

2. If this is part of an enterprise process, I‚Äôve already added appropriate dtypes so the performance difference is most gone away anyways. If the velocity of the data is so large it‚Äôs not fast enough in production you likely are too thin of margins anyways and using the wrong tools. If you have to worry about processing this data in prod on a Chromebook it‚Äôs a waste of your time find another job.

If you are actually using this to solve real problems a company has tons of competing priorities for your time and changing this speed is likely not the most impactful thing you are doing. | The new pandas has polars in its backend: https://youtu.be/cSLPyRI_ZD8 | I find it's funny that people use reading large csv as a benchmark, but none of them actually use a large csv in daily.

When working with actual large data, pandas is out of the consideration. | The computation speed literally matters. I‚Äôve seen numerous transformations which took pandas an hour to compute versus 20 seconds with polars. I‚Äôm not exaggerating. This is a daily routine for any data scientist working with large enough amounts of data. It‚Äôs literally impossible to use pandas after a certain amount of rows. Both due to its slowness and memory inefficiency. Polars now has larger-than-RAM capabilities similar to Spark too‚Ä¶ 

As for your other argument about the adoption complexity, I partly agree with it. However in my experience everyone was really happy to switch to polars once they saw how good it was, but this probably depends on the company (I‚Äôve introduced polars for 3 different projects). 

Polars has no dependencies (but a few optional dependencies like pyarrow which is probably already used), so that‚Äôs not a problem too, it won‚Äôt conflict with anything. It‚Äôs very easy to just start using it for newer workflows. | > The pure speed of Polars vs pandas goes against the grain of the argument for Python in the first place: developer time costs more than compute, which is why we‚Äôre not working in C++ to begin with.

What do you mean by this? As the article shows you'd still be using Python, it's a different library not a different language from the perspective of the library user. Both Polars and Pandas are written in other languages but have Python bindings | The things is there are cases when speed and optimization matters. No point in fast development speed when you wait 10 minutes for every notebook cell.

People use pandas and complained in those cases, library like polars solve it. | It's the same pattern over and over with any ""X-killer"" tech. Enthusiastic users, usually pretty early in their coding career, see a benefit in the new tech that is really great and think that's the sum total of considerations at play. Equally, it's important to have those voices to push forward standards as was mentioned earlier in the thread.

I actually work with very large data for which Polars seems like a great option and I looked into using it and found that, with some care, pandas was comfortably usable and had all the benefits of the more established ecosystem.

In my day job I have started asking my reports to go out and examine the documentation, repo activity, ecosystem etc for any new tools we might be considering. If I'm going to be putting aside a chunk of development time in a team with tight resources, then you can be sure my first questions are going to be about possible tech debt and future development, not ""can I get an extra few % of performance by completely changing my ecosystem""?

One of my main learning goals with juniors in my team is understanding the importance of these priorities for working professionally. I should mention that there are tools which we're now adopting despite a potential for biting us later after this review process. These usually have features like being backed by a well known player, being well confined to one area of the codebase so that a rip and replace won't be too awful, or clear signalling from the devs that they are aiming to integrate into the wider ecosystem. | In general yes, but there are always exceptions where performance improvements like this can be important to a valid process.

An external vendor sends us large (e.g. 1 GB) daily CSV files and has been doing so since 2011, each day the first thing we do is upload them to a database so that other consumers can easily query them. Performance is for each daily file is not critical, e.g. if the process takes 1 minute instead of 10 seconds that's not great but we'll manage.

However, let's say we find a subtle bug in our CSV to database process, we now want to apply this bug fix across the entire history of CSV files and check were there any changes. The performance improvement that in absolute terms was small but because it was big in relative terms now means checking this is a difference of days or hours.

FYI one of things I've done since joining the team is largely remove the need for `pandas.read_csv` in this kind of process, but I have not managed to get to all processes yet. | Obviously, you can do this, the author even has a previous article testing the performance of different ‚Äúengines‚Äù usable by Pandas for reading CSVs, but it looks like Polars is still faster b/c it uses the native Apache Arrow lib under the hood. | Not quite. It can optionally use the memory format which polars (among many other packages) is based on, which makes it easy to share data between pandas and polars; pandas does not use polars internally. | What sort of transformations / operations would be a good example of this?

I‚Äôve been looking to optimize some code but I think it‚Äôs pandas is the limiting factor, as there‚Äôs specific operations that take a long time but with I don‚Äôt think can be made more efficient in pandas | My point is that if you have a code base with a significant amount of pandas code in it, and a team with significant experience with pandas, the cost of learning how to do the things you‚Äôve been doing in pandas in Polars is significant.

~~Besides that, Polars‚Äô API doesn‚Äôt cover every pandas use case, so you could find yourself spending X time trying to get something working in Polars only to discover it‚Äôs not possible (or massively more obtuse).~~ | Agreed, and I‚Äôm not saying that those aren‚Äôt important, nor that Polars isn‚Äôt a great library (I think it‚Äôs awesome!).

My complaint is that on this sub there has been a huge influx of comments on pandas posts pushing Polars and ignoring the (human) costs of adding a big new dependency and learning how to use it effectively in a software engineering environment. | If I'm not wrong, pandas 2.0 will also start using Apache Arrow. So,maybe there will be no need to switch to Polars. | I am not saying this generally but for this specific use case of processing csv files in a data pipeline with pandas or exploratory data analysis and development of the pipeline code that execution speed just really isn‚Äôt a metric or success.

I can easily imagine some junior reading this article, changing the code and now I‚Äôve got another dependency for no benefit and then subsequent waste of time for the new dependency even with tools like docker and poetry it just adds extra work for something that doesn‚Äôt move the needle on success in this case.  An optimization that never needs to be made because as others have pointed out there‚Äôs much better solves to a performance problem of loading data | It has been grinding my gears for a while too. I‚Äôm noticing two kinds of people are evangelizing polars here. 1. Experienced people who understand the performance advantages and could benefit from it in some way, and 2. Beginners who didn‚Äôt manage to get to grips with rudimentary pandas syntax yet, and are enthusiastic about polars purely because they became fed up trying to learn pandas. 

The first group of people I have no issues with, but the second group are adding a lot of noise to the discussion and probably won‚Äôt benefit from the performance boost anyway. | I went through this recently on my one-man somewhat large 10k+ lines project. It's very difficult at first to adjust your thought process as the Polars API is radically different (what's an index?). But once you get going, it's far easier than expected and Polars code is significantly cleaner. There are still areas where Pandas is more mature, resampling and time data manipulation comes to mind - groupby\_dynamic isn't as intuitive as pd.resample. However, I doubt that will be the case in a year. | DuckDB did a rerun of the h20ai db-benchmark. Here are the results. Both pandas 2.0 and the arrow backend are much slower than polars.

https://tmonster.github.io/h2oai-db-benchmark/ | If polars has an easier to use syntax, isn't that still good for the group 2 folks you mention? 

I've written and published software that depends on pandas and dealing with pandas felt like pulling teeth. 

I'm excited to try polars on my next project. | Haha, fair point; I‚Äôve wiped out that paragraph.

For the avoidance of doubt (since you‚Äôre here): Polars is a great achievement; I just get annoyed with relatively inexperienced commenters on this sub who pile on every pandas post advocating for it. | So after this discussion I had a brief look yesterday with some of our code, and at first glance it would be incredibly difficult. We tend to use multiindexes in both the index and columns, and do a lot of stacking and unstacking. None of these is supported: Polars doesn‚Äôt have an index at all, and doesn‚Äôt allow for multiple column levels. It‚Äôs got pivot and melt, but using those would be a lot more verbose. Oh, and rename doesn‚Äôt accept a callable, so you‚Äôd have to construct a dictionary first and pass that.

All this is understandable‚Äîspeed and simplicity are its principles, and there‚Äôs a lot of irreducible complexity in the stuff pandas does‚Äîbut it does wipe out a lot of use cases. | Disclosure: I am the author of polars. 

Polars adheres to the arrow memory spec. It uses arrow2, which is a Rust native implementation of the arrow spec. I am also one of the maintainers of that crate. From that crate polars uses the parquet, parquet and json readers.

The csv parser is written by me in polars itself. As well as most algorithms and compute. Those are not from any arrow library, but are written in the polars crates themselves.  


That's why claims as pandas 2 is just as fast as polars because both use arrow, don't make much sense. Yes, they both adhere to the arrow memory format, but the compute and IO are from completely different engines.  


This difference also shows in the latest h20AI db-benchmark run (ran by duckDB):  [https://tmonster.github.io/h2oai-db-benchmark/](https://tmonster.github.io/h2oai-db-benchmark/)  


The reason I want to correct these misconceptions is that they dismiss all the hard work we have done in polars. | Well, unfortunately, i'm not that aware about how the things work under the hood in pandas or polars (or any other package).
However, I've read some articles from pandas developers (or maintainers or how may I call those persons) that were claiming a significant boost in performance after Arrow implementation. So, I've decided to share this information here. | We do‚Ä¶ with pandas | That's why I share benchmarks. Claims should be backed by data.

For context, I am the author of polars, so I am aware how things work under the hood of polars.

As you can see both current pandas 2.0 and the arrow compute backend are included in the benchmark. 

Both are miles behind polars. 

A significant boost doesn't mean faster. Similarly if we say polars has a significant boost in the performance of operation X, we wouldn't claim to be faster than hyper. Because benchmarks show, we are not."
12m2gn8,How do you guys handle pandas and its sh*tty data type inference,"I often like to dump CSVs with 100s of columns and millions of rows into python pandas. and I find it very very frustrating when it gets various data types for columns wrong. nothing helps

including `infer_objects().dtypes` and `convert_dtypes().dtypes`

how do you guys auto-detect dtypes for your columns?

is there a better library out there ?","I feel like y'all need to learn how to read docs, you can (and should) specify your schema beforehand, which you can do by setting `dtype` param on `read_csv` to a dictionary in the form of `""column_name"": pandas_type`. [Docs](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) | Highly recommend parquet files   


[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read\_parquet.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html)

[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to\_parquet.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html) | I just read an article that discussed this, but can‚Äôt remember the site or who wrote it‚Ä¶ he did a bunch of comparisons and iirc some general takeaways were:

-	Polars is much faster than Pandas for reading CSV data into a data frame 
-	if you know ahead of time you don‚Äôt need all the columns and only read the ones you need, that‚Äôs also much faster. 
-	setting types is faster after you have a data frame, not during the read.

Edit: [found it!](https://itnext.io/the-fastest-way-to-read-a-csv-file-in-pandas-2-0-532c1f978201) | Don‚Äôt use pandas for dataframes this large. Also don‚Äôt use csv. Use polars or pyspark and parquet files. | pandas 2 is a step in the right direction for consistent type handling, but the api still sucks .. | A better library than pandas? üò≠ | No man, it's never the users fault! It's gotta be pandas fault because excel is already awesome!

/s

ü§£üòÇ | I use pandas on investment data at work coming from all sources...IDC, Bloomberg, etc. All data types and pandas works wonderfully. 

Pandas does the hard part all the end user needs to do is specify! | I just recently discovered that it's actually best to use read_csv() with dtype and dtype_backend as defaults and then do .astype() with you desired dtypes afterwards, for best performance.
Have a read here: https://itnext.io/the-fastest-way-to-read-a-csv-file-in-pandas-2-0-532c1f978201 | >which is taking a bunch of strings and figuring out what it's supposed to be with no other context

Except when Excel gets into its tiny silicon brain that something is an american date value. Then it cannot be convinced otherwise. At least with pandas we can compel it. | Say each experiment is 1000 data points, you will have 3 columns, 1 is experiment number, 2 is read datetime and 3 is read value. Instead of 6000 data files instead you should have 1 data file with 6 million rows and 3 columns.

This won't work well for excel, but it should be fine with pandas and parquet | I don‚Äôt see how that‚Äôs more advantageous than storing each experiments data in its own column. Having its own column makes manual verification of the data easier (a bit of a moot point since I‚Äôm automating it). Would structuring the data in your proposed way be better suited or optimized for pandas?

At the end of the day I‚Äôm a chemist who has started working with python to automate things I would have to do manually, so I still have a lot to learn about data structures and python in general."
1bt4w6z,How to parallelize Pandas with Modin,"Discover the power of Modin in overcoming the performance bottlenecks of Pandas. We'll walk you through how a simple switch can reduce your waiting times and make your workflow smooth and speedy, all without leaving the comfort of Pandas behind.

Modin is an active  open source project. Take a look to this blog post to see if it is applicable for your code: [https://dchigarev.github.io/modin_perf_examples/](https://dchigarev.github.io/modin_perf_examples/)","Modin is designed to be a drop-in replacement for Pandas, so the existing Pandas code can be executed faster. Pandas has 40k stars in GitHub, Ibis has 4k stars in GitHub, so objectively Pandas API is 10 times more popular than Ibis API. | My guess is this probably under estimates the dominance of Pandas."
1cedln6,I made an easy and secure data lake for Pandas,"**What My Project Does** Shoots is essentially a ""data lake"" where you can easily store pandas dataframes, and retrieve them later or from different locations or in different tools. Shoots has a client and a server. After choosing a place to run the server, you can easily use the client to ""put"" and ""get"" dataframes. Shoots supports SQL, allowing you to put very large dataframes, and then use a query to only get a subset. Shoots also allows you to resample on the server.


```python
# put a dataframe, uploads it to the server  
df = pd.read\_csv('sensor\_data.csv')  
shoots.put(""sensor\_data"", dataframe=df, mode=PutMode.REPLACE)  


# retrieve the whole data frame  
df0 = shoots.get(""sensor\_data"")  
print(df0)  

# or use sql to retrieve just some of the data  
sql = 'select ""Sensor\_1"" from sensor\_data where ""Sensor\_2"" < .2'  
df1 = shoots.get(""sensor\_data"", sql=sql)
```

**Target Audience** Shoots is designed to be used in production by data scientists and other python devs using pandas. The server is configurable to run in various settings, including locally on a laptop if desired. It is useful for anyone who wants to share dataframes, or store dataframes so they can be easily accessed from different sources.

**Comparison** To my knowledge, Shoots is the only data lake with a client that is 100% pandas native. The get() method returns pandas dataframes natively, so there is no cumbersome translations such as required from typical databases and data lakes. The server is build on top of Apache Arrow Flight, and is very efficient with storage because it uses Parquet as the storage format natively. While the Shoots client does all of the heavy listing, if desired, the server can be accessed with any Apache Flight client library, so other languages are supported by the server.

**Get Shoots**
There is full documentation available in the Github repo: https://github.com/rickspencer3/shoots

It is packaged for Pypi as well: (https://pypi.org/project/shoots/) ```pip install shoots""","Can you add support for Polars? Will never again use pandas | Looks like a good, well-focussed project. How is it at preserving column types? E.g. if I upload a dataframe of null values of type string will it be guaranteed to come out of the lake with the same type and not be re-interpreted by pandas as having some other type?

Other question would be - given that data lakes are about long term storage for multiple users - how well does it handle uploading and downloading pandas dataframes from different pandas versions? | What would be the benefit of your library over something that is really well supported like delta lake? 

https://delta-io.github.io/delta-rs/

I'm not really seeing any significant improvement in interfacing with pandas and this library also provides many other interfaces and better data security/safety guarantees. Also, it's very unlikely you will be able to compete with their performance. | I second this. Seriously after getting a taste of what Polars can do, it is my go-to and I have been decommissioning my pandas scripts in favor of what Polars offers. | You can think of shoots like a sort of pandas-native database. You can store, retrieve, and process large datasets on a server. Multiple users can access and process the data from different locations. You can have automatic processes creating and updating the data. The difference between shoots and a normal database is that the client let's you work with dataframes directly, you don't have to have to translate back and forth between pandas dataframes and whatever storage scheme the database uses. | I haven't focused on either of these issues.

For typing, they types get converted from Pandas to Arrow to Parquet for puts, and then back again for gets. There are likely some lossy conversions in that path, but I don't have any tests for it or anything.

For the Pandas versions, the conversions from to and from Pandas and Arrow occurs in the client code, and is handled by Arrow.

Addressing such issues sounds doable, but complex. I'll wait and see if any kind of user community forms and raises such issues."
1c5b5ky,Using Pandas 2 and different datetime erros,"Hey Folks,

I am working on a project that uses a bit old pandas version (1.5.3). I am trying to update it and use a more recent version (2.2.2). Its the first time I use pandas 2 btw. I simply ran all my unit tests and got multiple and different errors all concerning some datetime aspects. I tried troubleshooting by searching the errors and look in pandas release note but I find really complicated. It feels like the documentation is really exhaustive but I could really use a note on principal errors one can get switching to pandas 2.

Does anyone have any handy blogpost, article, documentation that specifies this ? My focus is mainly on datetime errors when differences computed or comparaisons

&#x200B;

Thanks","Maybe check out the release notes.¬† Pandas 2.2 became more strict with datetime values.¬† It wants higher resolution dates per their release [notes](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html#backwards-incompatible-api-changes). | What worked for me was replacing anywhere I had Python datetime with pandas.Timestamp https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html#pandas.Timestamp.

If you are inside of a function working on a single value use pandas.Timestamp instead of datetime. If you have a Series or DataFrame you can use https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to\_datetime.html.

If you do that, then the other operations on dates should work as expected. | I would try to post the errors. But honestly, if you are updating the project you might want to switch Pandas with Polars while you are at it. | I've heard about Polars but I thought pandas 2 was bringing similar advantages. Maybe I should check it ! | I have always used Pandas, but have now started to use Polars and refactoring to use Polars in older project that is still in use. There is a big performance gain, obviously most noticeable for larger data. While it was something else to wrap your head around the syntax in Polars, especially coming from Pandas, I find that once it clicked I like it."
1c34vk8,Pandas - value replacement or interpolation,"Hi all, here trying to replace values for a range of dates in a data frame copy I've made where DateTime is the index and the values I need to mass replace are in the 'Value' column.

The reason why is I need to interpolate between two dates in a time series, where the data I have is incorrect due to measurement error.  I need to replace it with interpolated data to make a more representative data set for forecasting

Can anyone advise? 
",
13yrcl9,"Python for Finance: Pandas Resample, Groupby, and Rolling",,"Are we allowed to mention pandas in this sub? Don't let the Polars bros hear you | A great tutorial, thanks! Here‚Äôs also a step-by-step breakdown of perform aggregation, transformation, or other operations with Gropuby: [Getting Started with Pandas Groupby](https://www.codium.ai/blog/efficient-data-analysis-with-pandas-groupby/) | Okey, can someone explain what is ploblem with pandas, seen a very good library. | Just realized ‚Äî was the name polars chosen because it‚Äôs another kind of bear? Admittedly I never think of ‚Äúpanda bear‚Äù when I say pandas (probably because I say it so often that it becomes its own sort of word) but if they chose ‚Äúpolars‚Äù as a foil to ‚Äúpandas‚Äù it would make sense. Should we expect grizzlies in 2030? | Fwiw, when I need to, I use pyarrow to read csvs then to_pandas().

Although I use duckdb for most of this stuff anyway | Yeah, this is a popular thing to do. There's also a library called ""koalas"" that tries to help bridge between Pandas and Spark and another library called ""kangas"" that tries to expand visualizations with pandas. | Have you benchmarked that process? I'd be very surprised if reading a csv in polars (lazy or eager) and then converting the whole thing into an in memory pandas dataframe is in any way faster than just reading straight into memory with pandas. Especially when compared to Pandas 2.0 native pyarrow support.

Also, it largely defeats the benefits of polars to convert it into an in-memory dataframe at all. At that point saving a few seconds on read time doesn't count for much. | Someone is transpiling Pandas into Rust even now... | pandas is not designed to work out of memory, that‚Äôs why there are other libraries build on top of pandas like dask or modin who take care of this | The Pandas API really isn't confusing. Especially when compared to Polars. 

Also, you aren't meant to use Pandas in situations where your data has reached a massive scale. That's what things like Spark and Dask are for. 

Polars occupies a weird middle ground that frankly makes it a non-starter for most applications. If you are trying to do local development on smaller data then Polars offers really no significant advantages and lots of drawbacks. Conversely, if your data has grown too big for Pandas you really should just be using a proper distributed system instead. | I'll indulge you but this is well tested and benchmarked -

 1. pd.read_csv(): ~2min
 2. pd.read_csv(filepath, engine='pyarrow', dtype_backend=""pyarrow""): ~50sec
 3. pl.read_csv().to_pandas(): ~30sec: numpy/copy
 4. pyarrow.csv.read_csv().to_pandas(): ~30sec: numpy/copy
 5. pl.read_csv().to_pandas(use_pyarrow_extension_array=True): ~10sec
 6. pyarrow.csv.read_csv(filepath) .to_pandas(types_mapper=pd.ArrowDtype): ~10sec
    
dataset from  https://www.kaggle.com/datasets/zanjibar/100-million-data-csv?resource=download | > The index API was an interesting and unsuccessful experiment.

Indexes are not an implementation detail or experiment, they are a fundamental concept underlying the ability to represent data in an ndarray style. Pandas works on data in both a long relational format (e.g. polars) and a wide ndarray style format (e.g. numpy). Even the polars devs  realized this and they even removed this quote from their docs:

> Indexes are not needed! Not having them makes things easier ‚Äî convince us otherwise!

Polars is faster than pandas in the feature set they both provide (relational sql style operations) no question about it. But polars doesn‚Äôt provide ndarray style data representation or operations, which is an important feature set of pandas."
11eqczv,üêº Pandas 2.0 Up To 32x Faster,,"I have been playing with pyarrow lately and the read write is amazing compared to pandas to\_csv & read\_csv. The issue I'm having is reading crappy text files that need more handling that pandas can easily do but I haven't figured pyarrow's approach. 

When this comes out, I'll be checking it out for sure. It's not too bad switch from dataframe to pyarrow table but feel like I am not getting the full benefit of pyarrow. I hope this change closes the gap for me! | Polars will always have the edge because it's a fairly pure query layer over arrow, with a lazy multithreaded query planner. But this could close the gap a bit for certain operations.

One of the nice things here is that, once the next pandas and polars versions have come out, interop between then should be fairly seamless so you'll be able to use pandas where that makes sense and then dip into polars for more performance-limiting stuff. | Really curious which frameworks are going to be actually adopted the most (for production use). Thinking if there will be only Spark and Polars? Or Spark and Pandas? Spark and this Ballista? ü§î | Spark will still reign. I am really intrigued by Ballista, I hope that in 3/4 years it becomes supported.

As far as in-memory is concerned, pandas will still be widely used - after all, it's getting faster thanks to arrow. polars will always be faster and will raise in popularity. I believe it will be the second most popular framework. Maybe it will become the first in a couple of years, when there will be a thriving ecosystem of compatible libraries (say Pandera) and all plotting libraries will natively support it (as of now, seaborn does). Still, pandas is written in python (and Cython) so contributions are easier for python-first developers. | pandas is mostly C, Fortran and C++."
1b2qb5d,"Pandopt, basicaly pandas, but optimized","# Pandas is Great, But...

Pandas excels when your dataset is manageable in size, and you adhere strictly to its built-in functions. However, when either of these conditions is not met, performance drastically declines. Relying solely on custom, albeit user-friendly, built-in functions for decent performance seems suboptimal. Despite this, for many operations, learning an array of built-ins isn't necessary; you can accomplish a great deal using just a few key methods like `apply`, `aggregate`, `groupby`, and `rolling` (not addressing I/O here). However, the apply is known to be one of the worst in terms of performance, that's why I wanted to make it the best..!

Nonetheless, pandas shines for several reasons, including its user-friendly API and widespread adoption.

## Why I'm Skeptical of Alternatives

In the quest for better performance, you might consider options like Modin or Polars. However, my experience with them has been mixed: they either introduce a steep learning curve or fail to function in 90% of the environments I've tested.

## The Essence of Pandopt

Pandopt aims to enhance basic pandas operations, such as `apply` and `rolling`, by implementing dynamic code transformation and JIT compilation via Numba. This approach significantly boosts performance without altering other aspects of pandas functionality.

## Target Audience

This message is intended for data science enthusiasts, optimization fans, and those who appreciate elegant coding.

## Performance Comparison

Pandopt dramatically outperforms pandas in executing `apply` functions on large datasets (over 1 million rows), offering speed improvements of approximately 10,000 to 20,000 times post-initial compilation (which is cached). Even for standard operations like `sum`, it outpaces numpy or Polars by roughly 8 times. While gains in `rolling` functions are less pronounced, Pandopt still enhances flexibility, allowing the application of custom functions across multiple columns and functions natively.

Project is public on git and available on Pypi even if unstable: [https://github.com/remigenet/pandopt](https://github.com/remigenet/pandopt)","Why not just use numba directly?

I agree the apply method from pandas is not great, but use of numba to accelerate this method means the optimization is only relevant for numerical operations. Otherwise, it defaults to pandas builtin apply method.

As a general alternative, it is possible to use numpy's [vectorize](https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html) class. | Well of course you can do it with numba directly, but you have to change your function yourself (I mean you numba don't handle dict access, only on the array), and one of the nice pandas features is to be able to work with named column directly, the apply to just write your function with name of the column (readable and quick to write) and apply it ! Instead of having to know yourself well my columns are in this order I wanna do my apply with numba, I do my loop, I think about which index it is etc.. 
But yeah for sure if you search only performance and want to do more cases it can be done by hand, the purpose is to make it as easy as writing any apply in pandas ! 
Also the rolling is a bit more tricky to do directly with numpy yourself, while pandas one is very natural to me ! The whole point is staying with your usual dataframe and being able to have performance gain without any change in code
So it's just about making the simple optimisation tricks you can perform manually but automatically, after having tried many way to change any function to numba and kept the best one I found myself :)"
zs4kau,Get rid of SettingWithCopyWarning in pandas with Copy on Write,"Hi,

I am a member of the pandas core team (phofl on github). We are currently working on a new feature called Copy on Write. It is designed to get rid of all the inconsistencies in indexing operations. The feature is still actively developed. We would love to get feedback and general thoughts on this, since it will be a pretty substantial change. I wrote a post showing some different forms of behavior in indexing operations and how Copy on Write impacts them:

https://towardsdatascience.com/a-solution-for-inconsistencies-in-indexing-operations-in-pandas-b76e10719744

Happy to have a discussion here or on medium.","I must say that the SettingWithCopyWarning was one of the most puzzling things I had to wrap my head around in Pandas. Even though I got used to it I think it‚Äôs good to have a bit of consistency. It will probably be easier for beginners too. | Very informative post. As an R user who's starting to use pandas, this behavior is very odd. I expect everything to return a copy when assigned to a new variable. 

This is a great feature to have. | This will break many things. And I don‚Äôt only mean code itself, but just imagine the scale of OOMs that will be triggered once this change kicks in. To me, it will make pandas a lot less practical with large datasets. The solution will be to bump up provisioned memory to allow those spikes in usage  between the moment a copy is made and when the GC cleans the old copy out, which will drive up infra cost significantly for typical workloads. | Hopefully this triggers people to migrate towards a library that has more sensible behavior. 

Pandas has too much tech debt. nans vs actual NULLs was treated as a second class citizen until recently (and is still very much incomplete). They also recently rejected adhering to the standards - https://github.com/pandas-dev/pandas/issues/48880

Returning a copy for everything and deprecating inplace almost everywhere just makes pandas a non-starter in memory intensive jobs.

In all honesty though, what the pandas team really lacks is someone who has a clear vision of what the project ""should"" be. Maybe that's my personal preference, but I like projects that are opinionated and consistent. 

Before anyone tells me pandas is an all volunteer project - sure it is, but they also get proper funding for it. | DataFrame interfaces like those of pyspark and polars are so much simpler and more straightforward for issues like this once you get used to them. Pretty sure implementation is better too but that depends on use case.

Unless I am wrong, pandas dataframs dont have a withColumn() method. Why not? Maybe it has to do with memory implementation and pandas dataframes not being columnar like arrow tables. | A long overdue feature, imho. We had some not too large data mangling jobs last year (2-4 GB file size) , but with a somewhat complicated structure (time series with multiple channels, differing between measurement, varying sampling rates). Pandas just didn‚Äôt perform very well due to unpredictable copying behavior and clunky row indices. 

Although I blew the Python stack once, Polars‚Äò lazy paradigm seems much more scalable than Pandas. OTOH Pandas is amazing for EDA. | Performance aside, I see no downside to not mutating two or more distinct variables with a single value assignment. I do not want to troubleshoot back propagation of changes from inheritance. Sometimes I keep a source frame and break out derivative frames. Two child series or data frames should not permit change propagation from one to the other. Maybe permitting that unsafely is a feature for global updates but its a bad default.

Any time I've passed by reference to what I thought was an operation on a copy it's been a mistake. I understand the memory bonus of returning a view into a data frame for a row or col subset. I know not to assign anything to a row indexed iloc but columnar variable assignment  should present a distinct series from the source data frame. 

For example, I had a column of type numpy.ndarray and I thought by passing to a dataclass generator by using df['col'].values that was effectively scrubbing all pandas properties from the array. When I added +=2 to the dataclass attribute I was surprised to find the source data frame mutated. That was fixed with .values.copy() but this was already using a data frame pass by value copied into the function.

Think about casting from datetime to string in one data frame and it breaking an unrelated function using another variable. That may, in practice, create a copy data frame but my problem is I cannot reason for sure what the default behavior is.

In my opinion, row/index subsets make sense as a mutable view to the source because the index shows gaps. Column subsets would be best served with copy on default.

Thank you for the open discussion in advance. | That is what Spark figured out. I thought pandas was going to be moving towards that approach, but evidently not. | Hi,

at first glance it might look like it. But as soon as you use operations that aren‚Äôt indexing operations this is fortunately not a problem. The worst case of performing a setitem operation on a DataFrame with Copy on Write is making a copy of the whole DataFrame, this has the same memory spike as performing most pandas operations right now. A simple reset_index call will copy the data internally as well. The average pandas workflow should have a reduced memory footprint | >Why not?

Because pandas is terrible

Downvoted by people who have never used anything other than pandas lol | No, with the proposed behaviour, those methods won't copy the underlying data. Those methods don't ""write"" to the data, so no ""copy-on-write"" is needed (they only update the row/column labels, not the actual data in the columns). 

That's actually one of the improvements the proposal tries to achieve, because with current pandas, the snippet you show will have copied the data twice (each method makes a copy of the calling dataframe). The COW proposal tries to avoid all those unnecessary copies. | Yeah for syntax convenience this is what i was looking for.

But what i suspected is correct. In pandas .assign() creates a whole new dataframe in memory because of the array structure used to store df in memory. | We won‚Äôt make copies under cow in assign. Not sure if you are familiar with pandas internals, but we would add a new block to the blockmanager and deferring consolidation till a copy is necessary anyway | Thank you for the response. With respect to row subset and slicing you'd know better than I. & thanks for contributing to ongoing pandas development. I've gotten a ton of utility from it over the years.

I'll gladly take a memory hit with copy on default to avoid unintentional changes. | To me the entire pandas API is just a confusing mess, and I end up just doing guess and check to see if it gives me the results I want.

But I don't understand how this CoW logic handles the following:

     df = pd.DataFrame(....)
     df2 = df.loc[foo, ""bar""]
     df.loc[qux, ""bar""]  = 1 
     df2[""bar""] = 2

Ignoring what foo and qux might be.

* Assignment to `df.loc` never made much sense to me to begin with. `loc` is not a attribute of the dataframe, so how can you assign to it at all? It makes as little sense to me as assigning to `str.__len__`. So that it seems to work at all is just cryptic magic, that probably should never have been introduced into the API.
* Since `df.loc[...] = val` doesn't return any value it MUST modify `df` otherwise it would be useless.
* Similarly `df[...] = val` also MUST modify `df`.
* Assignment to `df2[""bar""]` in the above is really (by symbolic substitution) just assignment to `df.loc[foo, ""bar""][""bar""]` so the transitive property dictates that it must modify the original `df`.

Therefore I would expect `df` to show the assignment of 2 overlayed on top of the assignment of 1 on any elements that satisfy `foo` and `qux`.

That seems to be the behavior of pandas 1.4.1, and I would not expect that to change.

---------

In practice I don't do nonsense like this and generally try to either:

* Perform ""flat"" operations on the dataframe in sequence without interleaving them
* Otherwise treat the dataframe as an immutable object

My preference would be to move to an API without assignments to locators. Instead I would like to use an API that is more like Spark in having a `df_new = df.with_value_when(val, locator_clause)` or something that is very obviously making a copy of the full dataframe and giving me a new instance. | In other words the following may not be true in Pandas going forward:

    x[k1][k2] = v
    assert(x[k1][k2] == v) | Yes, one of the main drivers for the new Copy-on-Write behaviour is exactly to avoid this copy that pandas currently does in .assign() | I‚Äôm not sure, but I‚Äôd be surprised if you couldn‚Äôt work out how to do it. It‚Äôs built with integration as the principal feature. The other selling points for me are memory mapping and the documentation/support. Vaex uses pandas, pyarrow, and some other libraries for I/o so with some file binary types like HDF5 or arrow files, the data isn‚Äôt called into memory at all until you either write or materialize it some way. Your adjustments to a df or set are stored as expressions so it is extremely lightweight. Even if I need another library, I‚Äôll use Vaex for i/o.

Take this with a grain of salt. It‚Äôs a massive library and some of it is so far over my head that I know what‚Äôs happening but can‚Äôt always explain it perfectly. But if you want something lightweight and intuitive, Vaex is pretty great. They have an 11 minute crash course in the documentation that‚Äôll give you most of the answers you‚Äôre looking for. | > Assignment to `df2[""bar""]` in the above is really (by symbolic substitution) just assignment to `df.loc[foo, ""bar""][""bar""]` so the transitive property dictates that it must modify the original `df`.

This ""transitive property"" you mention is not something you can generally apply to python code. It depends on the semantics of those methods (do they return copies or views). 

With current pandas, this sometimes works and sometimes not, depending on what exactly `foo` is. 

With the proposed CoW behaviour, this will consistently _never_ work. Because each indexing call returns a new object that behaves as a copy, you cannot chain them if you want to assign to that expression.

> My preference would be to move to an API without assignments to locators.

Yes, and that is already possible (e.g. with `assign()`), but I certainly agree that this could be improved and made easier. | > So its very unusual for an API to distinguish between the first and subsequent calls.

I can certainly understand that you think this (and certainly in context of how pandas often behaved up to now), but as a counter example of standard python: also for python lists, you cannot do this:

```
>>> a_list = [1, 2, 3, 4, 5]
# single setitem -> modifies the list
>>> a_list[1:3] = [10, 11]
>>> a_list
[1, 10, 11, 4, 5]
# two [] operations (getitem + setitem) -> doesn't modify
>>> a_list[1:3][1] = 100
>>> a_list
[1, 10, 11, 4, 5]
```"
1bpdzfp,Load Apple's .numbers Files into Pandas,"I recently ran into some challenges while trying to work with Apple's .numbers files on Linux. After a bit of experimentation, I figured out a workflow that simplifies the process. If you're planning to use .numbers files and need to load them into pandas, I've created a tutorial that covers the required dependencies and the steps to follow: <https://nezhar.com/blog/load-apple-numbers-files-python-pandas-using-containers/>.

Has anyone else here worked with .numbers files in Python? I‚Äôd love to hear about your experiences or any tips you might have.",
104wqfg,Modern Polars: an extensive side-by-side comparison of Polars and Pandas,,"Author here! This work is a mix of:

- A translation guide for Pandas users who want to use Polars
- An opinionated review of Polars as a Pandas alternative
- A lengthy test drive of Polars | This seems like a very good alternative to pandas when used together with apache spark, as the syntax is much more similar. I'm going to give it a try for sure | After being almost 2 years working with Pandas, I find Polars quite interesting but still confusing. I attended one talk during PyCon ES about Polars and its advantages over Pandas but I didn't get the point at all.

Glad to see this, I'm gonna read it now and share with my local Python community :) | /u/ritchie46

A couple weeks ago I mentioned how I think one benefit of pandas indexes is the ability to designate the primary key/sorting columns of a dataframe and reduce the verbosity of code.

I think this has a really great example of that problem in section 2.4.1 https://kevinheavey.github.io/modern-polars/method_chaining.html

He has this bit of polars code and just count the number of times DepTime appears:

     df_pl
    .drop_nulls(subset=[""DepTime"", ""IATA_CODE_Reporting_Airline""])
    .filter(filter_expr)
    .sort(""DepTime"")
    .groupby_dynamic(
        ""DepTime"",
        every=""1h"",
        by=""IATA_CODE_Reporting_Airline"")
    .agg(pl.col(""Flight_Number_Reporting_Airline"").count())
    .pivot(
        index=""DepTime"",
        columns=""IATA_CODE_Reporting_Airline"",
        values=""Flight_Number_Reporting_Airline"",
    )
    .sort(""DepTime"")
    # fill every missing hour with 0 so the plot looks better
    .upsample(time_column=""DepTime"", every=""1h"")
    .fill_null(0)
    .select([pl.col(""DepTime""), pl.col(pl.UInt32).rolling_sum(24)])

SEVEN!!! Basically every single operation on this dataframe and you have to keep reiterating to polars that: ""This is time-series data, and the temporal element of this series is 'DepTime'""

That's why I suggest some kind of convenience method to establish defaults:

    with pl.DefaultCols(sort_key=""DepTime"", other_keys=""IATA_CODE_Reporting_Airline"") as pl_context:
      # and then all the same code, but not have to mention DepTime/IATA_CODE again
      # just have it automatically included in the right place
      # easier said than done I'm sure but...


----------

And that is basically what I see as one of the principal benefits of the pandas index. Once he calls `set_index` he can do the grouping and rolling and everything without even mentioning `DepTime`

    .set_index(""DepTime"")
    .groupby([""IATA_CODE_Reporting_Airline"", pd.Grouper(freq=""H"")])[""Flight_Number_Reporting_Airline""]
    .count()
    .unstack(0)
    .fillna(0)
    .rolling(24)
    .sum() | Very interesting - or maybe I'm just biased because I don't like pandas and would love to change to something new.

Also, love this quote:

> You don‚Äôt want folks assuming you‚Äôve lost your mind. | So I posted this in another thread about polars recently.

I really like polars, but one thing I wish it had is indexes. I know the lack of such is one of the reasons that polars can get the performance that it does, but they‚Äôre really useful in certain cases, especially multiindexes. I‚Äôd actually prefer to do everything in long format, which is what polars encourages, but that‚Äôs not practical in many cases as it can result in much larger memory requirements.

There‚Äôs also other benefits to multiindexes. For one with long format only all your data manipulations need to be done through relational operations. However if you take advantage of multiindexes you can manipulate your data through dimensional/structural operations, which can be easier to reason about in many cases.

That said I don‚Äôt think polars needs to worry about this use case. It‚Äôs very good at what it does (better than pandas), but I don‚Äôt think it‚Äôs a drop in replacement | I've been thinking about migrating to polars but the fact that it's so not in a stable release makes it harder. I use mainly pyspark but many of my projects are executed in a single machine so pyspark has way too much overhead for little benefit. It is still better than pandas though | Polars is a rust library too, and some of the chained methods look like rust builders. This isn‚Äôt in line with the pythonic way of doing things.

As a physicist myself, I don‚Äôt believe people in the natural sciences will be switching to polars. The native compatibility of pandas Series with numpy is an important feature. Most scientific code is written with numpy/scipy. And scientists hate charging tools, especially when something works.

I‚Äôll be giving polars a trial run, run it on my test projects too see if it‚Äôs a worthwhile upgrade. Nice article. | Awesome! been meaning to learn about polars,This is really a good comparison and overview. 

Not sure would I personally switch from pandas and pyspark for my workflow (yet)

Have interoperability with other packages through pandas and familiarity of the API then have scale for transformations with spark for heavy lifting jobs

Looking forward to seeing how it grows and matures, appreciate the similarities between the polars API and pyspark API as that will definitely help with adoption! | Spark now has the [pandas on spark api](https://sparkbyexamples.com/pyspark/pandas-api-on-apache-spark-pyspark/) which lets you manipulate dataframes using pandas syntax.... if that's something you really want to do. | The main advantage is the existence of the DAG of computations to be performed. Having that allows a form of ""compilation"" to be performed on the operations and then parallel dispatch of the individual steps.

That is very hard with pandas because much of the pandas api mutates the underlying object. You can't assume that because an operation on a dataframe touches a different set of columns from the previous command that it can be safely run in parallel.

In polars and spark and the like the baseline assumption is the reverse. You can run steps in parallel even if they operate on the same columns, because dataframes don't mutate. Instead you generate new dataframes. | The Polars api overview docs are so concise compared to pandas. Its a total breath of fresh air. | is pandas really easier to learn, or is there just a familiarity bias within the data science community to use pandas?

I always had a hard time being proficient with pandas due to the strange syntax & 100 ways to do the same operations. I feel polars and spark are actually much easier to reason about. They usually are a bit more verbose, and don't have as many conflicting ways of performing the same operations.

for example, selecting a column.

    # polars
    df.get_column(""foo"")
    # pandas
    df[""foo""]
    # also pandas
    df.foo
    # also pandas
    df.loc[:, ""foo""]

I can clearly see that polars is getting a column called ""foo"". | While I do think the the eager way of computation with pandas is initially slightly easier to reason about, the api of polars is much cleaner and easier to remember. | What do you mean, why do you think it‚Äôs better than pandas for data on a single machine? Performance testing, I don‚Äôt see a benefit to pyspark until we‚Äôre dealing with data frames 150gb+ in size (10 million rows or so), where the parallel processing ends up helping. | > Polars is a rust library too, and some of the chained methods look like rust builders. This isn‚Äôt in line with the pythonic way of doing things.

While I am somewhat sceptical of most claims that something is ‚ÄúPythonic‚Äù (it‚Äôs vague imo), I am curious if you noticed any examples where the Pandas code looked more Pythonic than Polars. People already say that Pandas is not Pythonic, though I disagree.

> The native compatibility of pandas Series with numpy is an important feature.

Does it change your mind if I say that Polars [works well with NumPy](https://kevinheavey.github.io/modern-polars/performance.html#numpy-can-make-polars-faster)? Would love to look at an example too | > Polars is a rust library too, and some of the chained methods look like rust builders.

The heavy use of chaining is a byproduct of the fact that polars dataframes are immutable. You see the same thing in pyspark.

> The native compatibility of pandas Series with numpy is an important feature.

There actually should be very good compatibility between polars and numpy, as both prioritize keeping data contiguous. In many instances the libraries can do everything with zero copies. The biggest headache here is that they do take different views on mutability, so that has to be tracked and managed if you try and go back-and-forth.

---------

Polars relies on Arrow for the memory store of the data itself. Arrow has some differences from numpy particularly where it comes to:

* null values -- Arrow uses masks where numpy uses sentinel or NaN values.
* multi-dimensional arrays and tensors
* and the aforementioned mutability

If a dataframe is what you are after (something with clearly defined rows, and columns of heterogeneous type) Arrow is a better foundation for memory storage than numpy.

If you want to link to your Fortran code that is doing matrix multiplications then numpy is the right tool.

-------

But you can start with one and shift to the other. Run your simulation/model with numpy+fortran, then convert the resulting outputs to Arrow/polars for summary and report generation. | That will probably never be a great experience. There is a base level mis-alignment between spark and pandas as to what a dataframe is, which leads to weird stuff.

In spark a dataframe is immutable, but not in pandas. So in spark APIs you always create new columns and new dataframes derived from the previous. In pandas you can replace the contents of an existing dataframe or directly modify them. | your link points to the exact opposite - translating pandas api to spark programs. This is great for some use cases, but not mine. I much prefer writing in spark's (or spark-like) syntax. | Pandas dfs are also columnar.

Dask parallelizes and distributes across chunks which is desirable when your dataset might exceed memory. It's DAG is generally composed of higher level tasks.

In some sense if you ran dask on top of polars you would be approximating what spark does."
zrhnq1,Alternative to Pandas,"Hi guys! Hope everybody is okay

Talking about data analysis using Python, is there another option excluding Pandas?

Excel sheet will be the data resource","An alternative to what features exactly?

Pandas is like a swiss army knife. It does quite a lot. Most people don't use all of its features, used mainly for in-memory data processing. It's gigantic too.

If you just need to read excel files and do only basic read operations, you could look at `openpyxl`. | Why would you not use pandas? Its basically the standard dataframe library unless dealing with huge data. Any real reason? | What's the problem you are experiencing with Pandas?  


If it's a production performance issue, I would suggest you simply stream the file row by row, read the data into dictionaries, perform your manipulations there, then put them all back into a relational format and pipe the data to its next destination. This solution doesn't pandas, or any other dependencies, it's fast and robust, and you can now call yourself a data engineer!  


(sorry not sorry)  


For real though, if you can tell us the specific problem you are trying to solve by using an alternate library, you'll get much better answers. | There are no alternatives to pandas, except koalas. They're so freaking cute. | Does anyone use numba decorators with pandas? I‚Äôve read it speeds up pandas considerably but never tried myself. | I use pandas for most everything but I find openpyxl nice as well | I am maintaining a small list of [pandas alternatives](https://github.com/baggiponte/awesome-pandas-alternatives) on github, but I guess that for your usecase pandas would be the perfect match. | I like to second this. But add that if the xlsheets are strangely formatted then xlwings is a really useful package for getting the data into pandas. It's also useful if the excel docs are password protected. | Most things you‚Äôd do in pandas, you can do in dask. Dask is better and much faster. It also doesn‚Äôt have all the issues that pandas has when trying to work with large datasets (20gb+). | Isn‚Äôt kolas the name of the pandas-on-spark library? | Kolas is the name of the drop in pandas library that  runs on spark.

Now you know!"
ts5mmf,I made a video about efficient memory use in pandas dataframes!,,"If you really want speed you should try [`modin.pandas`](https://github.com/modin-project/modin) which makes pandas multi-threaded. | Really cool stuff. Am a newcomer to Pandas, but I'm using very large datasets so, I'll definitely check if I can use any of this in my work | Memory efficient pandas? Thats an oxymoron. :P | Totally, I didn't realize this at first either. They mention it in the docs but it works best when the number of categories is low, otherwise it can use more memory:

https://pandas.pydata.org/pandas-docs/stable/user\_guide/categorical.html#categorical-memory"
165neyy,What's the point for Pandas and Numpy,"I'm going through a Datacamp Python course which details the use of Numpy and Pandas; you know loading and converting a csv file into datframes,comparing lists etc

What I noticed is that you can do all the same by using SQL and much easier too. nowadays it's even easier to load a csv into say sqlite or duckdb and then do your processing there.

I'm not a data scientist but a database developer so probably I'm looking at it from that perspective.

So when to use SQL and when to go with those python libraries?","> can do all the same by using SQL and much easier too

That ""easier"" is certainly debatable.  Though I would be interested to know what the processing time comparisons are for various data crunching operations.  

My gut says that once you have the data stored locally, numpy and pandas are probably about as efficient as you can easily get to.  But if you're sending SQL to a remote database, seems like that could greatly reduce data transfer time if you don't need the whole table. | Let me Google that for you ....

https://datascience.stackexchange.com/questions/34357/why-do-people-prefer-pandas-to-sql#34366 | I get that you percieve Pandas as redundant as someone comfortable with SQL, but at the very least it is often less verbose to quickly wrangle a dataframe with Pandas than it is with SQL.

Numpy though? What kind of tasks are you being taught to do with Numpy? Do people actually do things like linear algebra and high-dimensional tensor arithmetic with SQL? | You don't have to create a SQL server and install all the SQL packages and manage indices and all of that crap with pandas and numpy. You can just interact with the CSV directly. I also question whether SQL is faster - that isn't my experience. Both have their uses though. | Large SQL queries get unwieldy VERY fast.  Pandas is effortless to transform data in practically limitless ways. | It really depends on what you're doing, but there are a few differences.  Numpy and Pandas tend to have more compact representations for certain types of data than a SQL database does, which allows some operations to be done much faster than a SQL database can handle (eg, adding together billions of numbers simultaneously).

I would agree with you that certain things are much easier in SQL, and honestly I don't really view it as an either/or situation.  It's pretty straightforward to use most SQL databases from Python and they make a lot of data storage and some simple calculations a lot easier.  When you start noticing things slowing down (or you need to parallelize calculations) pull the data from the database into numpy and do it there. | SQL is not in-memory, it writes to the disk. Numpy and pandas do in-memory operations. | 1. What if you don‚Äôt have a database?

2. When you want to process data in python, say, for ML, pandas and numpy are much faster than using a database adapter.

3. Data manipulation/processing is soooo much easier and intuitive in pandas.

Now of course, no one is forcing you to use pandas or numpy. | No way.  I have a lot of experience with SQL and Pandas/numpy. There are some things that are extremely hard or even impossible to do in SQL. Believe me. A query language can do so much. | In my current job SQL in a nice database is ideal but with Pandas and NP I don‚Äôt have to take the time to get a database set up and worry about associations, data types, etc. it‚Äôs just a lot faster when not doing anything long term | For me, the only point to pandas is an intermediary step from Database, into Duckdb. I am much more comfortable around SQL than the pandas methods to do the same tasks. 

Also, duckdb handles data larger than memory much easier than Pandas and without even having to think about the data size. | 1. Is like saying ‚Äúwhat if you don‚Äôt have pandas?‚Äù I mean, pip is there for a reason.

2. Probably depends on the database and application. I‚Äôd be curious to see some blogs about that though. 

3. The real answer."
